pred_norm
y_test
sum(pred_norm$3)
sum(pred_norm[,3])
dim(pred_norm)
sum(y_test[,3])
sum(y_test[,1])
sum(y_test[,2])
sum(y_test[,4])
sum(y_train[,4])
sum(y_train[,1])
sum(y_train[,2])
sum(y_train[,3])
sum(y_train[,4])
sum(y_train[,5])
print("######### hyper paramenters and error scores ###########")
print("######### hyper paramenters and error scores ###########")#
configs <- rbind(configs,data.frame(hidden_rbm=hidden_rbm, numempochs_rbm = numepochs_rbm, batchsize_rbm=batchsize_rbm, lr_rbm=learningrate_rbm, cd=cd, hidden_nn=paste(hidden_nn,collapse=" "), lr_nn=learningrate_nn, numepochs_nn=numepochs_nn, batchsize_nn=batchsize_nn, onehot=onehot, err_score=score, err_score_norm = score_norm))
print("######### hyper paramenters and error scores ###########")#
configs
print("######### hyper paramenters and error scores ###########", configs)
library(deepnet)
clean()
clear()
clear
clean
library(deepnet)#
#
# run this only once#
 configs = data.frame(hidden_rbm=integer(), numempochs_rbm=integer(), batchsize_rbm=integer(), lr_rbm=numeric(), cd=integer(), hidden_nn=character(), lr_nn=numeric(), numepochs_nn=integer(), batchsize_nn=numeric(), onehot=integer(), err_score=numeric(), err_score_norm=numeric())#
#
onehot <- 1#
#
hidden_rbm <- 100#
numepochs_rbm <- 10#
batchsize_rbm <- 90#
learningrate_rbm <- 0.01#
learningrate_scale_rbm <- 0.9#
cd <- 3#
#
hidden_nn <- c(10)#
learningrate_nn <- 0.01#
learningrate_scale_nn <- 1#
numepochs_nn <- 3#
batchsize_nn <- 90#
#
X_train <- read.csv(file="../X_train.csv", header=T, sep=",", row.names=1)#
X_test <- read.csv(file="../X_test.csv", header=T, sep=",", row.names=1)#
y_train <- read.csv(file="../y_train.csv", header=T, sep=",", row.names=1)#
y_test <- read.csv(file="../y_test.csv", header=T, sep=",", row.names=1)#
#
X_train <- X_train[1:939,]#
X_test <- X_test[1:313,]#
#
# trying one hot encoding#
onehot_test <- matrix(0L, nrow=dim(y_test)[1], ncol=max(y_test)+1)#
counter <- 1#
for(y in 1:dim(y_test)[1]){#
	onehot_test[counter,y_test[y,]] <- 1#
	counter <- counter+1#
}#
# trying one hot encoding#
onehot_train <- matrix(0L, nrow=dim(y_train)[1], ncol=max(y_train)+1)#
counter <- 1#
for(y in 1:dim(y_train)[1]){#
	onehot_train[counter, y_train[y,]+1] <- 1#
	counter <- counter+1#
}#
X_train <- as.matrix(X_train)#
X_test <- as.matrix(X_test)#
y_train <- as.matrix(y_train)#
y_test <- as.matrix(y_test)#
#
if(onehot){#
	y_test <- onehot_test#
	y_train <- onehot_train#
}#
#
rbm <- rbm.train(x=X_train, hidden=hidden_rbm, numepochs = numepochs_rbm, batchsize = batchsize_rbm, learningrate = learningrate_rbm, learningrate_scale = learningrate_scale_rbm, momentum = 0.5, visible_type = "bin", hidden_type = "bin", cd = cd)#
#
transformed_train <- rbm.up(rbm, X_train)#
transformed_test <- rbm.up(rbm, X_test)#
#
nn = nn.train(x=transformed_train, y_train, initW = NULL, initB = NULL, hidden = hidden_nn, activationfun = "sigm", learningrate = learningrate_nn, momentum = 0.5, learningrate_scale = learningrate_scale_nn, output = "sigm", numepochs = numepochs_nn, batchsize = batchsize_nn, hidden_dropout = 0, visible_dropout = 0)#
#
score <- 0#
score <- nn.test(nn, transformed_test, y_test, t = 0.9)#
#
pred <- nn.predict(nn, transformed_test)#
pred_norm <- matrix(0L, nrow=dim(pred)[1], ncol=dim(pred)[2])#
for(i in 1:dim(pred)[1]){#
	max_row <- which.max(pred[i,])#
	pred_norm[i,max_row] <- 1#
}#
#
score_norm <- 0#
for(i in 1:dim(pred_norm)[1]){#
	if(which.max(pred_norm[i,]) == which.max(y_test[i,])){#
		score_norm <- score_norm +1#
	}#
}#
score_norm <- score_norm/dim(pred)[1]#
#
configs <- rbind(configs,data.frame(hidden_rbm=hidden_rbm, numempochs_rbm = numepochs_rbm, batchsize_rbm=batchsize_rbm, lr_rbm=learningrate_rbm, cd=cd, hidden_nn=paste(hidden_nn,collapse=" "), lr_nn=learningrate_nn, numepochs_nn=numepochs_nn, batchsize_nn=batchsize_nn, onehot=onehot, err_score=score, err_score_norm = score_norm))#
#
configs
library(deepnet)#
#
# run this only once#
# configs = data.frame(hidden_rbm=integer(), numempochs_rbm=integer(), batchsize_rbm=integer(), lr_rbm=numeric(), cd=integer(), hidden_nn=character(), lr_nn=numeric(), numepochs_nn=integer(), batchsize_nn=numeric(), onehot=integer(), err_score=numeric(), err_score_norm=numeric())#
#
onehot <- 1#
#
hidden_rbm <- 100#
numepochs_rbm <- 10#
batchsize_rbm <- 90#
learningrate_rbm <- 0.01#
learningrate_scale_rbm <- 0.9#
cd <- 3#
#
hidden_nn <- c(10)#
learningrate_nn <- 0.01#
learningrate_scale_nn <- 1#
numepochs_nn <- 3#
batchsize_nn <- 90#
#
X_train <- read.csv(file="../X_train.csv", header=T, sep=",", row.names=1)#
X_test <- read.csv(file="../X_test.csv", header=T, sep=",", row.names=1)#
y_train <- read.csv(file="../y_train.csv", header=T, sep=",", row.names=1)#
y_test <- read.csv(file="../y_test.csv", header=T, sep=",", row.names=1)#
#
X_train <- X_train[1:939,]#
X_test <- X_test[1:313,]#
#
# trying one hot encoding#
onehot_test <- matrix(0L, nrow=dim(y_test)[1], ncol=max(y_test)+1)#
counter <- 1#
for(y in 1:dim(y_test)[1]){#
	onehot_test[counter,y_test[y,]] <- 1#
	counter <- counter+1#
}#
# trying one hot encoding#
onehot_train <- matrix(0L, nrow=dim(y_train)[1], ncol=max(y_train)+1)#
counter <- 1#
for(y in 1:dim(y_train)[1]){#
	onehot_train[counter, y_train[y,]+1] <- 1#
	counter <- counter+1#
}#
X_train <- as.matrix(X_train)#
X_test <- as.matrix(X_test)#
y_train <- as.matrix(y_train)#
y_test <- as.matrix(y_test)#
#
if(onehot){#
	y_test <- onehot_test#
	y_train <- onehot_train#
}#
#
rbm <- rbm.train(x=X_train, hidden=hidden_rbm, numepochs = numepochs_rbm, batchsize = batchsize_rbm, learningrate = learningrate_rbm, learningrate_scale = learningrate_scale_rbm, momentum = 0.5, visible_type = "bin", hidden_type = "bin", cd = cd)#
#
transformed_train <- rbm.up(rbm, X_train)#
transformed_test <- rbm.up(rbm, X_test)#
#
nn = nn.train(x=transformed_train, y_train, initW = NULL, initB = NULL, hidden = hidden_nn, activationfun = "sigm", learningrate = learningrate_nn, momentum = 0.5, learningrate_scale = learningrate_scale_nn, output = "sigm", numepochs = numepochs_nn, batchsize = batchsize_nn, hidden_dropout = 0, visible_dropout = 0)#
#
score <- 0#
score <- nn.test(nn, transformed_test, y_test, t = 0.9)#
#
pred <- nn.predict(nn, transformed_test)#
pred_norm <- matrix(0L, nrow=dim(pred)[1], ncol=dim(pred)[2])#
for(i in 1:dim(pred)[1]){#
	max_row <- which.max(pred[i,])#
	pred_norm[i,max_row] <- 1#
}#
#
score_norm <- 0#
for(i in 1:dim(pred_norm)[1]){#
	if(which.max(pred_norm[i,]) == which.max(y_test[i,])){#
		score_norm <- score_norm +1#
	}#
}#
score_norm <- score_norm/dim(pred)[1]#
#
configs <- rbind(configs,data.frame(hidden_rbm=hidden_rbm, numempochs_rbm = numepochs_rbm, batchsize_rbm=batchsize_rbm, lr_rbm=learningrate_rbm, cd=cd, hidden_nn=paste(hidden_nn,collapse=" "), lr_nn=learningrate_nn, numepochs_nn=numepochs_nn, batchsize_nn=batchsize_nn, onehot=onehot, err_score=score, err_score_norm = score_norm))#
#
configs
library(deepnet)#
#
# run this only once#
# configs = data.frame(hidden_rbm=integer(), numempochs_rbm=integer(), batchsize_rbm=integer(), lr_rbm=numeric(), cd=integer(), hidden_nn=character(), lr_nn=numeric(), numepochs_nn=integer(), batchsize_nn=numeric(), onehot=integer(), err_score=numeric(), err_score_norm=numeric())#
#
onehot <- 1#
#
hidden_rbm <- 100#
numepochs_rbm <- 10#
batchsize_rbm <- 90#
learningrate_rbm <- 0.01#
learningrate_scale_rbm <- 0.9#
cd <- 3#
#
hidden_nn <- c(10)#
learningrate_nn <- 0.01#
learningrate_scale_nn <- 1#
numepochs_nn <- 3#
batchsize_nn <- 90#
#
X_train <- read.csv(file="../X_train.csv", header=T, sep=",", row.names=1)#
X_test <- read.csv(file="../X_test.csv", header=T, sep=",", row.names=1)#
y_train <- read.csv(file="../y_train.csv", header=T, sep=",", row.names=1)#
y_test <- read.csv(file="../y_test.csv", header=T, sep=",", row.names=1)#
#
X_train <- X_train[1:939,]#
X_test <- X_test[1:313,]#
#
# trying one hot encoding#
onehot_test <- matrix(0L, nrow=dim(y_test)[1], ncol=max(y_test)+1)#
counter <- 1#
for(y in 1:dim(y_test)[1]){#
	onehot_test[counter,y_test[y,]] <- 1#
	counter <- counter+1#
}#
# trying one hot encoding#
onehot_train <- matrix(0L, nrow=dim(y_train)[1], ncol=max(y_train)+1)#
counter <- 1#
for(y in 1:dim(y_train)[1]){#
	onehot_train[counter, y_train[y,]+1] <- 1#
	counter <- counter+1#
}#
X_train <- as.matrix(X_train)#
X_test <- as.matrix(X_test)#
y_train <- as.matrix(y_train)#
y_test <- as.matrix(y_test)#
#
if(onehot){#
	y_test <- onehot_test#
	y_train <- onehot_train#
}#
#
rbm <- rbm.train(x=X_train, hidden=hidden_rbm, numepochs = numepochs_rbm, batchsize = batchsize_rbm, learningrate = learningrate_rbm, learningrate_scale = learningrate_scale_rbm, momentum = 0.5, visible_type = "bin", hidden_type = "bin", cd = cd)#
#
transformed_train <- rbm.up(rbm, X_train)#
transformed_test <- rbm.up(rbm, X_test)#
#
nn = nn.train(x=transformed_train, y_train, initW = NULL, initB = NULL, hidden = hidden_nn, activationfun = "sigm", learningrate = learningrate_nn, momentum = 0.5, learningrate_scale = learningrate_scale_nn, output = "sigm", numepochs = numepochs_nn, batchsize = batchsize_nn, hidden_dropout = 0, visible_dropout = 0)#
#
score <- 0#
score <- nn.test(nn, transformed_test, y_test, t = 0.5)#
#
pred <- nn.predict(nn, transformed_test)#
pred_norm <- matrix(0L, nrow=dim(pred)[1], ncol=dim(pred)[2])#
for(i in 1:dim(pred)[1]){#
	max_row <- which.max(pred[i,])#
	pred_norm[i,max_row] <- 1#
}#
#
score_norm <- 0#
for(i in 1:dim(pred_norm)[1]){#
	if(which.max(pred_norm[i,]) == which.max(y_test[i,])){#
		score_norm <- score_norm +1#
	}#
}#
score_norm <- score_norm/dim(pred)[1]#
#
configs <- rbind(configs,data.frame(hidden_rbm=hidden_rbm, numempochs_rbm = numepochs_rbm, batchsize_rbm=batchsize_rbm, lr_rbm=learningrate_rbm, cd=cd, hidden_nn=paste(hidden_nn,collapse=" "), lr_nn=learningrate_nn, numepochs_nn=numepochs_nn, batchsize_nn=batchsize_nn, onehot=onehot, err_score=score, err_score_norm = score_norm))#
#
configs
pred_norm
pred
y_test
sum(y_test[,3])
dim(y_test)
library(deepnet)#
#
# run this only once#
# configs = data.frame(hidden_rbm=integer(), numempochs_rbm=integer(), batchsize_rbm=integer(), lr_rbm=numeric(), cd=integer(), hidden_nn=character(), lr_nn=numeric(), numepochs_nn=integer(), batchsize_nn=numeric(), onehot=integer(), err_score=numeric(), err_score_norm=numeric())#
#
onehot <- 1#
#
hidden_rbm <- 100#
numepochs_rbm <- 10#
batchsize_rbm <- 90#
learningrate_rbm <- 0.01#
learningrate_scale_rbm <- 0.9#
cd <- 3#
#
hidden_nn <- c(10)#
learningrate_nn <- 0.01#
learningrate_scale_nn <- 1#
numepochs_nn <- 3#
batchsize_nn <- 90#
#
X_train <- read.csv(file="../X_train.csv", header=T, sep=",", row.names=1)#
X_test <- read.csv(file="../X_test.csv", header=T, sep=",", row.names=1)#
y_train <- read.csv(file="../y_train.csv", header=T, sep=",", row.names=1)#
y_test <- read.csv(file="../y_test.csv", header=T, sep=",", row.names=1)#
#
X_train <- X_train[1:939,]#
X_test <- X_test[1:313,]#
#
# trying one hot encoding#
onehot_test <- matrix(0L, nrow=dim(y_test)[1], ncol=max(y_test)+1)#
counter <- 1#
for(y in 1:dim(y_test)[1]){#
	onehot_test[counter,y_test[y,]] <- 1#
	counter <- counter+1#
}#
# trying one hot encoding#
onehot_train <- matrix(0L, nrow=dim(y_train)[1], ncol=max(y_train)+1)#
counter <- 1#
for(y in 1:dim(y_train)[1]){#
	onehot_train[counter, y_train[y,]+1] <- 1#
	counter <- counter+1#
}#
X_train <- as.matrix(X_train)#
X_test <- as.matrix(X_test)#
y_train <- as.matrix(y_train)#
y_test <- as.matrix(y_test)#
#
if(onehot){#
	y_test <- onehot_test#
	y_train <- onehot_train#
}#
#
rbm <- rbm.train(x=X_train, hidden=hidden_rbm, numepochs = numepochs_rbm, batchsize = batchsize_rbm, learningrate = learningrate_rbm, learningrate_scale = learningrate_scale_rbm, momentum = 0.5, visible_type = "bin", hidden_type = "bin", cd = cd)#
#
transformed_train <- rbm.up(rbm, X_train)#
transformed_test <- rbm.up(rbm, X_test)#
#
nn = nn.train(x=transformed_train, y_train, initW = NULL, initB = NULL, hidden = hidden_nn, activationfun = "sigm", learningrate = learningrate_nn, momentum = 0.5, learningrate_scale = learningrate_scale_nn, output = "sigm", numepochs = numepochs_nn, batchsize = batchsize_nn, hidden_dropout = 0, visible_dropout = 0)#
#
score <- 0#
score <- nn.test(nn, transformed_test, y_test, t = 0.5)#
#
pred <- nn.predict(nn, transformed_test)#
pred_norm <- matrix(0L, nrow=dim(pred)[1], ncol=dim(pred)[2])#
for(i in 1:dim(pred)[1]){#
	max_row <- which.max(pred[i,])#
	pred_norm[i,max_row] <- 1#
}#
#
score_norm <- 0#
for(i in 1:dim(pred_norm)[1]){#
	if(which.max(pred_norm[i,]) == which.max(y_test[i,])){#
		score_norm <- score_norm +1#
	}#
}#
score_norm <- score_norm/dim(pred)[1]-1#
#
configs <- rbind(configs,data.frame(hidden_rbm=hidden_rbm, numempochs_rbm = numepochs_rbm, batchsize_rbm=batchsize_rbm, lr_rbm=learningrate_rbm, cd=cd, hidden_nn=paste(hidden_nn,collapse=" "), lr_nn=learningrate_nn, numepochs_nn=numepochs_nn, batchsize_nn=batchsize_nn, onehot=onehot, err_score=score, err_score_norm = score_norm))#
#
configs
library(deepnet)#
#
# run this only once#
# configs = data.frame(hidden_rbm=integer(), numempochs_rbm=integer(), batchsize_rbm=integer(), lr_rbm=numeric(), cd=integer(), hidden_nn=character(), lr_nn=numeric(), numepochs_nn=integer(), batchsize_nn=numeric(), onehot=integer(), err_score=numeric(), err_score_norm=numeric())#
#
onehot <- 1#
#
hidden_rbm <- 100#
numepochs_rbm <- 10#
batchsize_rbm <- 90#
learningrate_rbm <- 0.01#
learningrate_scale_rbm <- 0.9#
cd <- 3#
#
hidden_nn <- c(10)#
learningrate_nn <- 0.01#
learningrate_scale_nn <- 1#
numepochs_nn <- 3#
batchsize_nn <- 90#
#
X_train <- read.csv(file="../X_train.csv", header=T, sep=",", row.names=1)#
X_test <- read.csv(file="../X_test.csv", header=T, sep=",", row.names=1)#
y_train <- read.csv(file="../y_train.csv", header=T, sep=",", row.names=1)#
y_test <- read.csv(file="../y_test.csv", header=T, sep=",", row.names=1)#
#
X_train <- X_train[1:939,]#
X_test <- X_test[1:313,]#
#
# trying one hot encoding#
onehot_test <- matrix(0L, nrow=dim(y_test)[1], ncol=max(y_test)+1)#
counter <- 1#
for(y in 1:dim(y_test)[1]){#
	onehot_test[counter,y_test[y,]] <- 1#
	counter <- counter+1#
}#
# trying one hot encoding#
onehot_train <- matrix(0L, nrow=dim(y_train)[1], ncol=max(y_train)+1)#
counter <- 1#
for(y in 1:dim(y_train)[1]){#
	onehot_train[counter, y_train[y,]+1] <- 1#
	counter <- counter+1#
}#
X_train <- as.matrix(X_train)#
X_test <- as.matrix(X_test)#
y_train <- as.matrix(y_train)#
y_test <- as.matrix(y_test)#
#
if(onehot){#
	y_test <- onehot_test#
	y_train <- onehot_train#
}#
#
rbm <- rbm.train(x=X_train, hidden=hidden_rbm, numepochs = numepochs_rbm, batchsize = batchsize_rbm, learningrate = learningrate_rbm, learningrate_scale = learningrate_scale_rbm, momentum = 0.5, visible_type = "bin", hidden_type = "bin", cd = cd)#
#
transformed_train <- rbm.up(rbm, X_train)#
transformed_test <- rbm.up(rbm, X_test)#
#
nn = nn.train(x=transformed_train, y_train, initW = NULL, initB = NULL, hidden = hidden_nn, activationfun = "sigm", learningrate = learningrate_nn, momentum = 0.5, learningrate_scale = learningrate_scale_nn, output = "sigm", numepochs = numepochs_nn, batchsize = batchsize_nn, hidden_dropout = 0, visible_dropout = 0)#
#
score <- 0#
score <- nn.test(nn, transformed_test, y_test, t = 0.5)#
#
pred <- nn.predict(nn, transformed_test)#
pred_norm <- matrix(0L, nrow=dim(pred)[1], ncol=dim(pred)[2])#
for(i in 1:dim(pred)[1]){#
	max_row <- which.max(pred[i,])#
	pred_norm[i,max_row] <- 1#
}#
#
score_norm <- 0#
for(i in 1:dim(pred_norm)[1]){#
	if(which.max(pred_norm[i,]) == which.max(y_test[i,])){#
		score_norm <- score_norm +1#
	}#
}#
score_norm <- 1- score_norm/dim(pred)[1]#
#
configs <- rbind(configs,data.frame(hidden_rbm=hidden_rbm, numempochs_rbm = numepochs_rbm, batchsize_rbm=batchsize_rbm, lr_rbm=learningrate_rbm, cd=cd, hidden_nn=paste(hidden_nn,collapse=" "), lr_nn=learningrate_nn, numepochs_nn=numepochs_nn, batchsize_nn=batchsize_nn, onehot=onehot, err_score=score, err_score_norm = score_norm))#
#
configs
configs = data.frame(hidden_rbm=integer(), numempochs_rbm=integer(), batchsize_rbm=integer(), lr_rbm=numeric(), cd=integer(), hidden_nn=character(), lr_nn=numeric(), numepochs_nn=integer(), batchsize_nn=numeric(), onehot=integer(), err_score=numeric(), err_score_norm=numeric())
#library(deepnet)#
#
# run this only once#
# configs = data.frame(hidden_rbm=integer(), numempochs_rbm=integer(), batchsize_rbm=integer(), lr_rbm=numeric(), cd=integer(), hidden_nn=character(), lr_nn=numeric(), numepochs_nn=integer(), batchsize_nn=numeric(), onehot=integer(), err_score=numeric(), err_score_norm=numeric())#
#
onehot <- 1#
#
hidden_rbm <- 100#
numepochs_rbm <- 10#
batchsize_rbm <- 90#
learningrate_rbm <- 0.01#
learningrate_scale_rbm <- 0.9#
cd <- 3#
#
hidden_nn <- c(10)#
learningrate_nn <- 0.01#
learningrate_scale_nn <- 1#
numepochs_nn <- 3#
batchsize_nn <- 90#
#
X_train <- read.csv(file="../X_train.csv", header=T, sep=",", row.names=1)#
X_test <- read.csv(file="../X_test.csv", header=T, sep=",", row.names=1)#
y_train <- read.csv(file="../y_train.csv", header=T, sep=",", row.names=1)#
y_test <- read.csv(file="../y_test.csv", header=T, sep=",", row.names=1)#
#
X_train <- X_train[1:939,]#
X_test <- X_test[1:313,]#
#
# trying one hot encoding#
onehot_test <- matrix(0L, nrow=dim(y_test)[1], ncol=max(y_test)+1)#
counter <- 1#
for(y in 1:dim(y_test)[1]){#
	onehot_test[counter,y_test[y,]] <- 1#
	counter <- counter+1#
}#
# trying one hot encoding#
onehot_train <- matrix(0L, nrow=dim(y_train)[1], ncol=max(y_train)+1)#
counter <- 1#
for(y in 1:dim(y_train)[1]){#
	onehot_train[counter, y_train[y,]+1] <- 1#
	counter <- counter+1#
}#
X_train <- as.matrix(X_train)#
X_test <- as.matrix(X_test)#
y_train <- as.matrix(y_train)#
y_test <- as.matrix(y_test)#
#
if(onehot){#
	y_test <- onehot_test#
	y_train <- onehot_train#
}#
#
rbm <- rbm.train(x=X_train, hidden=hidden_rbm, numepochs = numepochs_rbm, batchsize = batchsize_rbm, learningrate = learningrate_rbm, learningrate_scale = learningrate_scale_rbm, momentum = 0.5, visible_type = "bin", hidden_type = "bin", cd = cd)#
#
transformed_train <- rbm.up(rbm, X_train)#
transformed_test <- rbm.up(rbm, X_test)#
#
nn = nn.train(x=transformed_train, y_train, initW = NULL, initB = NULL, hidden = hidden_nn, activationfun = "sigm", learningrate = learningrate_nn, momentum = 0.5, learningrate_scale = learningrate_scale_nn, output = "sigm", numepochs = numepochs_nn, batchsize = batchsize_nn, hidden_dropout = 0, visible_dropout = 0)#
#
score <- 0#
score <- nn.test(nn, transformed_test, y_test, t = 0.5)#
#
pred <- nn.predict(nn, transformed_test)#
pred_norm <- matrix(0L, nrow=dim(pred)[1], ncol=dim(pred)[2])#
for(i in 1:dim(pred)[1]){#
	max_row <- which.max(pred[i,])#
	pred_norm[i,max_row] <- 1#
}#
#
score_norm <- 0#
for(i in 1:dim(pred_norm)[1]){#
	if(which.max(pred_norm[i,]) == which.max(y_test[i,])){#
		score_norm <- score_norm +1#
	}#
}#
score_norm <- 1- score_norm/dim(pred)[1]#
#
configs <- rbind(configs,data.frame(hidden_rbm=hidden_rbm, numempochs_rbm = numepochs_rbm, batchsize_rbm=batchsize_rbm, lr_rbm=learningrate_rbm, cd=cd, hidden_nn=paste(hidden_nn,collapse=" "), lr_nn=learningrate_nn, numepochs_nn=numepochs_nn, batchsize_nn=batchsize_nn, onehot=onehot, err_score=score, err_score_norm = score_norm))#
#
configs
#library(deepnet)#
#
# run this only once#
# configs = data.frame(hidden_rbm=integer(), numempochs_rbm=integer(), batchsize_rbm=integer(), lr_rbm=numeric(), cd=integer(), hidden_nn=character(), lr_nn=numeric(), numepochs_nn=integer(), batchsize_nn=numeric(), onehot=integer(), err_score=numeric(), err_score_norm=numeric())#
#
onehot <- 1#
#
hidden_rbm <- 100#
numepochs_rbm <- 10#
batchsize_rbm <- 90#
learningrate_rbm <- 0.001#
learningrate_scale_rbm <- 0.9#
cd <- 3#
#
hidden_nn <- c(10)#
learningrate_nn <- 0.01#
learningrate_scale_nn <- 1#
numepochs_nn <- 3#
batchsize_nn <- 90#
#
X_train <- read.csv(file="../X_train.csv", header=T, sep=",", row.names=1)#
X_test <- read.csv(file="../X_test.csv", header=T, sep=",", row.names=1)#
y_train <- read.csv(file="../y_train.csv", header=T, sep=",", row.names=1)#
y_test <- read.csv(file="../y_test.csv", header=T, sep=",", row.names=1)#
#
X_train <- X_train[1:939,]#
X_test <- X_test[1:313,]#
#
# trying one hot encoding#
onehot_test <- matrix(0L, nrow=dim(y_test)[1], ncol=max(y_test)+1)#
counter <- 1#
for(y in 1:dim(y_test)[1]){#
	onehot_test[counter,y_test[y,]] <- 1#
	counter <- counter+1#
}#
# trying one hot encoding#
onehot_train <- matrix(0L, nrow=dim(y_train)[1], ncol=max(y_train)+1)#
counter <- 1#
for(y in 1:dim(y_train)[1]){#
	onehot_train[counter, y_train[y,]+1] <- 1#
	counter <- counter+1#
}#
X_train <- as.matrix(X_train)#
X_test <- as.matrix(X_test)#
y_train <- as.matrix(y_train)#
y_test <- as.matrix(y_test)#
#
if(onehot){#
	y_test <- onehot_test#
	y_train <- onehot_train#
}#
#
rbm <- rbm.train(x=X_train, hidden=hidden_rbm, numepochs = numepochs_rbm, batchsize = batchsize_rbm, learningrate = learningrate_rbm, learningrate_scale = learningrate_scale_rbm, momentum = 0.5, visible_type = "bin", hidden_type = "bin", cd = cd)#
#
transformed_train <- rbm.up(rbm, X_train)#
transformed_test <- rbm.up(rbm, X_test)#
#
nn = nn.train(x=transformed_train, y_train, initW = NULL, initB = NULL, hidden = hidden_nn, activationfun = "sigm", learningrate = learningrate_nn, momentum = 0.5, learningrate_scale = learningrate_scale_nn, output = "sigm", numepochs = numepochs_nn, batchsize = batchsize_nn, hidden_dropout = 0, visible_dropout = 0)#
#
score <- 0#
score <- nn.test(nn, transformed_test, y_test, t = 0.5)#
#
pred <- nn.predict(nn, transformed_test)#
pred_norm <- matrix(0L, nrow=dim(pred)[1], ncol=dim(pred)[2])#
for(i in 1:dim(pred)[1]){#
	max_row <- which.max(pred[i,])#
	pred_norm[i,max_row] <- 1#
}#
#
score_norm <- 0#
for(i in 1:dim(pred_norm)[1]){#
	if(which.max(pred_norm[i,]) == which.max(y_test[i,])){#
		score_norm <- score_norm +1#
	}#
}#
score_norm <- 1- score_norm/dim(pred)[1]#
#
configs <- rbind(configs,data.frame(hidden_rbm=hidden_rbm, numempochs_rbm = numepochs_rbm, batchsize_rbm=batchsize_rbm, lr_rbm=learningrate_rbm, cd=cd, hidden_nn=paste(hidden_nn,collapse=" "), lr_nn=learningrate_nn, numepochs_nn=numepochs_nn, batchsize_nn=batchsize_nn, onehot=onehot, err_score=score, err_score_norm = score_norm))#
#
configs
#library(deepnet)#
#
# run this only once#
# configs = data.frame(hidden_rbm=integer(), numempochs_rbm=integer(), batchsize_rbm=integer(), lr_rbm=numeric(), cd=integer(), hidden_nn=character(), lr_nn=numeric(), numepochs_nn=integer(), batchsize_nn=numeric(), onehot=integer(), err_score=numeric(), err_score_norm=numeric())#
#
onehot <- 1#
#
hidden_rbm <- 100#
numepochs_rbm <- 10#
batchsize_rbm <- 90#
learningrate_rbm <- 0.001#
learningrate_scale_rbm <- 0.9#
cd <- 3#
#
hidden_nn <- c(10)#
learningrate_nn <- 0.001#
learningrate_scale_nn <- 1#
numepochs_nn <- 3#
batchsize_nn <- 90#
#
X_train <- read.csv(file="../X_train.csv", header=T, sep=",", row.names=1)#
X_test <- read.csv(file="../X_test.csv", header=T, sep=",", row.names=1)#
y_train <- read.csv(file="../y_train.csv", header=T, sep=",", row.names=1)#
y_test <- read.csv(file="../y_test.csv", header=T, sep=",", row.names=1)#
#
X_train <- X_train[1:939,]#
X_test <- X_test[1:313,]#
#
# trying one hot encoding#
onehot_test <- matrix(0L, nrow=dim(y_test)[1], ncol=max(y_test)+1)#
counter <- 1#
for(y in 1:dim(y_test)[1]){#
	onehot_test[counter,y_test[y,]] <- 1#
	counter <- counter+1#
}#
# trying one hot encoding#
onehot_train <- matrix(0L, nrow=dim(y_train)[1], ncol=max(y_train)+1)#
counter <- 1#
for(y in 1:dim(y_train)[1]){#
	onehot_train[counter, y_train[y,]+1] <- 1#
	counter <- counter+1#
}#
X_train <- as.matrix(X_train)#
X_test <- as.matrix(X_test)#
y_train <- as.matrix(y_train)#
y_test <- as.matrix(y_test)#
#
if(onehot){#
	y_test <- onehot_test#
	y_train <- onehot_train#
}#
#
rbm <- rbm.train(x=X_train, hidden=hidden_rbm, numepochs = numepochs_rbm, batchsize = batchsize_rbm, learningrate = learningrate_rbm, learningrate_scale = learningrate_scale_rbm, momentum = 0.5, visible_type = "bin", hidden_type = "bin", cd = cd)#
#
transformed_train <- rbm.up(rbm, X_train)#
transformed_test <- rbm.up(rbm, X_test)#
#
nn = nn.train(x=transformed_train, y_train, initW = NULL, initB = NULL, hidden = hidden_nn, activationfun = "sigm", learningrate = learningrate_nn, momentum = 0.5, learningrate_scale = learningrate_scale_nn, output = "sigm", numepochs = numepochs_nn, batchsize = batchsize_nn, hidden_dropout = 0, visible_dropout = 0)#
#
score <- 0#
score <- nn.test(nn, transformed_test, y_test, t = 0.5)#
#
pred <- nn.predict(nn, transformed_test)#
pred_norm <- matrix(0L, nrow=dim(pred)[1], ncol=dim(pred)[2])#
for(i in 1:dim(pred)[1]){#
	max_row <- which.max(pred[i,])#
	pred_norm[i,max_row] <- 1#
}#
#
score_norm <- 0#
for(i in 1:dim(pred_norm)[1]){#
	if(which.max(pred_norm[i,]) == which.max(y_test[i,])){#
		score_norm <- score_norm +1#
	}#
}#
score_norm <- 1- score_norm/dim(pred)[1]#
#
configs <- rbind(configs,data.frame(hidden_rbm=hidden_rbm, numempochs_rbm = numepochs_rbm, batchsize_rbm=batchsize_rbm, lr_rbm=learningrate_rbm, cd=cd, hidden_nn=paste(hidden_nn,collapse=" "), lr_nn=learningrate_nn, numepochs_nn=numepochs_nn, batchsize_nn=batchsize_nn, onehot=onehot, err_score=score, err_score_norm = score_norm))#
#
configs
pred_norm
#library(deepnet)#
#
# run this only once#
# configs = data.frame(hidden_rbm=integer(), numempochs_rbm=integer(), batchsize_rbm=integer(), lr_rbm=numeric(), cd=integer(), hidden_nn=character(), lr_nn=numeric(), numepochs_nn=integer(), batchsize_nn=numeric(), onehot=integer(), err_score=numeric(), err_score_norm=numeric())#
#
onehot <- 1#
#
hidden_rbm <- 100#
numepochs_rbm <- 10#
batchsize_rbm <- 90#
learningrate_rbm <- 0.001#
learningrate_scale_rbm <- 0.9#
cd <- 3#
#
hidden_nn <- c(10)#
learningrate_nn <- 0.001#
learningrate_scale_nn <- 1#
numepochs_nn <- 3#
batchsize_nn <- 90#
#
X_train <- read.csv(file="../X_train.csv", header=T, sep=",", row.names=1)#
X_test <- read.csv(file="../X_test.csv", header=T, sep=",", row.names=1)#
y_train <- read.csv(file="../y_train.csv", header=T, sep=",", row.names=1)#
y_test <- read.csv(file="../y_test.csv", header=T, sep=",", row.names=1)#
#
X_train <- X_train[1:939,]#
X_test <- X_test[1:313,]#
#
# trying one hot encoding#
onehot_test <- matrix(0L, nrow=dim(y_test)[1], ncol=max(y_test)+1)#
counter <- 1#
for(y in 1:dim(y_test)[1]){#
	onehot_test[counter,y_test[y,]] <- 1#
	counter <- counter+1#
}#
# trying one hot encoding#
onehot_train <- matrix(0L, nrow=dim(y_train)[1], ncol=max(y_train)+1)#
counter <- 1#
for(y in 1:dim(y_train)[1]){#
	onehot_train[counter, y_train[y,]+1] <- 1#
	counter <- counter+1#
}#
X_train <- as.matrix(X_train)#
X_test <- as.matrix(X_test)#
y_train <- as.matrix(y_train)#
y_test <- as.matrix(y_test)#
#
if(onehot){#
	y_test <- onehot_test#
	y_train <- onehot_train#
}#
#
rbm <- rbm.train(x=X_train, hidden=hidden_rbm, numepochs = numepochs_rbm, batchsize = batchsize_rbm, learningrate = learningrate_rbm, learningrate_scale = learningrate_scale_rbm, momentum = 0.5, visible_type = "bin", hidden_type = "bin", cd = cd)#
#
transformed_train <- rbm.up(rbm, X_train)#
transformed_test <- rbm.up(rbm, X_test)#
#
nn = nn.train(x=transformed_train, y_train, initW = NULL, initB = NULL, hidden = hidden_nn, activationfun = "sigm", learningrate = learningrate_nn, momentum = 0.5, learningrate_scale = learningrate_scale_nn, output = "sigm", numepochs = numepochs_nn, batchsize = batchsize_nn, hidden_dropout = 0, visible_dropout = 0)#
#
score <- 0#
score <- nn.test(nn, transformed_test, y_test, t = 0.5)#
#
pred <- nn.predict(nn, transformed_test)#
pred_norm <- matrix(0L, nrow=dim(pred)[1], ncol=dim(pred)[2])#
for(i in 1:dim(pred)[1]){#
	max_row <- which.max(pred[i,])#
	pred_norm[i,max_row] <- 1#
}#
#
score_norm <- 0#
for(i in 1:dim(pred_norm)[1]){#
	if(which.max(pred_norm[i,]) == which.max(y_test[i,])){#
		score_norm <- score_norm +1#
	}#
}#
score_norm <- 1- score_norm/dim(pred)[1]#
#
configs <- rbind(configs,data.frame(hidden_rbm=hidden_rbm, numempochs_rbm = numepochs_rbm, batchsize_rbm=batchsize_rbm, lr_rbm=learningrate_rbm, cd=cd, hidden_nn=paste(hidden_nn,collapse=" "), lr_nn=learningrate_nn, numepochs_nn=numepochs_nn, batchsize_nn=batchsize_nn, onehot=onehot, err_score=score, err_score_norm = score_norm))#
#
configs
pred_norm
#library(deepnet)#
#
# run this only once#
# configs = data.frame(hidden_rbm=integer(), numempochs_rbm=integer(), batchsize_rbm=integer(), lr_rbm=numeric(), cd=integer(), hidden_nn=character(), lr_nn=numeric(), numepochs_nn=integer(), batchsize_nn=numeric(), onehot=integer(), err_score=numeric(), err_score_norm=numeric())#
#
onehot <- 1#
#
hidden_rbm <- 100#
numepochs_rbm <- 10#
batchsize_rbm <- 90#
learningrate_rbm <- 0.001#
learningrate_scale_rbm <- 0.9#
cd <- 3#
#
hidden_nn <- c(10)#
learningrate_nn <- 0.001#
learningrate_scale_nn <- 1#
numepochs_nn <- 3#
batchsize_nn <- 200#
#
X_train <- read.csv(file="../X_train.csv", header=T, sep=",", row.names=1)#
X_test <- read.csv(file="../X_test.csv", header=T, sep=",", row.names=1)#
y_train <- read.csv(file="../y_train.csv", header=T, sep=",", row.names=1)#
y_test <- read.csv(file="../y_test.csv", header=T, sep=",", row.names=1)#
#
X_train <- X_train[1:939,]#
X_test <- X_test[1:313,]#
#
# trying one hot encoding#
onehot_test <- matrix(0L, nrow=dim(y_test)[1], ncol=max(y_test)+1)#
counter <- 1#
for(y in 1:dim(y_test)[1]){#
	onehot_test[counter,y_test[y,]] <- 1#
	counter <- counter+1#
}#
# trying one hot encoding#
onehot_train <- matrix(0L, nrow=dim(y_train)[1], ncol=max(y_train)+1)#
counter <- 1#
for(y in 1:dim(y_train)[1]){#
	onehot_train[counter, y_train[y,]+1] <- 1#
	counter <- counter+1#
}#
X_train <- as.matrix(X_train)#
X_test <- as.matrix(X_test)#
y_train <- as.matrix(y_train)#
y_test <- as.matrix(y_test)#
#
if(onehot){#
	y_test <- onehot_test#
	y_train <- onehot_train#
}#
#
rbm <- rbm.train(x=X_train, hidden=hidden_rbm, numepochs = numepochs_rbm, batchsize = batchsize_rbm, learningrate = learningrate_rbm, learningrate_scale = learningrate_scale_rbm, momentum = 0.5, visible_type = "bin", hidden_type = "bin", cd = cd)#
#
transformed_train <- rbm.up(rbm, X_train)#
transformed_test <- rbm.up(rbm, X_test)#
#
nn = nn.train(x=transformed_train, y_train, initW = NULL, initB = NULL, hidden = hidden_nn, activationfun = "sigm", learningrate = learningrate_nn, momentum = 0.5, learningrate_scale = learningrate_scale_nn, output = "sigm", numepochs = numepochs_nn, batchsize = batchsize_nn, hidden_dropout = 0, visible_dropout = 0)#
#
score <- 0#
score <- nn.test(nn, transformed_test, y_test, t = 0.5)#
#
pred <- nn.predict(nn, transformed_test)#
pred_norm <- matrix(0L, nrow=dim(pred)[1], ncol=dim(pred)[2])#
for(i in 1:dim(pred)[1]){#
	max_row <- which.max(pred[i,])#
	pred_norm[i,max_row] <- 1#
}#
#
score_norm <- 0#
for(i in 1:dim(pred_norm)[1]){#
	if(which.max(pred_norm[i,]) == which.max(y_test[i,])){#
		score_norm <- score_norm +1#
	}#
}#
score_norm <- 1- score_norm/dim(pred)[1]#
#
configs <- rbind(configs,data.frame(hidden_rbm=hidden_rbm, numempochs_rbm = numepochs_rbm, batchsize_rbm=batchsize_rbm, lr_rbm=learningrate_rbm, cd=cd, hidden_nn=paste(hidden_nn,collapse=" "), lr_nn=learningrate_nn, numepochs_nn=numepochs_nn, batchsize_nn=batchsize_nn, onehot=onehot, err_score=score, err_score_norm = score_norm))#
#
configs
#library(deepnet)#
#
# run this only once#
# configs = data.frame(hidden_rbm=integer(), numempochs_rbm=integer(), batchsize_rbm=integer(), lr_rbm=numeric(), cd=integer(), hidden_nn=character(), lr_nn=numeric(), numepochs_nn=integer(), batchsize_nn=numeric(), onehot=integer(), err_score=numeric(), err_score_norm=numeric())#
#
onehot <- 1#
#
hidden_rbm <- 100#
numepochs_rbm <- 10#
batchsize_rbm <- 200#
learningrate_rbm <- 0.001#
learningrate_scale_rbm <- 0.9#
cd <- 3#
#
hidden_nn <- c(10)#
learningrate_nn <- 0.001#
learningrate_scale_nn <- 1#
numepochs_nn <- 3#
batchsize_nn <- 200#
#
X_train <- read.csv(file="../X_train.csv", header=T, sep=",", row.names=1)#
X_test <- read.csv(file="../X_test.csv", header=T, sep=",", row.names=1)#
y_train <- read.csv(file="../y_train.csv", header=T, sep=",", row.names=1)#
y_test <- read.csv(file="../y_test.csv", header=T, sep=",", row.names=1)#
#
X_train <- X_train[1:939,]#
X_test <- X_test[1:313,]#
#
# trying one hot encoding#
onehot_test <- matrix(0L, nrow=dim(y_test)[1], ncol=max(y_test)+1)#
counter <- 1#
for(y in 1:dim(y_test)[1]){#
	onehot_test[counter,y_test[y,]] <- 1#
	counter <- counter+1#
}#
# trying one hot encoding#
onehot_train <- matrix(0L, nrow=dim(y_train)[1], ncol=max(y_train)+1)#
counter <- 1#
for(y in 1:dim(y_train)[1]){#
	onehot_train[counter, y_train[y,]+1] <- 1#
	counter <- counter+1#
}#
X_train <- as.matrix(X_train)#
X_test <- as.matrix(X_test)#
y_train <- as.matrix(y_train)#
y_test <- as.matrix(y_test)#
#
if(onehot){#
	y_test <- onehot_test#
	y_train <- onehot_train#
}#
#
rbm <- rbm.train(x=X_train, hidden=hidden_rbm, numepochs = numepochs_rbm, batchsize = batchsize_rbm, learningrate = learningrate_rbm, learningrate_scale = learningrate_scale_rbm, momentum = 0.5, visible_type = "bin", hidden_type = "bin", cd = cd)#
#
transformed_train <- rbm.up(rbm, X_train)#
transformed_test <- rbm.up(rbm, X_test)#
#
nn = nn.train(x=transformed_train, y_train, initW = NULL, initB = NULL, hidden = hidden_nn, activationfun = "sigm", learningrate = learningrate_nn, momentum = 0.5, learningrate_scale = learningrate_scale_nn, output = "sigm", numepochs = numepochs_nn, batchsize = batchsize_nn, hidden_dropout = 0, visible_dropout = 0)#
#
score <- 0#
score <- nn.test(nn, transformed_test, y_test, t = 0.5)#
#
pred <- nn.predict(nn, transformed_test)#
pred_norm <- matrix(0L, nrow=dim(pred)[1], ncol=dim(pred)[2])#
for(i in 1:dim(pred)[1]){#
	max_row <- which.max(pred[i,])#
	pred_norm[i,max_row] <- 1#
}#
#
score_norm <- 0#
for(i in 1:dim(pred_norm)[1]){#
	if(which.max(pred_norm[i,]) == which.max(y_test[i,])){#
		score_norm <- score_norm +1#
	}#
}#
score_norm <- 1- score_norm/dim(pred)[1]#
#
configs <- rbind(configs,data.frame(hidden_rbm=hidden_rbm, numempochs_rbm = numepochs_rbm, batchsize_rbm=batchsize_rbm, lr_rbm=learningrate_rbm, cd=cd, hidden_nn=paste(hidden_nn,collapse=" "), lr_nn=learningrate_nn, numepochs_nn=numepochs_nn, batchsize_nn=batchsize_nn, onehot=onehot, err_score=score, err_score_norm = score_norm))#
#
configs
#library(deepnet)#
#
# run this only once#
# configs = data.frame(hidden_rbm=integer(), numempochs_rbm=integer(), batchsize_rbm=integer(), lr_rbm=numeric(), cd=integer(), hidden_nn=character(), lr_nn=numeric(), numepochs_nn=integer(), batchsize_nn=numeric(), onehot=integer(), err_score=numeric(), err_score_norm=numeric())#
#
onehot <- 1#
#
hidden_rbm <- 100#
numepochs_rbm <- 10#
batchsize_rbm <- 200#
learningrate_rbm <- 0.001#
learningrate_scale_rbm <- 0.9#
cd <- 3#
#
hidden_nn <- c(10)#
learningrate_nn <- 0.001#
learningrate_scale_nn <- 0.9#
numepochs_nn <- 3#
batchsize_nn <- 200#
#
X_train <- read.csv(file="../X_train.csv", header=T, sep=",", row.names=1)#
X_test <- read.csv(file="../X_test.csv", header=T, sep=",", row.names=1)#
y_train <- read.csv(file="../y_train.csv", header=T, sep=",", row.names=1)#
y_test <- read.csv(file="../y_test.csv", header=T, sep=",", row.names=1)#
#
X_train <- X_train[1:939,]#
X_test <- X_test[1:313,]#
#
# trying one hot encoding#
onehot_test <- matrix(0L, nrow=dim(y_test)[1], ncol=max(y_test)+1)#
counter <- 1#
for(y in 1:dim(y_test)[1]){#
	onehot_test[counter,y_test[y,]] <- 1#
	counter <- counter+1#
}#
# trying one hot encoding#
onehot_train <- matrix(0L, nrow=dim(y_train)[1], ncol=max(y_train)+1)#
counter <- 1#
for(y in 1:dim(y_train)[1]){#
	onehot_train[counter, y_train[y,]+1] <- 1#
	counter <- counter+1#
}#
X_train <- as.matrix(X_train)#
X_test <- as.matrix(X_test)#
y_train <- as.matrix(y_train)#
y_test <- as.matrix(y_test)#
#
if(onehot){#
	y_test <- onehot_test#
	y_train <- onehot_train#
}#
#
rbm <- rbm.train(x=X_train, hidden=hidden_rbm, numepochs = numepochs_rbm, batchsize = batchsize_rbm, learningrate = learningrate_rbm, learningrate_scale = learningrate_scale_rbm, momentum = 0.5, visible_type = "bin", hidden_type = "bin", cd = cd)#
#
transformed_train <- rbm.up(rbm, X_train)#
transformed_test <- rbm.up(rbm, X_test)#
#
nn = nn.train(x=transformed_train, y_train, initW = NULL, initB = NULL, hidden = hidden_nn, activationfun = "sigm", learningrate = learningrate_nn, momentum = 0.5, learningrate_scale = learningrate_scale_nn, output = "sigm", numepochs = numepochs_nn, batchsize = batchsize_nn, hidden_dropout = 0, visible_dropout = 0)#
#
score <- 0#
score <- nn.test(nn, transformed_test, y_test, t = 0.5)#
#
pred <- nn.predict(nn, transformed_test)#
pred_norm <- matrix(0L, nrow=dim(pred)[1], ncol=dim(pred)[2])#
for(i in 1:dim(pred)[1]){#
	max_row <- which.max(pred[i,])#
	pred_norm[i,max_row] <- 1#
}#
#
score_norm <- 0#
for(i in 1:dim(pred_norm)[1]){#
	if(which.max(pred_norm[i,]) == which.max(y_test[i,])){#
		score_norm <- score_norm +1#
	}#
}#
score_norm <- 1- score_norm/dim(pred)[1]#
#
configs <- rbind(configs,data.frame(hidden_rbm=hidden_rbm, numempochs_rbm = numepochs_rbm, batchsize_rbm=batchsize_rbm, lr_rbm=learningrate_rbm, cd=cd, hidden_nn=paste(hidden_nn,collapse=" "), lr_nn=learningrate_nn, numepochs_nn=numepochs_nn, batchsize_nn=batchsize_nn, onehot=onehot, err_score=score, err_score_norm = score_norm))#
#
configs
#library(deepnet)#
#
# run this only once#
# configs = data.frame(hidden_rbm=integer(), numempochs_rbm=integer(), batchsize_rbm=integer(), lr_rbm=numeric(), cd=integer(), hidden_nn=character(), lr_nn=numeric(), numepochs_nn=integer(), batchsize_nn=numeric(), onehot=integer(), err_score=numeric(), err_score_norm=numeric())#
#
onehot <- 1#
#
hidden_rbm <- 100#
numepochs_rbm <- 10#
batchsize_rbm <- 200#
learningrate_rbm <- 0.001#
learningrate_scale_rbm <- 0.9#
cd <- 10#
#
hidden_nn <- c(10)#
learningrate_nn <- 0.001#
learningrate_scale_nn <- 0.9#
numepochs_nn <- 3#
batchsize_nn <- 200#
#
X_train <- read.csv(file="../X_train.csv", header=T, sep=",", row.names=1)#
X_test <- read.csv(file="../X_test.csv", header=T, sep=",", row.names=1)#
y_train <- read.csv(file="../y_train.csv", header=T, sep=",", row.names=1)#
y_test <- read.csv(file="../y_test.csv", header=T, sep=",", row.names=1)#
#
X_train <- X_train[1:939,]#
X_test <- X_test[1:313,]#
#
# trying one hot encoding#
onehot_test <- matrix(0L, nrow=dim(y_test)[1], ncol=max(y_test)+1)#
counter <- 1#
for(y in 1:dim(y_test)[1]){#
	onehot_test[counter,y_test[y,]] <- 1#
	counter <- counter+1#
}#
# trying one hot encoding#
onehot_train <- matrix(0L, nrow=dim(y_train)[1], ncol=max(y_train)+1)#
counter <- 1#
for(y in 1:dim(y_train)[1]){#
	onehot_train[counter, y_train[y,]+1] <- 1#
	counter <- counter+1#
}#
X_train <- as.matrix(X_train)#
X_test <- as.matrix(X_test)#
y_train <- as.matrix(y_train)#
y_test <- as.matrix(y_test)#
#
if(onehot){#
	y_test <- onehot_test#
	y_train <- onehot_train#
}#
#
rbm <- rbm.train(x=X_train, hidden=hidden_rbm, numepochs = numepochs_rbm, batchsize = batchsize_rbm, learningrate = learningrate_rbm, learningrate_scale = learningrate_scale_rbm, momentum = 0.5, visible_type = "bin", hidden_type = "bin", cd = cd)#
#
transformed_train <- rbm.up(rbm, X_train)#
transformed_test <- rbm.up(rbm, X_test)#
#
nn = nn.train(x=transformed_train, y_train, initW = NULL, initB = NULL, hidden = hidden_nn, activationfun = "sigm", learningrate = learningrate_nn, momentum = 0.5, learningrate_scale = learningrate_scale_nn, output = "sigm", numepochs = numepochs_nn, batchsize = batchsize_nn, hidden_dropout = 0, visible_dropout = 0)#
#
score <- 0#
score <- nn.test(nn, transformed_test, y_test, t = 0.5)#
#
pred <- nn.predict(nn, transformed_test)#
pred_norm <- matrix(0L, nrow=dim(pred)[1], ncol=dim(pred)[2])#
for(i in 1:dim(pred)[1]){#
	max_row <- which.max(pred[i,])#
	pred_norm[i,max_row] <- 1#
}#
#
score_norm <- 0#
for(i in 1:dim(pred_norm)[1]){#
	if(which.max(pred_norm[i,]) == which.max(y_test[i,])){#
		score_norm <- score_norm +1#
	}#
}#
score_norm <- 1- score_norm/dim(pred)[1]#
#
configs <- rbind(configs,data.frame(hidden_rbm=hidden_rbm, numempochs_rbm = numepochs_rbm, batchsize_rbm=batchsize_rbm, lr_rbm=learningrate_rbm, cd=cd, hidden_nn=paste(hidden_nn,collapse=" "), lr_nn=learningrate_nn, numepochs_nn=numepochs_nn, batchsize_nn=batchsize_nn, onehot=onehot, err_score=score, err_score_norm = score_norm))#
#
configs
pred_norm
#library(deepnet)#
#
# run this only once#
# configs = data.frame(hidden_rbm=integer(), numempochs_rbm=integer(), batchsize_rbm=integer(), lr_rbm=numeric(), cd=integer(), hidden_nn=character(), lr_nn=numeric(), numepochs_nn=integer(), batchsize_nn=numeric(), onehot=integer(), err_score=numeric(), err_score_norm=numeric())#
#
onehot <- 1#
#
hidden_rbm <- 100#
numepochs_rbm <- 10#
batchsize_rbm <- 200#
learningrate_rbm <- 0.001#
learningrate_scale_rbm <- 0.9#
cd <- 10#
#
hidden_nn <- c(10)#
learningrate_nn <- 0.001#
learningrate_scale_nn <- 0.9#
numepochs_nn <- 3#
batchsize_nn <- 200#
#
X_train <- read.csv(file="../X_train.csv", header=T, sep=",", row.names=1)#
X_test <- read.csv(file="../X_test.csv", header=T, sep=",", row.names=1)#
y_train <- read.csv(file="../y_train.csv", header=T, sep=",", row.names=1)#
y_test <- read.csv(file="../y_test.csv", header=T, sep=",", row.names=1)#
#
X_train <- X_train[1:939,]#
X_test <- X_test[1:313,]#
#
# trying one hot encoding#
onehot_test <- matrix(0L, nrow=dim(y_test)[1], ncol=max(y_test)+1)#
counter <- 1#
for(y in 1:dim(y_test)[1]){#
	onehot_test[counter,y_test[y,]] <- 1#
	counter <- counter+1#
}#
# trying one hot encoding#
onehot_train <- matrix(0L, nrow=dim(y_train)[1], ncol=max(y_train)+1)#
counter <- 1#
for(y in 1:dim(y_train)[1]){#
	onehot_train[counter, y_train[y,]+1] <- 1#
	counter <- counter+1#
}#
X_train <- as.matrix(X_train)#
X_test <- as.matrix(X_test)#
y_train <- as.matrix(y_train)#
y_test <- as.matrix(y_test)#
#
if(onehot){#
	y_test <- onehot_test#
	y_train <- onehot_train#
}#
#
rbm <- rbm.train(x=X_train, hidden=hidden_rbm, numepochs = numepochs_rbm, batchsize = batchsize_rbm, learningrate = learningrate_rbm, learningrate_scale = learningrate_scale_rbm, momentum = 0.5, visible_type = "bin", hidden_type = "bin", cd = cd)#
#
transformed_train <- rbm.up(rbm, X_train)#
transformed_test <- rbm.up(rbm, X_test)#
#
nn = nn.train(x=transformed_train, y_train, initW = NULL, initB = NULL, hidden = hidden_nn, activationfun = "sigm", learningrate = learningrate_nn, momentum = 0.5, learningrate_scale = learningrate_scale_nn, output = "sigm", numepochs = numepochs_nn, batchsize = batchsize_nn, hidden_dropout = 0, visible_dropout = 0)#
#
score <- 0#
score <- nn.test(nn, transformed_test, y_test, t = 0.5)#
#
pred <- nn.predict(nn, transformed_test)#
pred_norm <- matrix(0L, nrow=dim(pred)[1], ncol=dim(pred)[2])#
for(i in 1:dim(pred)[1]){#
	max_row <- which.max(pred[i,])#
	pred_norm[i,max_row] <- 1#
}#
#
score_norm <- 0#
for(i in 1:dim(pred_norm)[1]){#
	if(which.max(pred_norm[i,]) == which.max(y_test[i,])){#
		score_norm <- score_norm +1#
	}#
}#
score_norm <- 1- score_norm/dim(pred)[1]#
#
configs <- rbind(configs,data.frame(hidden_rbm=hidden_rbm, numempochs_rbm = numepochs_rbm, batchsize_rbm=batchsize_rbm, lr_rbm=learningrate_rbm, cd=cd, hidden_nn=paste(hidden_nn,collapse=" "), lr_nn=learningrate_nn, numepochs_nn=numepochs_nn, batchsize_nn=batchsize_nn, onehot=onehot, err_score=score, err_score_norm = score_norm))#
#
configs
pred_norm
#library(deepnet)#
#
# run this only once#
# configs = data.frame(hidden_rbm=integer(), numempochs_rbm=integer(), batchsize_rbm=integer(), lr_rbm=numeric(), cd=integer(), hidden_nn=character(), lr_nn=numeric(), numepochs_nn=integer(), batchsize_nn=numeric(), onehot=integer(), err_score=numeric(), err_score_norm=numeric())#
#
onehot <- 1#
#
hidden_rbm <- 100#
numepochs_rbm <- 10#
batchsize_rbm <- 300#
learningrate_rbm <- 0.001#
learningrate_scale_rbm <- 0.9#
cd <- 10#
#
hidden_nn <- c(10)#
learningrate_nn <- 0.001#
learningrate_scale_nn <- 0.9#
numepochs_nn <- 10#
batchsize_nn <- 300#
#
X_train <- read.csv(file="../X_train.csv", header=T, sep=",", row.names=1)#
X_test <- read.csv(file="../X_test.csv", header=T, sep=",", row.names=1)#
y_train <- read.csv(file="../y_train.csv", header=T, sep=",", row.names=1)#
y_test <- read.csv(file="../y_test.csv", header=T, sep=",", row.names=1)#
#
X_train <- X_train[1:939,]#
X_test <- X_test[1:313,]#
#
# trying one hot encoding#
onehot_test <- matrix(0L, nrow=dim(y_test)[1], ncol=max(y_test)+1)#
counter <- 1#
for(y in 1:dim(y_test)[1]){#
	onehot_test[counter,y_test[y,]] <- 1#
	counter <- counter+1#
}#
# trying one hot encoding#
onehot_train <- matrix(0L, nrow=dim(y_train)[1], ncol=max(y_train)+1)#
counter <- 1#
for(y in 1:dim(y_train)[1]){#
	onehot_train[counter, y_train[y,]+1] <- 1#
	counter <- counter+1#
}#
X_train <- as.matrix(X_train)#
X_test <- as.matrix(X_test)#
y_train <- as.matrix(y_train)#
y_test <- as.matrix(y_test)#
#
if(onehot){#
	y_test <- onehot_test#
	y_train <- onehot_train#
}#
#
rbm <- rbm.train(x=X_train, hidden=hidden_rbm, numepochs = numepochs_rbm, batchsize = batchsize_rbm, learningrate = learningrate_rbm, learningrate_scale = learningrate_scale_rbm, momentum = 0.5, visible_type = "bin", hidden_type = "bin", cd = cd)#
#
transformed_train <- rbm.up(rbm, X_train)#
transformed_test <- rbm.up(rbm, X_test)#
#
nn = nn.train(x=transformed_train, y_train, initW = NULL, initB = NULL, hidden = hidden_nn, activationfun = "sigm", learningrate = learningrate_nn, momentum = 0.5, learningrate_scale = learningrate_scale_nn, output = "sigm", numepochs = numepochs_nn, batchsize = batchsize_nn, hidden_dropout = 0, visible_dropout = 0)#
#
score <- 0#
score <- nn.test(nn, transformed_test, y_test, t = 0.5)#
#
pred <- nn.predict(nn, transformed_test)#
pred_norm <- matrix(0L, nrow=dim(pred)[1], ncol=dim(pred)[2])#
for(i in 1:dim(pred)[1]){#
	max_row <- which.max(pred[i,])#
	pred_norm[i,max_row] <- 1#
}#
#
score_norm <- 0#
for(i in 1:dim(pred_norm)[1]){#
	if(which.max(pred_norm[i,]) == which.max(y_test[i,])){#
		score_norm <- score_norm +1#
	}#
}#
score_norm <- 1- score_norm/dim(pred)[1]#
#
configs <- rbind(configs,data.frame(hidden_rbm=hidden_rbm, numempochs_rbm = numepochs_rbm, batchsize_rbm=batchsize_rbm, lr_rbm=learningrate_rbm, cd=cd, hidden_nn=paste(hidden_nn,collapse=" "), lr_nn=learningrate_nn, numepochs_nn=numepochs_nn, batchsize_nn=batchsize_nn, onehot=onehot, err_score=score, err_score_norm = score_norm))#
#
configs
# run this only once#
if(exists(configs)){#
	library(deepnet)#
	configs = data.frame(hidden_rbm=integer(), numempochs_rbm=integer(), batchsize_rbm=integer(), lr_rbm=numeric(), cd=integer(), hidden_nn=character(), lr_nn=numeric(), numepochs_nn=integer(), batchsize_nn=numeric(), onehot=integer(), err_score=numeric(), err_score_norm=numeric())#
}#
onehot <- 1#
#
hidden_rbm <- 100#
numepochs_rbm <- 10#
batchsize_rbm <- 300#
learningrate_rbm <- 0.001#
learningrate_scale_rbm <- 0.9#
cd <- 10#
#
hidden_nn <- c(10)#
learningrate_nn <- 0.001#
learningrate_scale_nn <- 0.9#
numepochs_nn <- 10#
batchsize_nn <- 300#
#
X_train <- read.csv(file="../X_train.csv", header=T, sep=",", row.names=1)#
X_test <- read.csv(file="../X_test.csv", header=T, sep=",", row.names=1)#
y_train <- read.csv(file="../y_train.csv", header=T, sep=",", row.names=1)#
y_test <- read.csv(file="../y_test.csv", header=T, sep=",", row.names=1)#
#
X_train <- X_train[1:939,]#
X_test <- X_test[1:313,]#
#
# trying one hot encoding#
onehot_test <- matrix(0L, nrow=dim(y_test)[1], ncol=max(y_test)+1)#
counter <- 1#
for(y in 1:dim(y_test)[1]){#
	onehot_test[counter,y_test[y,]] <- 1#
	counter <- counter+1#
}#
# trying one hot encoding#
onehot_train <- matrix(0L, nrow=dim(y_train)[1], ncol=max(y_train)+1)#
counter <- 1#
for(y in 1:dim(y_train)[1]){#
	onehot_train[counter, y_train[y,]+1] <- 1#
	counter <- counter+1#
}#
X_train <- as.matrix(X_train)#
X_test <- as.matrix(X_test)#
y_train <- as.matrix(y_train)#
y_test <- as.matrix(y_test)#
#
if(onehot){#
	y_test <- onehot_test#
	y_train <- onehot_train#
}#
#
rbm <- rbm.train(x=X_train, hidden=hidden_rbm, numepochs = numepochs_rbm, batchsize = batchsize_rbm, learningrate = learningrate_rbm, learningrate_scale = learningrate_scale_rbm, momentum = 0.5, visible_type = "bin", hidden_type = "bin", cd = cd)#
#
transformed_train <- rbm.up(rbm, X_train)#
transformed_test <- rbm.up(rbm, X_test)#
#
nn = nn.train(x=transformed_train, y_train, initW = NULL, initB = NULL, hidden = hidden_nn, activationfun = "sigm", learningrate = learningrate_nn, momentum = 0.5, learningrate_scale = learningrate_scale_nn, output = "sigm", numepochs = numepochs_nn, batchsize = batchsize_nn, hidden_dropout = 0, visible_dropout = 0)#
#
score <- 0#
score <- nn.test(nn, transformed_test, y_test, t = 0.5)#
#
pred <- nn.predict(nn, transformed_test)#
pred_norm <- matrix(0L, nrow=dim(pred)[1], ncol=dim(pred)[2])#
for(i in 1:dim(pred)[1]){#
	max_row <- which.max(pred[i,])#
	pred_norm[i,max_row] <- 1#
}#
#
score_norm <- 0#
for(i in 1:dim(pred_norm)[1]){#
	if(which.max(pred_norm[i,]) == which.max(y_test[i,])){#
		score_norm <- score_norm +1#
	}#
}#
score_norm <- 1- score_norm/dim(pred)[1]#
#
configs <- rbind(configs,data.frame(hidden_rbm=hidden_rbm, numempochs_rbm = numepochs_rbm, batchsize_rbm=batchsize_rbm, lr_rbm=learningrate_rbm, cd=cd, hidden_nn=paste(hidden_nn,collapse=" "), lr_nn=learningrate_nn, numepochs_nn=numepochs_nn, batchsize_nn=batchsize_nn, onehot=onehot, err_score=score, err_score_norm = score_norm))#
#
configs
predict_list <-list()
predict_norm <- list()#
	nn_list <- list()
# run this only once#
if(exists(configs)){#
	library(deepnet)#
	configs = data.frame(hidden_rbm=integer(), numempochs_rbm=integer(), batchsize_rbm=integer(), lr_rbm=numeric(), cd=integer(), hidden_nn=character(), lr_nn=numeric(), numepochs_nn=integer(), batchsize_nn=numeric(), onehot=integer(), err_score=numeric(), err_score_norm=numeric())#
	predict_list <- list()#
	predict_norm <- list()#
	nn_list <- list()#
}#
onehot <- 1#
#
hidden_rbm <- 100#
numepochs_rbm <- 10#
batchsize_rbm <- 300#
learningrate_rbm <- 0.001#
learningrate_scale_rbm <- 0.9#
cd <- 10#
#
hidden_nn <- c(10)#
learningrate_nn <- 0.001#
learningrate_scale_nn <- 0.9#
numepochs_nn <- 10#
batchsize_nn <- 300#
#
X_train <- read.csv(file="../X_train.csv", header=T, sep=",", row.names=1)#
X_test <- read.csv(file="../X_test.csv", header=T, sep=",", row.names=1)#
y_train <- read.csv(file="../y_train.csv", header=T, sep=",", row.names=1)#
y_test <- read.csv(file="../y_test.csv", header=T, sep=",", row.names=1)#
#
X_train <- X_train[1:939,]#
X_test <- X_test[1:313,]#
#
# trying one hot encoding#
onehot_test <- matrix(0L, nrow=dim(y_test)[1], ncol=max(y_test)+1)#
counter <- 1#
for(y in 1:dim(y_test)[1]){#
	onehot_test[counter,y_test[y,]] <- 1#
	counter <- counter+1#
}#
# trying one hot encoding#
onehot_train <- matrix(0L, nrow=dim(y_train)[1], ncol=max(y_train)+1)#
counter <- 1#
for(y in 1:dim(y_train)[1]){#
	onehot_train[counter, y_train[y,]+1] <- 1#
	counter <- counter+1#
}#
X_train <- as.matrix(X_train)#
X_test <- as.matrix(X_test)#
y_train <- as.matrix(y_train)#
y_test <- as.matrix(y_test)#
#
if(onehot){#
	y_test <- onehot_test#
	y_train <- onehot_train#
}#
#
rbm <- rbm.train(x=X_train, hidden=hidden_rbm, numepochs = numepochs_rbm, batchsize = batchsize_rbm, learningrate = learningrate_rbm, learningrate_scale = learningrate_scale_rbm, momentum = 0.5, visible_type = "bin", hidden_type = "bin", cd = cd)#
#
transformed_train <- rbm.up(rbm, X_train)#
transformed_test <- rbm.up(rbm, X_test)#
#
nn = nn.train(x=transformed_train, y_train, initW = NULL, initB = NULL, hidden = hidden_nn, activationfun = "sigm", learningrate = learningrate_nn, momentum = 0.5, learningrate_scale = learningrate_scale_nn, output = "sigm", numepochs = numepochs_nn, batchsize = batchsize_nn, hidden_dropout = 0, visible_dropout = 0)#
#
score <- 0#
score <- nn.test(nn, transformed_test, y_test, t = 0.5)#
#
pred <- nn.predict(nn, transformed_test)#
pred_norm <- matrix(0L, nrow=dim(pred)[1], ncol=dim(pred)[2])#
for(i in 1:dim(pred)[1]){#
	max_row <- which.max(pred[i,])#
	pred_norm[i,max_row] <- 1#
}#
#
score_norm <- 0#
for(i in 1:dim(pred_norm)[1]){#
	if(which.max(pred_norm[i,]) == which.max(y_test[i,])){#
		score_norm <- score_norm +1#
	}#
}#
score_norm <- 1- score_norm/dim(pred)[1]#
#
configs <- rbind(configs,data.frame(hidden_rbm=hidden_rbm, numempochs_rbm = numepochs_rbm, batchsize_rbm=batchsize_rbm, lr_rbm=learningrate_rbm, cd=cd, hidden_nn=paste(hidden_nn,collapse=" "), lr_nn=learningrate_nn, numepochs_nn=numepochs_nn, batchsize_nn=batchsize_nn, onehot=onehot, err_score=score, err_score_norm = score_norm))#
predict_list[dim(configs)[1]] <- pred#
predict_norm[dim(configs)[1]] <- pred_norm#
nn_list[dim(configs)[1]] <- nn#
#
configs
exists(configs)
exists("configs")
# run this only once#
if(exists("configs")){#
	library(deepnet)#
	configs = data.frame(hidden_rbm=integer(), numempochs_rbm=integer(), batchsize_rbm=integer(), lr_rbm=numeric(), cd=integer(), hidden_nn=character(), lr_nn=numeric(), numepochs_nn=integer(), batchsize_nn=numeric(), onehot=integer(), err_score=numeric(), err_score_norm=numeric())#
	predict_list <- list()#
	predict_norm <- list()#
	nn_list <- list()#
}#
onehot <- 1#
#
hidden_rbm <- 100#
numepochs_rbm <- 10#
batchsize_rbm <- 300#
learningrate_rbm <- 0.001#
learningrate_scale_rbm <- 0.9#
cd <- 10#
#
hidden_nn <- c(10)#
learningrate_nn <- 0.001#
learningrate_scale_nn <- 0.9#
numepochs_nn <- 10#
batchsize_nn <- 300#
#
X_train <- read.csv(file="../X_train.csv", header=T, sep=",", row.names=1)#
X_test <- read.csv(file="../X_test.csv", header=T, sep=",", row.names=1)#
y_train <- read.csv(file="../y_train.csv", header=T, sep=",", row.names=1)#
y_test <- read.csv(file="../y_test.csv", header=T, sep=",", row.names=1)#
#
X_train <- X_train[1:939,]#
X_test <- X_test[1:313,]#
#
# trying one hot encoding#
onehot_test <- matrix(0L, nrow=dim(y_test)[1], ncol=max(y_test)+1)#
counter <- 1#
for(y in 1:dim(y_test)[1]){#
	onehot_test[counter,y_test[y,]] <- 1#
	counter <- counter+1#
}#
# trying one hot encoding#
onehot_train <- matrix(0L, nrow=dim(y_train)[1], ncol=max(y_train)+1)#
counter <- 1#
for(y in 1:dim(y_train)[1]){#
	onehot_train[counter, y_train[y,]+1] <- 1#
	counter <- counter+1#
}#
X_train <- as.matrix(X_train)#
X_test <- as.matrix(X_test)#
y_train <- as.matrix(y_train)#
y_test <- as.matrix(y_test)#
#
if(onehot){#
	y_test <- onehot_test#
	y_train <- onehot_train#
}#
#
rbm <- rbm.train(x=X_train, hidden=hidden_rbm, numepochs = numepochs_rbm, batchsize = batchsize_rbm, learningrate = learningrate_rbm, learningrate_scale = learningrate_scale_rbm, momentum = 0.5, visible_type = "bin", hidden_type = "bin", cd = cd)#
#
transformed_train <- rbm.up(rbm, X_train)#
transformed_test <- rbm.up(rbm, X_test)#
#
nn = nn.train(x=transformed_train, y_train, initW = NULL, initB = NULL, hidden = hidden_nn, activationfun = "sigm", learningrate = learningrate_nn, momentum = 0.5, learningrate_scale = learningrate_scale_nn, output = "sigm", numepochs = numepochs_nn, batchsize = batchsize_nn, hidden_dropout = 0, visible_dropout = 0)#
#
score <- 0#
score <- nn.test(nn, transformed_test, y_test, t = 0.5)#
#
pred <- nn.predict(nn, transformed_test)#
pred_norm <- matrix(0L, nrow=dim(pred)[1], ncol=dim(pred)[2])#
for(i in 1:dim(pred)[1]){#
	max_row <- which.max(pred[i,])#
	pred_norm[i,max_row] <- 1#
}#
#
score_norm <- 0#
for(i in 1:dim(pred_norm)[1]){#
	if(which.max(pred_norm[i,]) == which.max(y_test[i,])){#
		score_norm <- score_norm +1#
	}#
}#
score_norm <- 1- score_norm/dim(pred)[1]#
#
configs <- rbind(configs,data.frame(hidden_rbm=hidden_rbm, numempochs_rbm = numepochs_rbm, batchsize_rbm=batchsize_rbm, lr_rbm=learningrate_rbm, cd=cd, hidden_nn=paste(hidden_nn,collapse=" "), lr_nn=learningrate_nn, numepochs_nn=numepochs_nn, batchsize_nn=batchsize_nn, onehot=onehot, err_score=score, err_score_norm = score_norm))#
predict_list[dim(configs)[1]] <- pred#
predict_norm[dim(configs)[1]] <- pred_norm#
nn_list[dim(configs)[1]] <- nn#
#
configs
# run this only once#
if(exists("configs") == F){#
	library(deepnet)#
	configs = data.frame(hidden_rbm=integer(), numempochs_rbm=integer(), batchsize_rbm=integer(), lr_rbm=numeric(), cd=integer(), hidden_nn=character(), lr_nn=numeric(), numepochs_nn=integer(), batchsize_nn=numeric(), onehot=integer(), err_score=numeric(), err_score_norm=numeric())#
	predict_list <- list()#
	predict_norm <- list()#
	nn_list <- list()#
}#
onehot <- 1#
#
hidden_rbm <- 100#
numepochs_rbm <- 10#
batchsize_rbm <- 300#
learningrate_rbm <- 0.001#
learningrate_scale_rbm <- 0.9#
cd <- 10#
#
hidden_nn <- c(10)#
learningrate_nn <- 0.001#
learningrate_scale_nn <- 0.9#
numepochs_nn <- 10#
batchsize_nn <- 300#
#
X_train <- read.csv(file="../X_train.csv", header=T, sep=",", row.names=1)#
X_test <- read.csv(file="../X_test.csv", header=T, sep=",", row.names=1)#
y_train <- read.csv(file="../y_train.csv", header=T, sep=",", row.names=1)#
y_test <- read.csv(file="../y_test.csv", header=T, sep=",", row.names=1)#
#
X_train <- X_train[1:939,]#
X_test <- X_test[1:313,]#
#
# trying one hot encoding#
onehot_test <- matrix(0L, nrow=dim(y_test)[1], ncol=max(y_test)+1)#
counter <- 1#
for(y in 1:dim(y_test)[1]){#
	onehot_test[counter,y_test[y,]] <- 1#
	counter <- counter+1#
}#
# trying one hot encoding#
onehot_train <- matrix(0L, nrow=dim(y_train)[1], ncol=max(y_train)+1)#
counter <- 1#
for(y in 1:dim(y_train)[1]){#
	onehot_train[counter, y_train[y,]+1] <- 1#
	counter <- counter+1#
}#
X_train <- as.matrix(X_train)#
X_test <- as.matrix(X_test)#
y_train <- as.matrix(y_train)#
y_test <- as.matrix(y_test)#
#
if(onehot){#
	y_test <- onehot_test#
	y_train <- onehot_train#
}#
#
rbm <- rbm.train(x=X_train, hidden=hidden_rbm, numepochs = numepochs_rbm, batchsize = batchsize_rbm, learningrate = learningrate_rbm, learningrate_scale = learningrate_scale_rbm, momentum = 0.5, visible_type = "bin", hidden_type = "bin", cd = cd)#
#
transformed_train <- rbm.up(rbm, X_train)#
transformed_test <- rbm.up(rbm, X_test)#
#
nn = nn.train(x=transformed_train, y_train, initW = NULL, initB = NULL, hidden = hidden_nn, activationfun = "sigm", learningrate = learningrate_nn, momentum = 0.5, learningrate_scale = learningrate_scale_nn, output = "sigm", numepochs = numepochs_nn, batchsize = batchsize_nn, hidden_dropout = 0, visible_dropout = 0)#
#
score <- 0#
score <- nn.test(nn, transformed_test, y_test, t = 0.5)#
#
pred <- nn.predict(nn, transformed_test)#
pred_norm <- matrix(0L, nrow=dim(pred)[1], ncol=dim(pred)[2])#
for(i in 1:dim(pred)[1]){#
	max_row <- which.max(pred[i,])#
	pred_norm[i,max_row] <- 1#
}#
#
score_norm <- 0#
for(i in 1:dim(pred_norm)[1]){#
	if(which.max(pred_norm[i,]) == which.max(y_test[i,])){#
		score_norm <- score_norm +1#
	}#
}#
score_norm <- 1- score_norm/dim(pred)[1]#
#
configs <- rbind(configs,data.frame(hidden_rbm=hidden_rbm, numempochs_rbm = numepochs_rbm, batchsize_rbm=batchsize_rbm, lr_rbm=learningrate_rbm, cd=cd, hidden_nn=paste(hidden_nn,collapse=" "), lr_nn=learningrate_nn, numepochs_nn=numepochs_nn, batchsize_nn=batchsize_nn, onehot=onehot, err_score=score, err_score_norm = score_norm))#
predict_list[dim(configs)[1]] <- pred#
predict_norm[dim(configs)[1]] <- pred_norm#
nn_list[dim(configs)[1]] <- nn#
#
configs
predict_list[dim(configs)[1]]
dim(configs)[1]
pred
d = list(pred)
d
dim(d)
# run this only once#
if(exists("configs") == F){#
	library(deepnet)#
	configs = data.frame(hidden_rbm=integer(), numempochs_rbm=integer(), batchsize_rbm=integer(), lr_rbm=numeric(), cd=integer(), hidden_nn=character(), lr_nn=numeric(), numepochs_nn=integer(), batchsize_nn=numeric(), onehot=integer(), err_score=numeric(), err_score_norm=numeric())#
	predict_list <- list()#
	predict_norm <- list()#
	nn_list <- list()#
}#
onehot <- 1#
#
hidden_rbm <- 100#
numepochs_rbm <- 10#
batchsize_rbm <- 100#
learningrate_rbm <- 0.01#
learningrate_scale_rbm <- 0.9#
cd <- 10#
#
hidden_nn <- c(10)#
learningrate_nn <- 0.01#
learningrate_scale_nn <- 0.9#
numepochs_nn <- 10#
batchsize_nn <- 100#
#
X_train <- read.csv(file="../X_train.csv", header=T, sep=",", row.names=1)#
X_test <- read.csv(file="../X_test.csv", header=T, sep=",", row.names=1)#
y_train <- read.csv(file="../y_train.csv", header=T, sep=",", row.names=1)#
y_test <- read.csv(file="../y_test.csv", header=T, sep=",", row.names=1)#
#
X_train <- X_train[1:939,]#
X_test <- X_test[1:313,]#
#
# trying one hot encoding#
onehot_test <- matrix(0L, nrow=dim(y_test)[1], ncol=max(y_test)+1)#
counter <- 1#
for(y in 1:dim(y_test)[1]){#
	onehot_test[counter,y_test[y,]] <- 1#
	counter <- counter+1#
}#
# trying one hot encoding#
onehot_train <- matrix(0L, nrow=dim(y_train)[1], ncol=max(y_train)+1)#
counter <- 1#
for(y in 1:dim(y_train)[1]){#
	onehot_train[counter, y_train[y,]+1] <- 1#
	counter <- counter+1#
}#
X_train <- as.matrix(X_train)#
X_test <- as.matrix(X_test)#
y_train <- as.matrix(y_train)#
y_test <- as.matrix(y_test)#
#
if(onehot){#
	y_test <- onehot_test#
	y_train <- onehot_train#
}#
#
rbm <- rbm.train(x=X_train, hidden=hidden_rbm, numepochs = numepochs_rbm, batchsize = batchsize_rbm, learningrate = learningrate_rbm, learningrate_scale = learningrate_scale_rbm, momentum = 0.5, visible_type = "bin", hidden_type = "bin", cd = cd)#
#
transformed_train <- rbm.up(rbm, X_train)#
transformed_test <- rbm.up(rbm, X_test)#
#
nn = nn.train(x=transformed_train, y_train, initW = NULL, initB = NULL, hidden = hidden_nn, activationfun = "sigm", learningrate = learningrate_nn, momentum = 0.5, learningrate_scale = learningrate_scale_nn, output = "sigm", numepochs = numepochs_nn, batchsize = batchsize_nn, hidden_dropout = 0, visible_dropout = 0)#
#
score <- 0#
score <- nn.test(nn, transformed_test, y_test, t = 0.5)#
#
pred <- nn.predict(nn, transformed_test)#
pred_norm <- matrix(0L, nrow=dim(pred)[1], ncol=dim(pred)[2])#
for(i in 1:dim(pred)[1]){#
	max_row <- which.max(pred[i,])#
	pred_norm[i,max_row] <- 1#
}#
#
score_norm <- 0#
for(i in 1:dim(pred_norm)[1]){#
	if(which.max(pred_norm[i,]) == which.max(y_test[i,])){#
		score_norm <- score_norm +1#
	}#
}#
score_norm <- 1- score_norm/dim(pred)[1]#
#
configs <- rbind(configs,data.frame(hidden_rbm=hidden_rbm, numempochs_rbm = numepochs_rbm, batchsize_rbm=batchsize_rbm, lr_rbm=learningrate_rbm, cd=cd, hidden_nn=paste(hidden_nn,collapse=" "), lr_nn=learningrate_nn, numepochs_nn=numepochs_nn, batchsize_nn=batchsize_nn, onehot=onehot, err_score=score, err_score_norm = score_norm))#
predict_list[dim(configs)[1]] <- pred#
predict_norm[dim(configs)[1]] <- pred_norm#
nn_list[dim(configs)[1]] <- nn#
#
configs
dim(d)
d
d <- list(pred)
dim(d)
# run this only once#
if(exists("configs") == F){#
	library(deepnet)#
	configs = data.frame(hidden_rbm=integer(), numempochs_rbm=integer(), batchsize_rbm=integer(), lr_rbm=numeric(), cd=integer(), hidden_nn=character(), lr_nn=numeric(), numepochs_nn=integer(), batchsize_nn=numeric(), onehot=integer(), err_score=numeric(), err_score_norm=numeric())#
	predict_list <- list()#
	predict_norm <- list()#
	nn_list <- list()#
}#
onehot <- 1#
#
hidden_rbm <- 100#
numepochs_rbm <- 10#
batchsize_rbm <- 100#
learningrate_rbm <- 0.01#
learningrate_scale_rbm <- 0.9#
cd <- 10#
#
hidden_nn <- c(10)#
learningrate_nn <- 0.01#
learningrate_scale_nn <- 0.9#
numepochs_nn <- 10#
batchsize_nn <- 100#
#
X_train <- read.csv(file="../X_train.csv", header=T, sep=",", row.names=1)#
X_test <- read.csv(file="../X_test.csv", header=T, sep=",", row.names=1)#
y_train <- read.csv(file="../y_train.csv", header=T, sep=",", row.names=1)#
y_test <- read.csv(file="../y_test.csv", header=T, sep=",", row.names=1)#
#
X_train <- X_train[1:939,]#
X_test <- X_test[1:313,]#
#
# trying one hot encoding#
onehot_test <- matrix(0L, nrow=dim(y_test)[1], ncol=max(y_test)+1)#
counter <- 1#
for(y in 1:dim(y_test)[1]){#
	onehot_test[counter,y_test[y,]] <- 1#
	counter <- counter+1#
}#
# trying one hot encoding#
onehot_train <- matrix(0L, nrow=dim(y_train)[1], ncol=max(y_train)+1)#
counter <- 1#
for(y in 1:dim(y_train)[1]){#
	onehot_train[counter, y_train[y,]+1] <- 1#
	counter <- counter+1#
}#
X_train <- as.matrix(X_train)#
X_test <- as.matrix(X_test)#
y_train <- as.matrix(y_train)#
y_test <- as.matrix(y_test)#
#
if(onehot){#
	y_test <- onehot_test#
	y_train <- onehot_train#
}#
#
rbm <- rbm.train(x=X_train, hidden=hidden_rbm, numepochs = numepochs_rbm, batchsize = batchsize_rbm, learningrate = learningrate_rbm, learningrate_scale = learningrate_scale_rbm, momentum = 0.5, visible_type = "bin", hidden_type = "bin", cd = cd)#
#
transformed_train <- rbm.up(rbm, X_train)#
transformed_test <- rbm.up(rbm, X_test)#
#
nn = nn.train(x=transformed_train, y_train, initW = NULL, initB = NULL, hidden = hidden_nn, activationfun = "sigm", learningrate = learningrate_nn, momentum = 0.5, learningrate_scale = learningrate_scale_nn, output = "sigm", numepochs = numepochs_nn, batchsize = batchsize_nn, hidden_dropout = 0, visible_dropout = 0)#
#
score <- 0#
score <- nn.test(nn, transformed_test, y_test, t = 0.5)#
#
pred <- nn.predict(nn, transformed_test)#
pred_norm <- matrix(0L, nrow=dim(pred)[1], ncol=dim(pred)[2])#
for(i in 1:dim(pred)[1]){#
	max_row <- which.max(pred[i,])#
	pred_norm[i,max_row] <- 1#
}#
#
score_norm <- 0#
for(i in 1:dim(pred_norm)[1]){#
	if(which.max(pred_norm[i,]) == which.max(y_test[i,])){#
		score_norm <- score_norm +1#
	}#
}#
score_norm <- 1- score_norm/dim(pred)[1]#
#
configs <- rbind(configs,data.frame(hidden_rbm=hidden_rbm, numempochs_rbm = numepochs_rbm, batchsize_rbm=batchsize_rbm, lr_rbm=learningrate_rbm, cd=cd, hidden_nn=paste(hidden_nn,collapse=" "), lr_nn=learningrate_nn, numepochs_nn=numepochs_nn, batchsize_nn=batchsize_nn, onehot=onehot, err_score=score, err_score_norm = score_norm))#
predict_list[[dim(configs)[1]]] <- pred#
predict_norm[[dim(configs)[1]]] <- pred_norm#
nn_list[[dim(configs)[1]]] <- nn#
#
configs
dim(nn_list)
nn_list
lenght(nn_list)
length(nn_list)
# run this only once#
if(exists("configs") == F){#
	library(deepnet)#
	configs = data.frame(hidden_rbm=integer(), numempochs_rbm=integer(), batchsize_rbm=integer(), lr_rbm=numeric(), cd=integer(), hidden_nn=character(), lr_nn=numeric(), numepochs_nn=integer(), batchsize_nn=numeric(), onehot=integer(), err_score=numeric(), err_score_norm=numeric())#
	predict_list <- list()#
	predict_norm <- list()#
	nn_list <- list()#
}#
onehot <- 1#
#
hidden_rbm <- 100#
numepochs_rbm <- 10#
batchsize_rbm <- 100#
learningrate_rbm <- 0.01#
learningrate_scale_rbm <- 0.9#
cd <- 10#
#
hidden_nn <- c(10)#
learningrate_nn <- 0.01#
learningrate_scale_nn <- 0.9#
numepochs_nn <- 10#
batchsize_nn <- 100#
#
X_train <- read.csv(file="../X_train.csv", header=T, sep=",", row.names=1)#
X_test <- read.csv(file="../X_test.csv", header=T, sep=",", row.names=1)#
y_train <- read.csv(file="../y_train.csv", header=T, sep=",", row.names=1)#
y_test <- read.csv(file="../y_test.csv", header=T, sep=",", row.names=1)#
#
X_train <- X_train[1:939,]#
X_test <- X_test[1:313,]#
#
# trying one hot encoding#
onehot_test <- matrix(0L, nrow=dim(y_test)[1], ncol=max(y_test)+1)#
counter <- 1#
for(y in 1:dim(y_test)[1]){#
	onehot_test[counter,y_test[y,]] <- 1#
	counter <- counter+1#
}#
# trying one hot encoding#
onehot_train <- matrix(0L, nrow=dim(y_train)[1], ncol=max(y_train)+1)#
counter <- 1#
for(y in 1:dim(y_train)[1]){#
	onehot_train[counter, y_train[y,]+1] <- 1#
	counter <- counter+1#
}#
X_train <- as.matrix(X_train)#
X_test <- as.matrix(X_test)#
y_train <- as.matrix(y_train)#
y_test <- as.matrix(y_test)#
#
if(onehot){#
	y_test <- onehot_test#
	y_train <- onehot_train#
}#
#
rbm <- rbm.train(x=X_train, hidden=hidden_rbm, numepochs = numepochs_rbm, batchsize = batchsize_rbm, learningrate = learningrate_rbm, learningrate_scale = learningrate_scale_rbm, momentum = 0.5, visible_type = "bin", hidden_type = "bin", cd = cd)#
#
transformed_train <- rbm.up(rbm, X_train)#
transformed_test <- rbm.up(rbm, X_test)#
#
nn = nn.train(x=transformed_train, y_train, initW = NULL, initB = NULL, hidden = hidden_nn, activationfun = "sigm", learningrate = learningrate_nn, momentum = 0.5, learningrate_scale = learningrate_scale_nn, output = "sigm", numepochs = numepochs_nn, batchsize = batchsize_nn, hidden_dropout = 0, visible_dropout = 0)#
#
score <- 0#
score <- nn.test(nn, transformed_test, y_test, t = 0.5)#
#
pred <- nn.predict(nn, transformed_test)#
pred_norm <- matrix(0L, nrow=dim(pred)[1], ncol=dim(pred)[2])#
for(i in 1:dim(pred)[1]){#
	max_row <- which.max(pred[i,])#
	pred_norm[i,max_row] <- 1#
}#
#
score_norm <- 0#
for(i in 1:dim(pred_norm)[1]){#
	if(which.max(pred_norm[i,]) == which.max(y_test[i,])){#
		score_norm <- score_norm +1#
	}#
}#
score_norm <- 1- score_norm/dim(pred)[1]#
#
configs <- rbind(configs,data.frame(hidden_rbm=hidden_rbm, numempochs_rbm = numepochs_rbm, batchsize_rbm=batchsize_rbm, lr_rbm=learningrate_rbm, cd=cd, hidden_nn=paste(hidden_nn,collapse=" "), lr_nn=learningrate_nn, numepochs_nn=numepochs_nn, batchsize_nn=batchsize_nn, onehot=onehot, err_score=score, err_score_norm = score_norm))#
predict_list[[dim(configs)[1]]] <- pred#
predict_norm[[dim(configs)[1]]] <- pred_norm#
nn_list[[dim(configs)[1]]] <- nn#
#
configs
# run this only once#
if(exists("configs") == F){#
	library(deepnet)#
	configs = data.frame(hidden_rbm=integer(), numempochs_rbm=integer(), batchsize_rbm=integer(), lr_rbm=numeric(), cd=integer(), hidden_nn=character(), lr_nn=numeric(), numepochs_nn=integer(), batchsize_nn=numeric(), onehot=integer(), err_score=numeric(), err_score_norm=numeric())#
	predict_list <- list()#
	predict_norm <- list()#
	nn_list <- list()#
}#
onehot <- 1#
#
hidden_rbm <- 100#
numepochs_rbm <- 10#
batchsize_rbm <- 100#
learningrate_rbm <- 0.001#
learningrate_scale_rbm <- 0.9#
cd <- 10#
#
hidden_nn <- c(10)#
learningrate_nn <- 0.001#
learningrate_scale_nn <- 0.9#
numepochs_nn <- 10#
batchsize_nn <- 100#
#
X_train <- read.csv(file="../X_train.csv", header=T, sep=",", row.names=1)#
X_test <- read.csv(file="../X_test.csv", header=T, sep=",", row.names=1)#
y_train <- read.csv(file="../y_train.csv", header=T, sep=",", row.names=1)#
y_test <- read.csv(file="../y_test.csv", header=T, sep=",", row.names=1)#
#
X_train <- X_train[1:939,]#
X_test <- X_test[1:313,]#
#
# trying one hot encoding#
onehot_test <- matrix(0L, nrow=dim(y_test)[1], ncol=max(y_test)+1)#
counter <- 1#
for(y in 1:dim(y_test)[1]){#
	onehot_test[counter,y_test[y,]] <- 1#
	counter <- counter+1#
}#
# trying one hot encoding#
onehot_train <- matrix(0L, nrow=dim(y_train)[1], ncol=max(y_train)+1)#
counter <- 1#
for(y in 1:dim(y_train)[1]){#
	onehot_train[counter, y_train[y,]+1] <- 1#
	counter <- counter+1#
}#
X_train <- as.matrix(X_train)#
X_test <- as.matrix(X_test)#
y_train <- as.matrix(y_train)#
y_test <- as.matrix(y_test)#
#
if(onehot){#
	y_test <- onehot_test#
	y_train <- onehot_train#
}#
#
rbm <- rbm.train(x=X_train, hidden=hidden_rbm, numepochs = numepochs_rbm, batchsize = batchsize_rbm, learningrate = learningrate_rbm, learningrate_scale = learningrate_scale_rbm, momentum = 0.5, visible_type = "bin", hidden_type = "bin", cd = cd)#
#
transformed_train <- rbm.up(rbm, X_train)#
transformed_test <- rbm.up(rbm, X_test)#
#
nn = nn.train(x=transformed_train, y_train, initW = NULL, initB = NULL, hidden = hidden_nn, activationfun = "sigm", learningrate = learningrate_nn, momentum = 0.5, learningrate_scale = learningrate_scale_nn, output = "sigm", numepochs = numepochs_nn, batchsize = batchsize_nn, hidden_dropout = 0, visible_dropout = 0)#
#
score <- 0#
score <- nn.test(nn, transformed_test, y_test, t = 0.5)#
#
pred <- nn.predict(nn, transformed_test)#
pred_norm <- matrix(0L, nrow=dim(pred)[1], ncol=dim(pred)[2])#
for(i in 1:dim(pred)[1]){#
	max_row <- which.max(pred[i,])#
	pred_norm[i,max_row] <- 1#
}#
#
score_norm <- 0#
for(i in 1:dim(pred_norm)[1]){#
	if(which.max(pred_norm[i,]) == which.max(y_test[i,])){#
		score_norm <- score_norm +1#
	}#
}#
score_norm <- 1- score_norm/dim(pred)[1]#
#
configs <- rbind(configs,data.frame(hidden_rbm=hidden_rbm, numempochs_rbm = numepochs_rbm, batchsize_rbm=batchsize_rbm, lr_rbm=learningrate_rbm, cd=cd, hidden_nn=paste(hidden_nn,collapse=" "), lr_nn=learningrate_nn, numepochs_nn=numepochs_nn, batchsize_nn=batchsize_nn, onehot=onehot, err_score=score, err_score_norm = score_norm))#
predict_list[[dim(configs)[1]]] <- pred#
predict_norm[[dim(configs)[1]]] <- pred_norm#
nn_list[[dim(configs)[1]]] <- nn#
#
configs
head(pred_norm_list[8])
predict_norm_list <- list()
# run this only once#
if(exists("configs") == F){#
	library(deepnet)#
	configs = data.frame(hidden_rbm=integer(), numempochs_rbm=integer(), batchsize_rbm=integer(), lr_rbm=numeric(), cd=integer(), hidden_nn=character(), lr_nn=numeric(), numepochs_nn=integer(), batchsize_nn=numeric(), onehot=integer(), err_score=numeric(), err_score_norm=numeric())#
	predict_list <- list()#
	predict_norm_list <- list()#
	nn_list <- list()#
}#
onehot <- 1#
#
hidden_rbm <- 100#
numepochs_rbm <- 10#
batchsize_rbm <- 100#
learningrate_rbm <- 0.001#
learningrate_scale_rbm <- 0.9#
cd <- 10#
#
hidden_nn <- c(10)#
learningrate_nn <- 0.001#
learningrate_scale_nn <- 0.9#
numepochs_nn <- 10#
batchsize_nn <- 100#
#
X_train <- read.csv(file="../X_train.csv", header=T, sep=",", row.names=1)#
X_test <- read.csv(file="../X_test.csv", header=T, sep=",", row.names=1)#
y_train <- read.csv(file="../y_train.csv", header=T, sep=",", row.names=1)#
y_test <- read.csv(file="../y_test.csv", header=T, sep=",", row.names=1)#
#
X_train <- X_train[1:939,]#
X_test <- X_test[1:313,]#
#
# trying one hot encoding#
onehot_test <- matrix(0L, nrow=dim(y_test)[1], ncol=max(y_test)+1)#
counter <- 1#
for(y in 1:dim(y_test)[1]){#
	onehot_test[counter,y_test[y,]] <- 1#
	counter <- counter+1#
}#
# trying one hot encoding#
onehot_train <- matrix(0L, nrow=dim(y_train)[1], ncol=max(y_train)+1)#
counter <- 1#
for(y in 1:dim(y_train)[1]){#
	onehot_train[counter, y_train[y,]+1] <- 1#
	counter <- counter+1#
}#
X_train <- as.matrix(X_train)#
X_test <- as.matrix(X_test)#
y_train <- as.matrix(y_train)#
y_test <- as.matrix(y_test)#
#
if(onehot){#
	y_test <- onehot_test#
	y_train <- onehot_train#
}#
#
rbm <- rbm.train(x=X_train, hidden=hidden_rbm, numepochs = numepochs_rbm, batchsize = batchsize_rbm, learningrate = learningrate_rbm, learningrate_scale = learningrate_scale_rbm, momentum = 0.5, visible_type = "bin", hidden_type = "bin", cd = cd)#
#
transformed_train <- rbm.up(rbm, X_train)#
transformed_test <- rbm.up(rbm, X_test)#
#
nn = nn.train(x=transformed_train, y_train, initW = NULL, initB = NULL, hidden = hidden_nn, activationfun = "sigm", learningrate = learningrate_nn, momentum = 0.5, learningrate_scale = learningrate_scale_nn, output = "sigm", numepochs = numepochs_nn, batchsize = batchsize_nn, hidden_dropout = 0, visible_dropout = 0)#
#
score <- 0#
score <- nn.test(nn, transformed_test, y_test, t = 0.5)#
#
pred <- nn.predict(nn, transformed_test)#
pred_norm <- matrix(0L, nrow=dim(pred)[1], ncol=dim(pred)[2])#
for(i in 1:dim(pred)[1]){#
	max_row <- which.max(pred[i,])#
	pred_norm[i,max_row] <- 1#
}#
#
score_norm <- 0#
for(i in 1:dim(pred_norm)[1]){#
	if(which.max(pred_norm[i,]) == which.max(y_test[i,])){#
		score_norm <- score_norm +1#
	}#
}#
score_norm <- 1- score_norm/dim(pred)[1]#
#
configs <- rbind(configs,data.frame(hidden_rbm=hidden_rbm, numempochs_rbm = numepochs_rbm, batchsize_rbm=batchsize_rbm, lr_rbm=learningrate_rbm, cd=cd, hidden_nn=paste(hidden_nn,collapse=" "), lr_nn=learningrate_nn, numepochs_nn=numepochs_nn, batchsize_nn=batchsize_nn, onehot=onehot, err_score=score, err_score_norm = score_norm))#
predict_list[[dim(configs)[1]]] <- pred#
predict_norm_list[[dim(configs)[1]]] <- pred_norm#
nn_list[[dim(configs)[1]]] <- nn#
#
configs
predict_norm_list[2]
predict_norm_list[4]
predict_norm_list[3]
configs
y_test
sum(y_test[,1])
sum(y_test[,2])
sum(y_test[,3])
sum(y_test[,4])
sum(y_test[,5])
44+163+71+18
dim(y_test)
# run this only once#
if(exists("configs") == F){#
	library(deepnet)#
	configs = data.frame(hidden_rbm=integer(), numempochs_rbm=integer(), batchsize_rbm=integer(), lr_rbm=numeric(), cd=integer(), hidden_nn=character(), lr_nn=numeric(), numepochs_nn=integer(), batchsize_nn=numeric(), onehot=integer(), err_score=numeric(), err_score_norm=numeric())#
	predict_list <- list()#
	predict_norm_list <- list()#
	nn_list <- list()#
}#
onehot <- 1#
#
hidden_rbm <- 100#
numepochs_rbm <- 10#
batchsize_rbm <- 100#
learningrate_rbm <- 0.001#
learningrate_scale_rbm <- 0.9#
cd <- 10#
#
hidden_nn <- c(10)#
learningrate_nn <- 0.001#
learningrate_scale_nn <- 0.9#
numepochs_nn <- 10#
batchsize_nn <- 100#
#
X_train <- read.csv(file="../X_train.csv", header=T, sep=",", row.names=1)#
X_test_org <- read.csv(file="../X_test.csv", header=T, sep=",", row.names=1)#
y_train <- read.csv(file="../y_train.csv", header=T, sep=",", row.names=1)#
y_test_org <- read.csv(file="../y_test.csv", header=T, sep=",", row.names=1)#
#
X_train <- X_train[1:939,]#
X_test <- X_test[1:313,]#
#
# trying one hot encoding#
onehot_test <- matrix(0L, nrow=dim(y_test_org)[1], ncol=max(y_test_org)+1)#
counter <- 1#
for(y in 1:dim(y_test_org)[1]){#
	onehot_test[counter,y_test_org[y,]] <- 1#
	counter <- counter+1#
}#
# trying one hot encoding#
onehot_train <- matrix(0L, nrow=dim(y_train_org)[1], ncol=max(y_train_org)+1)#
counter <- 1#
for(y in 1:dim(y_train_org)[1]){#
	onehot_train[counter, y_train_org[y,]+1] <- 1#
	counter <- counter+1#
}#
X_train <- as.matrix(X_train)#
X_test <- as.matrix(X_test)#
y_train <- as.matrix(y_train_org)#
y_test <- as.matrix(y_test_org)#
#
if(onehot){#
	y_test <- onehot_test#
	y_train <- onehot_train#
}#
#
rbm <- rbm.train(x=X_train, hidden=hidden_rbm, numepochs = numepochs_rbm, batchsize = batchsize_rbm, learningrate = learningrate_rbm, learningrate_scale = learningrate_scale_rbm, momentum = 0.5, visible_type = "bin", hidden_type = "bin", cd = cd)#
#
transformed_train <- rbm.up(rbm, X_train)#
transformed_test <- rbm.up(rbm, X_test)#
#
nn = nn.train(x=transformed_train, y_train, initW = NULL, initB = NULL, hidden = hidden_nn, activationfun = "sigm", learningrate = learningrate_nn, momentum = 0.5, learningrate_scale = learningrate_scale_nn, output = "sigm", numepochs = numepochs_nn, batchsize = batchsize_nn, hidden_dropout = 0, visible_dropout = 0)#
#
score <- 0#
score <- nn.test(nn, transformed_test, y_test, t = 0.5)#
#
pred <- nn.predict(nn, transformed_test)#
pred_norm <- matrix(0L, nrow=dim(pred)[1], ncol=dim(pred)[2])#
for(i in 1:dim(pred)[1]){#
	max_row <- which.max(pred[i,])#
	pred_norm[i,max_row] <- 1#
}#
#
score_norm <- 0#
for(i in 1:dim(pred_norm)[1]){#
	if(which.max(pred_norm[i,]) == which.max(y_test[i,])){#
		score_norm <- score_norm +1#
	}#
}#
score_norm <- 1- score_norm/dim(pred)[1]#
#
configs <- rbind(configs,data.frame(hidden_rbm=hidden_rbm, numempochs_rbm = numepochs_rbm, batchsize_rbm=batchsize_rbm, lr_rbm=learningrate_rbm, cd=cd, hidden_nn=paste(hidden_nn,collapse=" "), lr_nn=learningrate_nn, numepochs_nn=numepochs_nn, batchsize_nn=batchsize_nn, onehot=onehot, err_score=score, err_score_norm = score_norm))#
predict_list[[dim(configs)[1]]] <- pred#
predict_norm_list[[dim(configs)[1]]] <- pred_norm#
nn_list[[dim(configs)[1]]] <- nn#
#
configs
# run this only once#
if(exists("configs") == F){#
	library(deepnet)#
	configs = data.frame(hidden_rbm=integer(), numempochs_rbm=integer(), batchsize_rbm=integer(), lr_rbm=numeric(), cd=integer(), hidden_nn=character(), lr_nn=numeric(), numepochs_nn=integer(), batchsize_nn=numeric(), onehot=integer(), err_score=numeric(), err_score_norm=numeric())#
	predict_list <- list()#
	predict_norm_list <- list()#
	nn_list <- list()#
}#
onehot <- 1#
#
hidden_rbm <- 100#
numepochs_rbm <- 10#
batchsize_rbm <- 100#
learningrate_rbm <- 0.001#
learningrate_scale_rbm <- 0.9#
cd <- 10#
#
hidden_nn <- c(10)#
learningrate_nn <- 0.001#
learningrate_scale_nn <- 0.9#
numepochs_nn <- 10#
batchsize_nn <- 100#
#
X_train <- read.csv(file="../X_train.csv", header=T, sep=",", row.names=1)#
X_test <- read.csv(file="../X_test.csv", header=T, sep=",", row.names=1)#
y_train_org <- read.csv(file="../y_train.csv", header=T, sep=",", row.names=1)#
y_test_org <- read.csv(file="../y_test.csv", header=T, sep=",", row.names=1)#
#
X_train <- X_train[1:939,]#
X_test <- X_test[1:313,]#
#
# trying one hot encoding#
onehot_test <- matrix(0L, nrow=dim(y_test_org)[1], ncol=max(y_test_org)+1)#
counter <- 1#
for(y in 1:dim(y_test_org)[1]){#
	onehot_test[counter,y_test_org[y,]] <- 1#
	counter <- counter+1#
}#
# trying one hot encoding#
onehot_train <- matrix(0L, nrow=dim(y_train_org)[1], ncol=max(y_train_org)+1)#
counter <- 1#
for(y in 1:dim(y_train_org)[1]){#
	onehot_train[counter, y_train_org[y,]+1] <- 1#
	counter <- counter+1#
}#
X_train <- as.matrix(X_train)#
X_test <- as.matrix(X_test)#
y_train <- as.matrix(y_train_org)#
y_test <- as.matrix(y_test_org)#
#
if(onehot){#
	y_test <- onehot_test#
	y_train <- onehot_train#
}#
#
rbm <- rbm.train(x=X_train, hidden=hidden_rbm, numepochs = numepochs_rbm, batchsize = batchsize_rbm, learningrate = learningrate_rbm, learningrate_scale = learningrate_scale_rbm, momentum = 0.5, visible_type = "bin", hidden_type = "bin", cd = cd)#
#
transformed_train <- rbm.up(rbm, X_train)#
transformed_test <- rbm.up(rbm, X_test)#
#
nn = nn.train(x=transformed_train, y_train, initW = NULL, initB = NULL, hidden = hidden_nn, activationfun = "sigm", learningrate = learningrate_nn, momentum = 0.5, learningrate_scale = learningrate_scale_nn, output = "sigm", numepochs = numepochs_nn, batchsize = batchsize_nn, hidden_dropout = 0, visible_dropout = 0)#
#
score <- 0#
score <- nn.test(nn, transformed_test, y_test, t = 0.5)#
#
pred <- nn.predict(nn, transformed_test)#
pred_norm <- matrix(0L, nrow=dim(pred)[1], ncol=dim(pred)[2])#
for(i in 1:dim(pred)[1]){#
	max_row <- which.max(pred[i,])#
	pred_norm[i,max_row] <- 1#
}#
#
score_norm <- 0#
for(i in 1:dim(pred_norm)[1]){#
	if(which.max(pred_norm[i,]) == which.max(y_test[i,])){#
		score_norm <- score_norm +1#
	}#
}#
score_norm <- 1- score_norm/dim(pred)[1]#
#
configs <- rbind(configs,data.frame(hidden_rbm=hidden_rbm, numempochs_rbm = numepochs_rbm, batchsize_rbm=batchsize_rbm, lr_rbm=learningrate_rbm, cd=cd, hidden_nn=paste(hidden_nn,collapse=" "), lr_nn=learningrate_nn, numepochs_nn=numepochs_nn, batchsize_nn=batchsize_nn, onehot=onehot, err_score=score, err_score_norm = score_norm))#
predict_list[[dim(configs)[1]]] <- pred#
predict_norm_list[[dim(configs)[1]]] <- pred_norm#
nn_list[[dim(configs)[1]]] <- nn#
#
configs
y_test
sum(y_test)
sum(y_test[,1])
sum(y_test[,0])
sum(y_test[,1])
sum(y_test[,2])
sum(y_test[,3])
sum(y_test[,4])
sum(y_test[,5])
dim(y_test)
18+71+163+44
296-313
y_test_org
# run this only once#
if(exists("configs") == F){#
	library(deepnet)#
	configs = data.frame(hidden_rbm=integer(), numempochs_rbm=integer(), batchsize_rbm=integer(), lr_rbm=numeric(), cd=integer(), hidden_nn=character(), lr_nn=numeric(), numepochs_nn=integer(), batchsize_nn=numeric(), onehot=integer(), err_score=numeric(), err_score_norm=numeric())#
	predict_list <- list()#
	predict_norm_list <- list()#
	nn_list <- list()#
}#
onehot <- 1#
#
hidden_rbm <- 100#
numepochs_rbm <- 10#
batchsize_rbm <- 100#
learningrate_rbm <- 0.001#
learningrate_scale_rbm <- 0.9#
cd <- 10#
#
hidden_nn <- c(10)#
learningrate_nn <- 0.001#
learningrate_scale_nn <- 0.9#
numepochs_nn <- 10#
batchsize_nn <- 100#
#
X_train <- read.csv(file="../X_train.csv", header=T, sep=",", row.names=1)#
X_test <- read.csv(file="../X_test.csv", header=T, sep=",", row.names=1)#
y_train_org <- read.csv(file="../y_train.csv", header=T, sep=",", row.names=1)#
y_test_org <- read.csv(file="../y_test.csv", header=T, sep=",", row.names=1)#
#
X_train <- X_train[1:939,]#
X_test <- X_test[1:313,]#
#
# trying one hot encoding#
onehot_test <- matrix(0L, nrow=dim(y_test_org)[1], ncol=max(y_test_org)+1)#
counter <- 1#
for(y in 1:dim(y_test_org)[1]){#
	onehot_test[counter,y_test_org[y,]+1] <- 1#
	counter <- counter+1#
}#
# trying one hot encoding#
onehot_train <- matrix(0L, nrow=dim(y_train_org)[1], ncol=max(y_train_org)+1)#
counter <- 1#
for(y in 1:dim(y_train_org)[1]){#
	onehot_train[counter, y_train_org[y,]+1] <- 1#
	counter <- counter+1#
}#
X_train <- as.matrix(X_train)#
X_test <- as.matrix(X_test)#
y_train <- as.matrix(y_train_org)#
y_test <- as.matrix(y_test_org)#
#
if(onehot){#
	y_test <- onehot_test#
	y_train <- onehot_train#
}#
#
rbm <- rbm.train(x=X_train, hidden=hidden_rbm, numepochs = numepochs_rbm, batchsize = batchsize_rbm, learningrate = learningrate_rbm, learningrate_scale = learningrate_scale_rbm, momentum = 0.5, visible_type = "bin", hidden_type = "bin", cd = cd)#
#
transformed_train <- rbm.up(rbm, X_train)#
transformed_test <- rbm.up(rbm, X_test)#
#
nn = nn.train(x=transformed_train, y_train, initW = NULL, initB = NULL, hidden = hidden_nn, activationfun = "sigm", learningrate = learningrate_nn, momentum = 0.5, learningrate_scale = learningrate_scale_nn, output = "sigm", numepochs = numepochs_nn, batchsize = batchsize_nn, hidden_dropout = 0, visible_dropout = 0)#
#
score <- 0#
score <- nn.test(nn, transformed_test, y_test, t = 0.5)#
#
pred <- nn.predict(nn, transformed_test)#
pred_norm <- matrix(0L, nrow=dim(pred)[1], ncol=dim(pred)[2])#
for(i in 1:dim(pred)[1]){#
	max_row <- which.max(pred[i,])#
	pred_norm[i,max_row] <- 1#
}#
#
score_norm <- 0#
for(i in 1:dim(pred_norm)[1]){#
	if(which.max(pred_norm[i,]) == which.max(y_test[i,])){#
		score_norm <- score_norm +1#
	}#
}#
score_norm <- 1- score_norm/dim(pred)[1]#
#
configs <- rbind(configs,data.frame(hidden_rbm=hidden_rbm, numempochs_rbm = numepochs_rbm, batchsize_rbm=batchsize_rbm, lr_rbm=learningrate_rbm, cd=cd, hidden_nn=paste(hidden_nn,collapse=" "), lr_nn=learningrate_nn, numepochs_nn=numepochs_nn, batchsize_nn=batchsize_nn, onehot=onehot, err_score=score, err_score_norm = score_norm))#
predict_list[[dim(configs)[1]]] <- pred#
predict_norm_list[[dim(configs)[1]]] <- pred_norm#
nn_list[[dim(configs)[1]]] <- nn#
#
configs
sum(y_test[,1])
sum(y_test[,2])
sum(y_test[,3])
sum(y_test[,4])
sum(y_test[,5])
17+44+163+71+18
# run this only once#
if(exists("configs") == F){#
	library(deepnet)#
	configs = data.frame(hidden_rbm=integer(), numempochs_rbm=integer(), batchsize_rbm=integer(), lr_rbm=numeric(), cd=integer(), hidden_nn=character(), lr_nn=numeric(), numepochs_nn=integer(), batchsize_nn=numeric(), onehot=integer(), err_score=numeric(), err_score_norm=numeric())#
	predict_list <- list()#
	predict_norm_list <- list()#
	nn_list <- list()#
}#
onehot <- 1#
#
hidden_rbm <- 100#
numepochs_rbm <- 10#
batchsize_rbm <- 100#
learningrate_rbm <- 0.001#
learningrate_scale_rbm <- 0.9#
cd <- 10#
#
hidden_nn <- c(10)#
learningrate_nn <- 0.001#
learningrate_scale_nn <- 0.9#
numepochs_nn <- 10#
batchsize_nn <- 100#
#
X_train <- read.csv(file="../X_train.csv", header=T, sep=",", row.names=1)#
X_test <- read.csv(file="../X_test.csv", header=T, sep=",", row.names=1)#
y_train_org <- read.csv(file="../y_train.csv", header=T, sep=",", row.names=1)#
y_test_org <- read.csv(file="../y_test.csv", header=T, sep=",", row.names=1)#
#
X_train <- X_train[1:939,]#
X_test <- X_test[1:313,]#
#
# trying one hot encoding#
onehot_test <- matrix(0L, nrow=dim(y_test_org)[1], ncol=max(y_test_org)+1)#
counter <- 1#
for(y in 1:dim(y_test_org)[1]){#
	onehot_test[counter,y_test_org[y,]+1] <- 1#
	counter <- counter+1#
}#
# trying one hot encoding#
onehot_train <- matrix(0L, nrow=dim(y_train_org)[1], ncol=max(y_train_org)+1)#
counter <- 1#
for(y in 1:dim(y_train_org)[1]){#
	onehot_train[counter, y_train_org[y,]+1] <- 1#
	counter <- counter+1#
}#
X_train <- as.matrix(X_train)#
X_test <- as.matrix(X_test)#
y_train <- as.matrix(y_train_org)#
y_test <- as.matrix(y_test_org)#
#
if(onehot){#
	y_test <- onehot_test#
	y_train <- onehot_train#
}#
#
rbm <- rbm.train(x=X_train, hidden=hidden_rbm, numepochs = numepochs_rbm, batchsize = batchsize_rbm, learningrate = learningrate_rbm, learningrate_scale = learningrate_scale_rbm, momentum = 0.5, visible_type = "bin", hidden_type = "bin", cd = cd)#
#
transformed_train <- rbm.up(rbm, X_train)#
transformed_test <- rbm.up(rbm, X_test)#
#
nn = nn.train(x=transformed_train, y_train, initW = NULL, initB = NULL, hidden = hidden_nn, activationfun = "sigm", learningrate = learningrate_nn, momentum = 0.5, learningrate_scale = learningrate_scale_nn, output = "sigm", numepochs = numepochs_nn, batchsize = batchsize_nn, hidden_dropout = 0, visible_dropout = 0)#
#
score <- 0#
score <- nn.test(nn, transformed_test, y_test, t = 0.5)#
#
pred <- nn.predict(nn, transformed_test)#
pred_norm <- matrix(0L, nrow=dim(pred)[1], ncol=dim(pred)[2])#
for(i in 1:dim(pred)[1]){#
	max_row <- which.max(pred[i,])#
	pred_norm[i,max_row] <- 1#
}#
#
score_norm <- 0#
for(i in 1:dim(pred_norm)[1]){#
	if(which.max(pred_norm[i,]) == which.max(y_test[i,])){#
		score_norm <- score_norm +1#
	}#
}#
score_norm <- 1- score_norm/dim(pred)[1]#
#
configs <- rbind(configs,data.frame(hidden_rbm=hidden_rbm, numempochs_rbm = numepochs_rbm, batchsize_rbm=batchsize_rbm, lr_rbm=learningrate_rbm, cd=cd, hidden_nn=paste(hidden_nn,collapse=" "), lr_nn=learningrate_nn, numepochs_nn=numepochs_nn, batchsize_nn=batchsize_nn, onehot=onehot, err_score=score, err_score_norm = score_norm))#
predict_list[[dim(configs)[1]]] <- pred#
predict_norm_list[[dim(configs)[1]]] <- pred_norm#
nn_list[[dim(configs)[1]]] <- nn#
#
configs
predict_norm_list[4]
predict_norm_list[1]
# run this only once#
if(exists("configs") == F){#
	library(deepnet)#
	configs = data.frame(hidden_rbm=integer(), numempochs_rbm=integer(), batchsize_rbm=integer(), lr_rbm=numeric(), cd=integer(), hidden_nn=character(), lr_nn=numeric(), numepochs_nn=integer(), batchsize_nn=numeric(), onehot=integer(), err_score=numeric(), err_score_norm=numeric())#
	predict_list <- list()#
	predict_norm_list <- list()#
	nn_list <- list()#
}#
onehot <- 1#
#
hidden_rbm <- 100#
numepochs_rbm <- 10#
batchsize_rbm <- 100#
learningrate_rbm <- 0.001#
learningrate_scale_rbm <- 0.9#
cd <- 10#
#
hidden_nn <- c(10)#
learningrate_nn <- 0.001#
learningrate_scale_nn <- 0.9#
numepochs_nn <- 10#
batchsize_nn <- 100#
#
X_train <- read.csv(file="../X_train.csv", header=T, sep=",", row.names=1)#
X_test <- read.csv(file="../X_test.csv", header=T, sep=",", row.names=1)#
y_train_org <- read.csv(file="../y_train.csv", header=T, sep=",", row.names=1)#
y_test_org <- read.csv(file="../y_test.csv", header=T, sep=",", row.names=1)#
#
X_train <- X_train[1:939,]#
X_test <- X_test[1:313,]#
#
# trying one hot encoding#
onehot_test <- matrix(0L, nrow=dim(y_test_org)[1], ncol=max(y_test_org)+1)#
counter <- 1#
for(y in 1:dim(y_test_org)[1]){#
	onehot_test[counter,y_test_org[y,]+1] <- 1#
	counter <- counter+1#
}#
# trying one hot encoding#
onehot_train <- matrix(0L, nrow=dim(y_train_org)[1], ncol=max(y_train_org)+1)#
counter <- 1#
for(y in 1:dim(y_train_org)[1]){#
	onehot_train[counter, y_train_org[y,]+1] <- 1#
	counter <- counter+1#
}#
X_train <- as.matrix(X_train)#
X_test <- as.matrix(X_test)#
y_train <- as.matrix(y_train_org)#
y_test <- as.matrix(y_test_org)#
#
if(onehot){#
	y_test <- onehot_test#
	y_train <- onehot_train#
}#
#
rbm <- rbm.train(x=X_train, hidden=hidden_rbm, numepochs = numepochs_rbm, batchsize = batchsize_rbm, learningrate = learningrate_rbm, learningrate_scale = learningrate_scale_rbm, momentum = 0.5, visible_type = "bin", hidden_type = "bin", cd = cd)#
#
transformed_train <- rbm.up(rbm, X_train)#
transformed_test <- rbm.up(rbm, X_test)#
#
nn = nn.train(x=transformed_train, y_train, initW = NULL, initB = NULL, hidden = hidden_nn, activationfun = "sigm", learningrate = learningrate_nn, momentum = 0.5, learningrate_scale = learningrate_scale_nn, output = "sigm", numepochs = numepochs_nn, batchsize = batchsize_nn, hidden_dropout = 0, visible_dropout = 0)#
#
score <- 0#
score <- nn.test(nn, transformed_test, y_test, t = 0.5)#
#
pred <- nn.predict(nn, transformed_test)#
pred_norm <- matrix(0L, nrow=dim(pred)[1], ncol=dim(pred)[2])#
for(i in 1:dim(pred)[1]){#
	max_row <- which.max(pred[i,])#
	pred_norm[i,max_row] <- 1#
}#
#
score_norm <- 0#
for(i in 1:dim(pred_norm)[1]){#
	if(which.max(pred_norm[i,]) == which.max(y_test[i,])){#
		score_norm <- score_norm +1#
	}#
}#
score_norm <- 1- score_norm/dim(pred)[1]#
#
configs <- rbind(configs,data.frame(hidden_rbm=hidden_rbm, numempochs_rbm = numepochs_rbm, batchsize_rbm=batchsize_rbm, lr_rbm=learningrate_rbm, cd=cd, hidden_nn=paste(hidden_nn,collapse=" "), lr_nn=learningrate_nn, numepochs_nn=numepochs_nn, batchsize_nn=batchsize_nn, onehot=onehot, err_score=score, err_score_norm = score_norm))#
predict_list[[dim(configs)[1]]] <- pred#
predict_norm_list[[dim(configs)[1]]] <- pred_norm#
nn_list[[dim(configs)[1]]] <- nn#
#
configs
predict_norm_list[1]
predict_norm_list[2]
configs
# run this only once#
if(exists("configs") == F){#
	library(deepnet)#
	configs = data.frame(hidden_rbm=integer(), numempochs_rbm=integer(), batchsize_rbm=integer(), lr_rbm=numeric(), cd=integer(), hidden_nn=character(), lr_nn=numeric(), numepochs_nn=integer(), batchsize_nn=numeric(), onehot=integer(), err_score=numeric(), err_score_norm=numeric())#
	predict_list <- list()#
	predict_norm_list <- list()#
	nn_list <- list()#
}#
onehot <- 1#
#
hidden_rbm <- 100#
numepochs_rbm <- 10#
batchsize_rbm <- 100#
learningrate_rbm <- 0.001#
learningrate_scale_rbm <- 0.9#
cd <- 10#
#
hidden_nn <- c(10)#
learningrate_nn <- 0.001#
learningrate_scale_nn <- 0.9#
numepochs_nn <- 10#
batchsize_nn <- 100#
#
X_train <- read.csv(file="../X_train.csv", header=T, sep=",", row.names=1)#
X_test <- read.csv(file="../X_test.csv", header=T, sep=",", row.names=1)#
y_train_org <- read.csv(file="../y_train.csv", header=T, sep=",", row.names=1)#
y_test_org <- read.csv(file="../y_test.csv", header=T, sep=",", row.names=1)#
#
X_train <- X_train[1:939,]#
X_test <- X_test[1:313,]#
#
# trying one hot encoding#
onehot_test <- matrix(0L, nrow=dim(y_test_org)[1], ncol=max(y_test_org)+1)#
counter <- 1#
for(y in 1:dim(y_test_org)[1]){#
	onehot_test[counter,y_test_org[y,]+1] <- 1#
	counter <- counter+1#
}#
# trying one hot encoding#
onehot_train <- matrix(0L, nrow=dim(y_train_org)[1], ncol=max(y_train_org)+1)#
counter <- 1#
for(y in 1:dim(y_train_org)[1]){#
	onehot_train[counter, y_train_org[y,]+1] <- 1#
	counter <- counter+1#
}#
X_train <- as.matrix(X_train)#
X_test <- as.matrix(X_test)#
y_train <- as.matrix(y_train_org)#
y_test <- as.matrix(y_test_org)#
#
if(onehot){#
	y_test <- onehot_test#
	y_train <- onehot_train#
}#
#
rbm <- rbm.train(x=X_train, hidden=hidden_rbm, numepochs = numepochs_rbm, batchsize = batchsize_rbm, learningrate = learningrate_rbm, learningrate_scale = learningrate_scale_rbm, momentum = 0.5, visible_type = "bin", hidden_type = "bin", cd = cd)#
#
transformed_train <- rbm.up(rbm, X_train)#
transformed_test <- rbm.up(rbm, X_test)#
#
nn = nn.train(x=transformed_train, y_train, initW = NULL, initB = NULL, hidden = hidden_nn, activationfun = "sigm", learningrate = learningrate_nn, momentum = 0.5, learningrate_scale = learningrate_scale_nn, output = "sigm", numepochs = numepochs_nn, batchsize = batchsize_nn, hidden_dropout = 0, visible_dropout = 0)#
#
score <- 0#
score <- nn.test(nn, transformed_test, y_test, t = 0.5)#
#
pred <- nn.predict(nn, transformed_test)#
pred_norm <- matrix(0L, nrow=dim(pred)[1], ncol=dim(pred)[2])#
for(i in 1:dim(pred)[1]){#
	max_row <- which.max(pred[i,])#
	pred_norm[i,max_row] <- 1#
}#
#
score_norm <- 0#
for(i in 1:dim(pred_norm)[1]){#
	if(which.max(pred_norm[i,]) == which.max(y_test[i,])){#
		score_norm <- score_norm +1#
	}#
}#
score_norm <- 1- score_norm/dim(pred)[1]#
#
configs <- rbind(configs,data.frame(hidden_rbm=hidden_rbm, numempochs_rbm = numepochs_rbm, batchsize_rbm=batchsize_rbm, lr_rbm=learningrate_rbm, cd=cd, hidden_nn=paste(hidden_nn,collapse=" "), lr_nn=learningrate_nn, numepochs_nn=numepochs_nn, batchsize_nn=batchsize_nn, onehot=onehot, err_score=score, err_score_norm = score_norm))#
predict_list[[dim(configs)[1]]] <- pred#
predict_norm_list[[dim(configs)[1]]] <- pred_norm#
nn_list[[dim(configs)[1]]] <- nn#
#
configs
predict_norm_list[3]
predict_norm_list[2]
predict_norm_list[1]
# run this only once#
if(exists("configs") == F){#
	library(deepnet)#
	configs = data.frame(hidden_rbm=integer(), numempochs_rbm=integer(), batchsize_rbm=integer(), lr_rbm=numeric(), cd=integer(), hidden_nn=character(), lr_nn=numeric(), numepochs_nn=integer(), batchsize_nn=numeric(), onehot=integer(), err_score=numeric(), err_score_norm=numeric())#
	predict_list <- list()#
	predict_norm_list <- list()#
	nn_list <- list()#
}#
onehot <- 1#
#
hidden_rbm <- 100#
numepochs_rbm <- 10#
batchsize_rbm <- 100#
learningrate_rbm <- 0.001#
learningrate_scale_rbm <- 0.9#
cd <- 10#
#
hidden_nn <- c(10)#
learningrate_nn <- 0.001#
learningrate_scale_nn <- 0.9#
numepochs_nn <- 10#
batchsize_nn <- 100#
#
X_train <- read.csv(file="../X_train.csv", header=T, sep=",", row.names=1)#
X_test <- read.csv(file="../X_test.csv", header=T, sep=",", row.names=1)#
y_train_org <- read.csv(file="../y_train.csv", header=T, sep=",", row.names=1)#
y_test_org <- read.csv(file="../y_test.csv", header=T, sep=",", row.names=1)#
#
X_train <- X_train[1:939,]#
X_test <- X_test[1:313,]#
#
# trying one hot encoding#
onehot_test <- matrix(0L, nrow=dim(y_test_org)[1], ncol=max(y_test_org)+1)#
counter <- 1#
for(y in 1:dim(y_test_org)[1]){#
	onehot_test[counter,y_test_org[y,]+1] <- 1#
	counter <- counter+1#
}#
# trying one hot encoding#
onehot_train <- matrix(0L, nrow=dim(y_train_org)[1], ncol=max(y_train_org)+1)#
counter <- 1#
for(y in 1:dim(y_train_org)[1]){#
	onehot_train[counter, y_train_org[y,]+1] <- 1#
	counter <- counter+1#
}#
X_train <- as.matrix(X_train)#
X_test <- as.matrix(X_test)#
y_train <- as.matrix(y_train_org)#
y_test <- as.matrix(y_test_org)#
#
if(onehot){#
	y_test <- onehot_test#
	y_train <- onehot_train#
}#
#
rbm <- rbm.train(x=X_train, hidden=hidden_rbm, numepochs = numepochs_rbm, batchsize = batchsize_rbm, learningrate = learningrate_rbm, learningrate_scale = learningrate_scale_rbm, momentum = 0.5, visible_type = "bin", hidden_type = "bin", cd = cd)#
#
transformed_train <- rbm.up(rbm, X_train)#
transformed_test <- rbm.up(rbm, X_test)#
#
nn = nn.train(x=transformed_train, y_train, initW = NULL, initB = NULL, hidden = hidden_nn, activationfun = "sigm", learningrate = learningrate_nn, momentum = 0.5, learningrate_scale = learningrate_scale_nn, output = "sigm", numepochs = numepochs_nn, batchsize = batchsize_nn, hidden_dropout = 0, visible_dropout = 0)#
#
score <- 0#
score <- nn.test(nn, transformed_test, y_test, t = 0.5)#
#
pred <- nn.predict(nn, transformed_test)#
pred_norm <- matrix(0L, nrow=dim(pred)[1], ncol=dim(pred)[2])#
for(i in 1:dim(pred)[1]){#
	max_row <- which.max(pred[i,])#
	pred_norm[i,max_row] <- 1#
}#
#
score_norm <- 0#
for(i in 1:dim(pred_norm)[1]){#
	if(which.max(pred_norm[i,]) == which.max(y_test[i,])){#
		score_norm <- score_norm +1#
	}#
}#
score_norm <- 1- score_norm/dim(pred)[1]#
#
configs <- rbind(configs,data.frame(hidden_rbm=hidden_rbm, numempochs_rbm = numepochs_rbm, batchsize_rbm=batchsize_rbm, lr_rbm=learningrate_rbm, cd=cd, hidden_nn=paste(hidden_nn,collapse=" "), lr_nn=learningrate_nn, numepochs_nn=numepochs_nn, batchsize_nn=batchsize_nn, onehot=onehot, err_score=score, err_score_norm = score_norm))#
predict_list[[dim(configs)[1]]] <- pred#
predict_norm_list[[dim(configs)[1]]] <- pred_norm#
nn_list[[dim(configs)[1]]] <- nn#
#
configs
# run this only once#
if(exists("configs") == F){#
	library(deepnet)#
	configs = data.frame(hidden_rbm=integer(), numempochs_rbm=integer(), batchsize_rbm=integer(), lr_rbm=numeric(), cd=integer(), hidden_nn=character(), lr_nn=numeric(), numepochs_nn=integer(), batchsize_nn=numeric(), onehot=integer(), err_score=numeric(), err_score_norm=numeric())#
	predict_list <- list()#
	predict_norm_list <- list()#
	nn_list <- list()#
}#
onehot <- 1#
#
hidden_rbm <- 100#
numepochs_rbm <- 10#
batchsize_rbm <- 100#
learningrate_rbm <- 0.001#
learningrate_scale_rbm <- 0.9#
cd <- 10#
#
hidden_nn <- c(100)#
learningrate_nn <- 0.001#
learningrate_scale_nn <- 0.9#
numepochs_nn <- 10#
batchsize_nn <- 100#
#
X_train <- read.csv(file="../X_train.csv", header=T, sep=",", row.names=1)#
X_test <- read.csv(file="../X_test.csv", header=T, sep=",", row.names=1)#
y_train_org <- read.csv(file="../y_train.csv", header=T, sep=",", row.names=1)#
y_test_org <- read.csv(file="../y_test.csv", header=T, sep=",", row.names=1)#
#
X_train <- X_train[1:939,]#
X_test <- X_test[1:313,]#
#
# trying one hot encoding#
onehot_test <- matrix(0L, nrow=dim(y_test_org)[1], ncol=max(y_test_org)+1)#
counter <- 1#
for(y in 1:dim(y_test_org)[1]){#
	onehot_test[counter,y_test_org[y,]+1] <- 1#
	counter <- counter+1#
}#
# trying one hot encoding#
onehot_train <- matrix(0L, nrow=dim(y_train_org)[1], ncol=max(y_train_org)+1)#
counter <- 1#
for(y in 1:dim(y_train_org)[1]){#
	onehot_train[counter, y_train_org[y,]+1] <- 1#
	counter <- counter+1#
}#
X_train <- as.matrix(X_train)#
X_test <- as.matrix(X_test)#
y_train <- as.matrix(y_train_org)#
y_test <- as.matrix(y_test_org)#
#
if(onehot){#
	y_test <- onehot_test#
	y_train <- onehot_train#
}#
#
rbm <- rbm.train(x=X_train, hidden=hidden_rbm, numepochs = numepochs_rbm, batchsize = batchsize_rbm, learningrate = learningrate_rbm, learningrate_scale = learningrate_scale_rbm, momentum = 0.5, visible_type = "bin", hidden_type = "bin", cd = cd)#
#
transformed_train <- rbm.up(rbm, X_train)#
transformed_test <- rbm.up(rbm, X_test)#
#
nn = nn.train(x=transformed_train, y_train, initW = NULL, initB = NULL, hidden = hidden_nn, activationfun = "sigm", learningrate = learningrate_nn, momentum = 0.5, learningrate_scale = learningrate_scale_nn, output = "sigm", numepochs = numepochs_nn, batchsize = batchsize_nn, hidden_dropout = 0, visible_dropout = 0)#
#
score <- 0#
score <- nn.test(nn, transformed_test, y_test, t = 0.5)#
#
pred <- nn.predict(nn, transformed_test)#
pred_norm <- matrix(0L, nrow=dim(pred)[1], ncol=dim(pred)[2])#
for(i in 1:dim(pred)[1]){#
	max_row <- which.max(pred[i,])#
	pred_norm[i,max_row] <- 1#
}#
#
score_norm <- 0#
for(i in 1:dim(pred_norm)[1]){#
	if(which.max(pred_norm[i,]) == which.max(y_test[i,])){#
		score_norm <- score_norm +1#
	}#
}#
score_norm <- 1- score_norm/dim(pred)[1]#
#
configs <- rbind(configs,data.frame(hidden_rbm=hidden_rbm, numempochs_rbm = numepochs_rbm, batchsize_rbm=batchsize_rbm, lr_rbm=learningrate_rbm, cd=cd, hidden_nn=paste(hidden_nn,collapse=" "), lr_nn=learningrate_nn, numepochs_nn=numepochs_nn, batchsize_nn=batchsize_nn, onehot=onehot, err_score=score, err_score_norm = score_norm))#
predict_list[[dim(configs)[1]]] <- pred#
predict_norm_list[[dim(configs)[1]]] <- pred_norm#
nn_list[[dim(configs)[1]]] <- nn#
#
configs
# run this only once#
if(exists("configs") == F){#
	library(deepnet)#
	configs = data.frame(hidden_rbm=integer(), numempochs_rbm=integer(), batchsize_rbm=integer(), lr_rbm=numeric(), cd=integer(), hidden_nn=character(), lr_nn=numeric(), numepochs_nn=integer(), batchsize_nn=numeric(), onehot=integer(), err_score=numeric(), err_score_norm=numeric())#
	predict_list <- list()#
	predict_norm_list <- list()#
	nn_list <- list()#
}#
onehot <- 1#
#
hidden_rbm <- 1000#
numepochs_rbm <- 10#
batchsize_rbm <- 100#
learningrate_rbm <- 0.001#
learningrate_scale_rbm <- 0.9#
cd <- 10#
#
hidden_nn <- c(100,100)#
learningrate_nn <- 0.001#
learningrate_scale_nn <- 0.9#
numepochs_nn <- 10#
batchsize_nn <- 100#
#
X_train <- read.csv(file="../X_train.csv", header=T, sep=",", row.names=1)#
X_test <- read.csv(file="../X_test.csv", header=T, sep=",", row.names=1)#
y_train_org <- read.csv(file="../y_train.csv", header=T, sep=",", row.names=1)#
y_test_org <- read.csv(file="../y_test.csv", header=T, sep=",", row.names=1)#
#
X_train <- X_train[1:939,]#
X_test <- X_test[1:313,]#
#
# trying one hot encoding#
onehot_test <- matrix(0L, nrow=dim(y_test_org)[1], ncol=max(y_test_org)+1)#
counter <- 1#
for(y in 1:dim(y_test_org)[1]){#
	onehot_test[counter,y_test_org[y,]+1] <- 1#
	counter <- counter+1#
}#
# trying one hot encoding#
onehot_train <- matrix(0L, nrow=dim(y_train_org)[1], ncol=max(y_train_org)+1)#
counter <- 1#
for(y in 1:dim(y_train_org)[1]){#
	onehot_train[counter, y_train_org[y,]+1] <- 1#
	counter <- counter+1#
}#
X_train <- as.matrix(X_train)#
X_test <- as.matrix(X_test)#
y_train <- as.matrix(y_train_org)#
y_test <- as.matrix(y_test_org)#
#
if(onehot){#
	y_test <- onehot_test#
	y_train <- onehot_train#
}#
#
rbm <- rbm.train(x=X_train, hidden=hidden_rbm, numepochs = numepochs_rbm, batchsize = batchsize_rbm, learningrate = learningrate_rbm, learningrate_scale = learningrate_scale_rbm, momentum = 0.5, visible_type = "bin", hidden_type = "bin", cd = cd)#
#
transformed_train <- rbm.up(rbm, X_train)#
transformed_test <- rbm.up(rbm, X_test)#
#
nn = nn.train(x=transformed_train, y_train, initW = NULL, initB = NULL, hidden = hidden_nn, activationfun = "sigm", learningrate = learningrate_nn, momentum = 0.5, learningrate_scale = learningrate_scale_nn, output = "sigm", numepochs = numepochs_nn, batchsize = batchsize_nn, hidden_dropout = 0, visible_dropout = 0)#
#
score <- 0#
score <- nn.test(nn, transformed_test, y_test, t = 0.5)#
#
pred <- nn.predict(nn, transformed_test)#
pred_norm <- matrix(0L, nrow=dim(pred)[1], ncol=dim(pred)[2])#
for(i in 1:dim(pred)[1]){#
	max_row <- which.max(pred[i,])#
	pred_norm[i,max_row] <- 1#
}#
#
score_norm <- 0#
for(i in 1:dim(pred_norm)[1]){#
	if(which.max(pred_norm[i,]) == which.max(y_test[i,])){#
		score_norm <- score_norm +1#
	}#
}#
score_norm <- 1- score_norm/dim(pred)[1]#
#
configs <- rbind(configs,data.frame(hidden_rbm=hidden_rbm, numempochs_rbm = numepochs_rbm, batchsize_rbm=batchsize_rbm, lr_rbm=learningrate_rbm, cd=cd, hidden_nn=paste(hidden_nn,collapse=" "), lr_nn=learningrate_nn, numepochs_nn=numepochs_nn, batchsize_nn=batchsize_nn, onehot=onehot, err_score=score, err_score_norm = score_norm))#
predict_list[[dim(configs)[1]]] <- pred#
predict_norm_list[[dim(configs)[1]]] <- pred_norm#
nn_list[[dim(configs)[1]]] <- nn#
#
configs
# run this only once#
if(exists("configs") == F){#
	library(deepnet)#
	configs = data.frame(hidden_rbm=integer(), numempochs_rbm=integer(), batchsize_rbm=integer(), lr_rbm=numeric(), cd=integer(), hidden_nn=character(), lr_nn=numeric(), numepochs_nn=integer(), batchsize_nn=numeric(), onehot=integer(), err_score=numeric(), err_score_norm=numeric())#
	predict_list <- list()#
	predict_norm_list <- list()#
	nn_list <- list()#
}#
onehot <- 1#
#
hidden_rbm <- 1000#
numepochs_rbm <- 10#
batchsize_rbm <- 100#
learningrate_rbm <- 0.001#
learningrate_scale_rbm <- 0.7#
cd <- 10#
#
hidden_nn <- c(100,100)#
learningrate_nn <- 0.001#
learningrate_scale_nn <- 0.7#
numepochs_nn <- 10#
batchsize_nn <- 100#
#
X_train <- read.csv(file="../X_train.csv", header=T, sep=",", row.names=1)#
X_test <- read.csv(file="../X_test.csv", header=T, sep=",", row.names=1)#
y_train_org <- read.csv(file="../y_train.csv", header=T, sep=",", row.names=1)#
y_test_org <- read.csv(file="../y_test.csv", header=T, sep=",", row.names=1)#
#
X_train <- X_train[1:939,]#
X_test <- X_test[1:313,]#
#
# trying one hot encoding#
onehot_test <- matrix(0L, nrow=dim(y_test_org)[1], ncol=max(y_test_org)+1)#
counter <- 1#
for(y in 1:dim(y_test_org)[1]){#
	onehot_test[counter,y_test_org[y,]+1] <- 1#
	counter <- counter+1#
}#
# trying one hot encoding#
onehot_train <- matrix(0L, nrow=dim(y_train_org)[1], ncol=max(y_train_org)+1)#
counter <- 1#
for(y in 1:dim(y_train_org)[1]){#
	onehot_train[counter, y_train_org[y,]+1] <- 1#
	counter <- counter+1#
}#
X_train <- as.matrix(X_train)#
X_test <- as.matrix(X_test)#
y_train <- as.matrix(y_train_org)#
y_test <- as.matrix(y_test_org)#
#
if(onehot){#
	y_test <- onehot_test#
	y_train <- onehot_train#
}#
#
rbm <- rbm.train(x=X_train, hidden=hidden_rbm, numepochs = numepochs_rbm, batchsize = batchsize_rbm, learningrate = learningrate_rbm, learningrate_scale = learningrate_scale_rbm, momentum = 0.5, visible_type = "bin", hidden_type = "bin", cd = cd)#
#
transformed_train <- rbm.up(rbm, X_train)#
transformed_test <- rbm.up(rbm, X_test)#
#
nn = nn.train(x=transformed_train, y_train, initW = NULL, initB = NULL, hidden = hidden_nn, activationfun = "sigm", learningrate = learningrate_nn, momentum = 0.5, learningrate_scale = learningrate_scale_nn, output = "sigm", numepochs = numepochs_nn, batchsize = batchsize_nn, hidden_dropout = 0, visible_dropout = 0)#
#
score <- 0#
score <- nn.test(nn, transformed_test, y_test, t = 0.5)#
#
pred <- nn.predict(nn, transformed_test)#
pred_norm <- matrix(0L, nrow=dim(pred)[1], ncol=dim(pred)[2])#
for(i in 1:dim(pred)[1]){#
	max_row <- which.max(pred[i,])#
	pred_norm[i,max_row] <- 1#
}#
#
score_norm <- 0#
for(i in 1:dim(pred_norm)[1]){#
	if(which.max(pred_norm[i,]) == which.max(y_test[i,])){#
		score_norm <- score_norm +1#
	}#
}#
score_norm <- 1- score_norm/dim(pred)[1]#
#
configs <- rbind(configs,data.frame(hidden_rbm=hidden_rbm, numempochs_rbm = numepochs_rbm, batchsize_rbm=batchsize_rbm, lr_rbm=learningrate_rbm, cd=cd, hidden_nn=paste(hidden_nn,collapse=" "), lr_nn=learningrate_nn, numepochs_nn=numepochs_nn, batchsize_nn=batchsize_nn, onehot=onehot, err_score=score, err_score_norm = score_norm))#
predict_list[[dim(configs)[1]]] <- pred#
predict_norm_list[[dim(configs)[1]]] <- pred_norm#
nn_list[[dim(configs)[1]]] <- nn#
#
configs
predict_norm_list[11]
sum(pred_norm[,1])
sum(pred_norm[,2])
sum(pred_norm[,3])
sum(pred_norm[,4])
sum(y_test[,4])
sum(y_test[,3])
sum(y_test[,1])
sum(y_test[,2])
sum(y_test[,5])
# run this only once#
if(exists("configs") == F){#
	library(deepnet)#
	configs = data.frame(hidden_rbm=integer(), numempochs_rbm=integer(), batchsize_rbm=integer(), lr_rbm=numeric(), cd=integer(), hidden_nn=character(), lr_nn=numeric(), numepochs_nn=integer(), batchsize_nn=numeric(), onehot=integer(), err_score=numeric(), err_score_norm=numeric())#
	predict_list <- list()#
	predict_norm_list <- list()#
	nn_list <- list()#
}#
onehot <- 1#
#
hidden_rbm <- 1000#
numepochs_rbm <- 10#
batchsize_rbm <- 100#
learningrate_rbm <- 0.001#
learningrate_scale_rbm <- 0.5#
cd <- 10#
#
hidden_nn <- c(100)#
learningrate_nn <- 0.001#
learningrate_scale_nn <- 0.5#
numepochs_nn <- 10#
batchsize_nn <- 100#
#
X_train <- read.csv(file="../X_train.csv", header=T, sep=",", row.names=1)#
X_test <- read.csv(file="../X_test.csv", header=T, sep=",", row.names=1)#
y_train_org <- read.csv(file="../y_train.csv", header=T, sep=",", row.names=1)#
y_test_org <- read.csv(file="../y_test.csv", header=T, sep=",", row.names=1)#
#
X_train <- X_train[1:939,]#
X_test <- X_test[1:313,]#
#
# trying one hot encoding#
onehot_test <- matrix(0L, nrow=dim(y_test_org)[1], ncol=max(y_test_org)+1)#
counter <- 1#
for(y in 1:dim(y_test_org)[1]){#
	onehot_test[counter,y_test_org[y,]+1] <- 1#
	counter <- counter+1#
}#
# trying one hot encoding#
onehot_train <- matrix(0L, nrow=dim(y_train_org)[1], ncol=max(y_train_org)+1)#
counter <- 1#
for(y in 1:dim(y_train_org)[1]){#
	onehot_train[counter, y_train_org[y,]+1] <- 1#
	counter <- counter+1#
}#
X_train <- as.matrix(X_train)#
X_test <- as.matrix(X_test)#
y_train <- as.matrix(y_train_org)#
y_test <- as.matrix(y_test_org)#
#
if(onehot){#
	y_test <- onehot_test#
	y_train <- onehot_train#
}#
#
rbm <- rbm.train(x=X_train, hidden=hidden_rbm, numepochs = numepochs_rbm, batchsize = batchsize_rbm, learningrate = learningrate_rbm, learningrate_scale = learningrate_scale_rbm, momentum = 0.5, visible_type = "bin", hidden_type = "bin", cd = cd)#
#
transformed_train <- rbm.up(rbm, X_train)#
transformed_test <- rbm.up(rbm, X_test)#
#
nn = nn.train(x=transformed_train, y_train, initW = NULL, initB = NULL, hidden = hidden_nn, activationfun = "sigm", learningrate = learningrate_nn, momentum = 0.5, learningrate_scale = learningrate_scale_nn, output = "sigm", numepochs = numepochs_nn, batchsize = batchsize_nn, hidden_dropout = 0, visible_dropout = 0)#
#
score <- 0#
score <- nn.test(nn, transformed_test, y_test, t = 0.5)#
#
pred <- nn.predict(nn, transformed_test)#
pred_norm <- matrix(0L, nrow=dim(pred)[1], ncol=dim(pred)[2])#
for(i in 1:dim(pred)[1]){#
	max_row <- which.max(pred[i,])#
	pred_norm[i,max_row] <- 1#
}#
#
score_norm <- 0#
for(i in 1:dim(pred_norm)[1]){#
	if(which.max(pred_norm[i,]) == which.max(y_test[i,])){#
		score_norm <- score_norm +1#
	}#
}#
score_norm <- 1- score_norm/dim(pred)[1]#
#
configs <- rbind(configs,data.frame(hidden_rbm=hidden_rbm, numempochs_rbm = numepochs_rbm, batchsize_rbm=batchsize_rbm, lr_rbm=learningrate_rbm, cd=cd, hidden_nn=paste(hidden_nn,collapse=" "), lr_nn=learningrate_nn, numepochs_nn=numepochs_nn, batchsize_nn=batchsize_nn, onehot=onehot, err_score=score, err_score_norm = score_norm))#
predict_list[[dim(configs)[1]]] <- pred#
predict_norm_list[[dim(configs)[1]]] <- pred_norm#
nn_list[[dim(configs)[1]]] <- nn#
#
configs
# run this only once#
if(exists("configs") == F){#
	library(deepnet)#
	configs = data.frame(hidden_rbm=integer(), numempochs_rbm=integer(), batchsize_rbm=integer(), lr_rbm=numeric(), cd=integer(), hidden_nn=character(), lr_nn=numeric(), numepochs_nn=integer(), batchsize_nn=numeric(), onehot=integer(), err_score=numeric(), err_score_norm=numeric())#
	predict_list <- list()#
	predict_norm_list <- list()#
	nn_list <- list()#
}#
onehot <- 1#
#
hidden_rbm <- 1000#
numepochs_rbm <- 10#
batchsize_rbm <- 500#
learningrate_rbm <- 0.001#
learningrate_scale_rbm <- 0.5#
cd <- 10#
#
hidden_nn <- c(100)#
learningrate_nn <- 0.001#
learningrate_scale_nn <- 0.5#
numepochs_nn <- 10#
batchsize_nn <- 500#
#
X_train <- read.csv(file="../X_train.csv", header=T, sep=",", row.names=1)#
X_test <- read.csv(file="../X_test.csv", header=T, sep=",", row.names=1)#
y_train_org <- read.csv(file="../y_train.csv", header=T, sep=",", row.names=1)#
y_test_org <- read.csv(file="../y_test.csv", header=T, sep=",", row.names=1)#
#
X_train <- X_train[1:939,]#
X_test <- X_test[1:313,]#
#
# trying one hot encoding#
onehot_test <- matrix(0L, nrow=dim(y_test_org)[1], ncol=max(y_test_org)+1)#
counter <- 1#
for(y in 1:dim(y_test_org)[1]){#
	onehot_test[counter,y_test_org[y,]+1] <- 1#
	counter <- counter+1#
}#
# trying one hot encoding#
onehot_train <- matrix(0L, nrow=dim(y_train_org)[1], ncol=max(y_train_org)+1)#
counter <- 1#
for(y in 1:dim(y_train_org)[1]){#
	onehot_train[counter, y_train_org[y,]+1] <- 1#
	counter <- counter+1#
}#
X_train <- as.matrix(X_train)#
X_test <- as.matrix(X_test)#
y_train <- as.matrix(y_train_org)#
y_test <- as.matrix(y_test_org)#
#
if(onehot){#
	y_test <- onehot_test#
	y_train <- onehot_train#
}#
#
rbm <- rbm.train(x=X_train, hidden=hidden_rbm, numepochs = numepochs_rbm, batchsize = batchsize_rbm, learningrate = learningrate_rbm, learningrate_scale = learningrate_scale_rbm, momentum = 0.5, visible_type = "bin", hidden_type = "bin", cd = cd)#
#
transformed_train <- rbm.up(rbm, X_train)#
transformed_test <- rbm.up(rbm, X_test)#
#
nn = nn.train(x=transformed_train, y_train, initW = NULL, initB = NULL, hidden = hidden_nn, activationfun = "sigm", learningrate = learningrate_nn, momentum = 0.5, learningrate_scale = learningrate_scale_nn, output = "sigm", numepochs = numepochs_nn, batchsize = batchsize_nn, hidden_dropout = 0, visible_dropout = 0)#
#
score <- 0#
score <- nn.test(nn, transformed_test, y_test, t = 0.5)#
#
pred <- nn.predict(nn, transformed_test)#
pred_norm <- matrix(0L, nrow=dim(pred)[1], ncol=dim(pred)[2])#
for(i in 1:dim(pred)[1]){#
	max_row <- which.max(pred[i,])#
	pred_norm[i,max_row] <- 1#
}#
#
score_norm <- 0#
for(i in 1:dim(pred_norm)[1]){#
	if(which.max(pred_norm[i,]) == which.max(y_test[i,])){#
		score_norm <- score_norm +1#
	}#
}#
score_norm <- 1- score_norm/dim(pred)[1]#
#
configs <- rbind(configs,data.frame(hidden_rbm=hidden_rbm, numempochs_rbm = numepochs_rbm, batchsize_rbm=batchsize_rbm, lr_rbm=learningrate_rbm, cd=cd, hidden_nn=paste(hidden_nn,collapse=" "), lr_nn=learningrate_nn, numepochs_nn=numepochs_nn, batchsize_nn=batchsize_nn, onehot=onehot, err_score=score, err_score_norm = score_norm))#
predict_list[[dim(configs)[1]]] <- pred#
predict_norm_list[[dim(configs)[1]]] <- pred_norm#
nn_list[[dim(configs)[1]]] <- nn#
#
configs
pred_norm
rbm <- rbm.train(x=X_train, hidden=hidden_rbm, numepochs = numepochs_rbm, batchsize = batchsize_rbm, learningrate = learningrate_rbm, learningrate_scale = learningrate_scale_rbm, momentum = 0.5, visible_type = "sigm", hidden_type = "bin", cd = cd)
X_test
Var1 <- c(rep(1, 50), rep(0, 50))
Var1
# Loading library#
library(darch)#
#
# MNIST example with pre-training#
dataFolder = "data/"#
downloadMNIST = T#
# Make sure to prove the correct folder if you have already downloaded the#
# MNIST data somewhere, or otherwise set downloadMNIST to TRUE#
provideMNIST(dataFolder, downloadMNIST)#
# Load MNIST data#
load(paste0(dataFolder, "train.RData")) # trainData, trainLabels#
load(paste0(dataFolder, "test.RData")) # testData, testLabels#
#
onehot <- 1#
# Load csv#
#
X_train <- read.csv(file="../X_train.csv", header=T, sep=",", row.names=1)#
X_test <- read.csv(file="../X_test.csv", header=T, sep=",", row.names=1)#
y_train_org <- read.csv(file="../y_train.csv", header=T, sep=",", row.names=1)#
y_test_org <- read.csv(file="../y_test.csv", header=T, sep=",", row.names=1)#
#
X_train <- X_train[1:939,]#
X_test <- X_test[1:313,]#
#
# trying one hot encoding#
onehot_test <- matrix(0L, nrow=dim(y_test_org)[1], ncol=max(y_test_org)+1)#
counter <- 1#
for(y in 1:dim(y_test_org)[1]){#
	onehot_test[counter,y_test_org[y,]+1] <- 1#
	counter <- counter+1#
}#
# trying one hot encoding#
onehot_train <- matrix(0L, nrow=dim(y_train_org)[1], ncol=max(y_train_org)+1)#
counter <- 1#
for(y in 1:dim(y_train_org)[1]){#
	onehot_train[counter, y_train_org[y,]+1] <- 1#
	counter <- counter+1#
}#
X_train <- as.matrix(X_train)#
X_test <- as.matrix(X_test)#
y_train <- as.matrix(y_train_org)#
y_test <- as.matrix(y_test_org)#
#
if(onehot){#
	y_test <- onehot_test#
	y_train <- onehot_train#
}
y_test
y_train
darch  <- darch(X_train, y_train,#
  rbm.numEpochs = 5,#
  rbm.consecutive = F, # each RBM is trained one epoch at a time#
  rbm.batchSize = 100,#
  rbm.lastLayer = -1, # don't train output layer#
  rbm.allData = T, # use bootstrap validation data as well for training#
  rbm.errorFunction = rmseError,#
  rbm.initialMomentum = .5,#
  rbm.finalMomentum = .7,#
  rbm.learnRate = .1,#
  rbm.learnRateScale = .98,#
  rbm.momentumRampLength = .8,#
  rbm.numCD = 2,#
  rbm.unitFunction = sigmoidUnitRbm,#
  rbm.weightDecay = .001,#
  layers = c(6,10,5),#
  darch.batchSize = 100,#
  darch.dither = T,#
  darch.initialMomentum = .4,#
  darch.finalMomentum = .9,#
  darch.momentumRampLength = .75,#
  bp.learnRate = 1,#
  bp.learnRateScale = .99,#
  darch.unitFunction = c(tanhUnit, softmaxUnit),#
  bootstrap = T,#
  darch.numEpochs = 20,#
  gputools = T, # try to use gputools#
  gputools.deviceId = 0#
)
predictions <- predict(darch, newdata=X_test, type="class")
labels <- cbind(predictions, testLabels)#
numIncorrect <- sum(apply(labels, 1, function(i) { any(i[1:10] != i[11:20]) }))#
cat(paste0("Incorrect classifications on test data: ", numIncorrect,#
           " (", round(numIncorrect/nrow(testLabels)*100, 2), "%)\n"))#
 darch
# Loading library#
library(darch)#
#
# MNIST example with pre-training#
dataFolder = "data/"#
downloadMNIST = T#
# Make sure to prove the correct folder if you have already downloaded the#
# MNIST data somewhere, or otherwise set downloadMNIST to TRUE#
provideMNIST(dataFolder, downloadMNIST)#
# Load MNIST data#
load(paste0(dataFolder, "train.RData")) # trainData, trainLabels#
load(paste0(dataFolder, "test.RData")) # testData, testLabels#
#
onehot <- 1#
# Load csv#
#
X_train <- read.csv(file="../X_train.csv", header=T, sep=",", row.names=1)#
X_test <- read.csv(file="../X_test.csv", header=T, sep=",", row.names=1)#
y_train_org <- read.csv(file="../y_train.csv", header=T, sep=",", row.names=1)#
y_test_org <- read.csv(file="../y_test.csv", header=T, sep=",", row.names=1)#
#
X_train <- X_train[1:939,]#
X_test <- X_test[1:313,]#
#
# trying one hot encoding#
onehot_test <- matrix(0L, nrow=dim(y_test_org)[1], ncol=max(y_test_org)+1)#
counter <- 1#
for(y in 1:dim(y_test_org)[1]){#
	onehot_test[counter,y_test_org[y,]+1] <- 1#
	counter <- counter+1#
}#
# trying one hot encoding#
onehot_train <- matrix(0L, nrow=dim(y_train_org)[1], ncol=max(y_train_org)+1)#
counter <- 1#
for(y in 1:dim(y_train_org)[1]){#
	onehot_train[counter, y_train_org[y,]+1] <- 1#
	counter <- counter+1#
}#
#
X_train <- as.matrix(X_train)#
X_test <- as.matrix(X_test)#
y_train <- as.matrix(y_train_org)#
y_test <- as.matrix(y_test_org)#
#
if(onehot){#
	y_test <- onehot_test#
	y_train <- onehot_train#
}#
# only take 1000 samples, otherwise training takes increasingly long#
chosenRowsTrain <- sample(1:nrow(trainData), size=1000)#
trainDataSmall <- trainData[chosenRowsTrain,]#
trainLabelsSmall <- trainLabels[chosenRowsTrain,]#
darch  <- darch(X_train, y_train,#
  rbm.numEpochs = 10#
  rbm.consecutive = F, # each RBM is trained one epoch at a time#
  rbm.batchSize = 100,#
  rbm.lastLayer = -1, # don't train output layer#
  rbm.allData = T, # use bootstrap validation data as well for training#
  rbm.errorFunction = rmseError,#
  rbm.initialMomentum = .5,#
  rbm.finalMomentum = .7,#
  rbm.learnRate = .1,#
  rbm.learnRateScale = .98,#
  rbm.momentumRampLength = .8,#
  rbm.numCD = 2,#
  rbm.unitFunction = sigmoidUnitRbm,#
  rbm.weightDecay = .001,#
  layers = c(6,10,5),#
  darch.batchSize = 100,#
  darch.dither = T,#
  darch.initialMomentum = .4,#
  darch.finalMomentum = .9,#
  darch.momentumRampLength = .75,#
  bp.learnRate = 1,#
  bp.learnRateScale = .99,#
  darch.unitFunction = c(tanhUnit, softmaxUnit),#
  bootstrap = T,#
  darch.numEpochs = 20,#
  gputools = T, # try to use gputools#
  gputools.deviceId = 0#
)#
#
predictions <- predict(darch, newdata=X_test, type="class")#
labels <- cbind(predictions, testLabels)#
numIncorrect <- sum(apply(labels, 1, function(i) { any(i[1:10] != i[11:20]) }))#
cat(paste0("Incorrect classifications on test data: ", numIncorrect,#
           " (", round(numIncorrect/nrow(testLabels)*100, 2), "%)\n"))#
 darch
# Loading library#
library(darch)#
#
# MNIST example with pre-training#
dataFolder = "data/"#
downloadMNIST = T#
# Make sure to prove the correct folder if you have already downloaded the#
# MNIST data somewhere, or otherwise set downloadMNIST to TRUE#
provideMNIST(dataFolder, downloadMNIST)#
# Load MNIST data#
load(paste0(dataFolder, "train.RData")) # trainData, trainLabels#
load(paste0(dataFolder, "test.RData")) # testData, testLabels#
#
onehot <- 1#
# Load csv#
#
X_train <- read.csv(file="../X_train.csv", header=T, sep=",", row.names=1)#
X_test <- read.csv(file="../X_test.csv", header=T, sep=",", row.names=1)#
y_train_org <- read.csv(file="../y_train.csv", header=T, sep=",", row.names=1)#
y_test_org <- read.csv(file="../y_test.csv", header=T, sep=",", row.names=1)#
#
X_train <- X_train[1:939,]#
X_test <- X_test[1:313,]#
#
# trying one hot encoding#
onehot_test <- matrix(0L, nrow=dim(y_test_org)[1], ncol=max(y_test_org)+1)#
counter <- 1#
for(y in 1:dim(y_test_org)[1]){#
	onehot_test[counter,y_test_org[y,]+1] <- 1#
	counter <- counter+1#
}#
# trying one hot encoding#
onehot_train <- matrix(0L, nrow=dim(y_train_org)[1], ncol=max(y_train_org)+1)#
counter <- 1#
for(y in 1:dim(y_train_org)[1]){#
	onehot_train[counter, y_train_org[y,]+1] <- 1#
	counter <- counter+1#
}#
#
X_train <- as.matrix(X_train)#
X_test <- as.matrix(X_test)#
y_train <- as.matrix(y_train_org)#
y_test <- as.matrix(y_test_org)#
#
if(onehot){#
	y_test <- onehot_test#
	y_train <- onehot_train#
}#
# only take 1000 samples, otherwise training takes increasingly long#
chosenRowsTrain <- sample(1:nrow(trainData), size=1000)#
trainDataSmall <- trainData[chosenRowsTrain,]#
trainLabelsSmall <- trainLabels[chosenRowsTrain,]#
darch  <- darch(X_train, y_train,#
  rbm.numEpochs = 10#
  rbm.consecutive = F, # each RBM is trained one epoch at a time#
  rbm.batchSize = 100,#
  rbm.lastLayer = -1, # don't train output layer#
  rbm.allData = T, # use bootstrap validation data as well for training#
  rbm.errorFunction = rmseError,#
  rbm.initialMomentum = .5,#
  rbm.finalMomentum = .7,#
  rbm.learnRate = .01,#
  rbm.learnRateScale = .98,#
  rbm.momentumRampLength = .8,#
  rbm.numCD = 2,#
  rbm.unitFunction = sigmoidUnitRbm,#
  rbm.weightDecay = .001,#
  layers = c(6,10,5),#
  darch.batchSize = 100,#
  darch.dither = T,#
  darch.initialMomentum = .4,#
  darch.finalMomentum = .9,#
  darch.momentumRampLength = .75,#
  bp.learnRate = 1,#
  bp.learnRateScale = .99,#
  darch.unitFunction = c(tanhUnit, softmaxUnit),#
  bootstrap = T,#
  darch.numEpochs = 20,#
  gputools = T, # try to use gputools#
  gputools.deviceId = 0#
)#
#
predictions <- predict(darch, newdata=X_test, type="class")#
labels <- cbind(predictions, testLabels)#
numIncorrect <- sum(apply(labels, 1, function(i) { any(i[1:10] != i[11:20]) }))#
cat(paste0("Incorrect classifications on test data: ", numIncorrect,#
           " (", round(numIncorrect/nrow(testLabels)*100, 2), "%)\n"))#
 darch
# Loading library#
library(darch)#
#
# MNIST example with pre-training#
dataFolder = "data/"#
downloadMNIST = T#
# Make sure to prove the correct folder if you have already downloaded the#
# MNIST data somewhere, or otherwise set downloadMNIST to TRUE#
provideMNIST(dataFolder, downloadMNIST)#
# Load MNIST data#
load(paste0(dataFolder, "train.RData")) # trainData, trainLabels#
load(paste0(dataFolder, "test.RData")) # testData, testLabels#
#
onehot <- 1#
# Load csv#
#
X_train <- read.csv(file="../X_train.csv", header=T, sep=",", row.names=1)#
X_test <- read.csv(file="../X_test.csv", header=T, sep=",", row.names=1)#
y_train_org <- read.csv(file="../y_train.csv", header=T, sep=",", row.names=1)#
y_test_org <- read.csv(file="../y_test.csv", header=T, sep=",", row.names=1)#
#
X_train <- X_train[1:939,]#
X_test <- X_test[1:313,]#
#
# trying one hot encoding#
onehot_test <- matrix(0L, nrow=dim(y_test_org)[1], ncol=max(y_test_org)+1)#
counter <- 1#
for(y in 1:dim(y_test_org)[1]){#
	onehot_test[counter,y_test_org[y,]+1] <- 1#
	counter <- counter+1#
}#
# trying one hot encoding#
onehot_train <- matrix(0L, nrow=dim(y_train_org)[1], ncol=max(y_train_org)+1)#
counter <- 1#
for(y in 1:dim(y_train_org)[1]){#
	onehot_train[counter, y_train_org[y,]+1] <- 1#
	counter <- counter+1#
}#
#
X_train <- as.matrix(X_train)#
X_test <- as.matrix(X_test)#
y_train <- as.matrix(y_train_org)#
y_test <- as.matrix(y_test_org)#
#
if(onehot){#
	y_test <- onehot_test#
	y_train <- onehot_train#
}#
# only take 1000 samples, otherwise training takes increasingly long#
chosenRowsTrain <- sample(1:nrow(trainData), size=1000)#
trainDataSmall <- trainData[chosenRowsTrain,]#
trainLabelsSmall <- trainLabels[chosenRowsTrain,]#
darch  <- darch(X_train, y_train,#
  rbm.numEpochs = 10,#
  rbm.consecutive = F, # each RBM is trained one epoch at a time#
  rbm.batchSize = 100,#
  rbm.lastLayer = -1, # don't train output layer#
  rbm.allData = T, # use bootstrap validation data as well for training#
  rbm.errorFunction = rmseError,#
  rbm.initialMomentum = .5,#
  rbm.finalMomentum = .7,#
  rbm.learnRate = .01,#
  rbm.learnRateScale = .98,#
  rbm.momentumRampLength = .8,#
  rbm.numCD = 2,#
  rbm.unitFunction = sigmoidUnitRbm,#
  rbm.weightDecay = .001,#
  layers = c(6,10,5),#
  darch.batchSize = 100,#
  darch.dither = T,#
  darch.initialMomentum = .4,#
  darch.finalMomentum = .9,#
  darch.momentumRampLength = .75,#
  bp.learnRate = 1,#
  bp.learnRateScale = .99,#
  darch.unitFunction = c(tanhUnit, softmaxUnit),#
  bootstrap = T,#
  darch.numEpochs = 20,#
  gputools = T, # try to use gputools#
  gputools.deviceId = 0#
)#
#
predictions <- predict(darch, newdata=X_test, type="class")#
labels <- cbind(predictions, testLabels)#
numIncorrect <- sum(apply(labels, 1, function(i) { any(i[1:10] != i[11:20]) }))#
cat(paste0("Incorrect classifications on test data: ", numIncorrect,#
           " (", round(numIncorrect/nrow(testLabels)*100, 2), "%)\n"))#
 darch
numIncorrect <- sum(apply(labels, 1, function(i) { any(i[1:10] != i[11:20]) }))
labels <- cbind(predictions, testLabels)
labels <- cbind(predictions, y_test)
numIncorrect <- sum(apply(labels, 1, function(i) { any(i[1:10] != i[11:20]) }))
cat(paste0("Incorrect classifications on test data: ", numIncorrect,#
           " (", round(numIncorrect/nrow(testLabels)*100, 2), "%)\n"))
predictions
labels
labels <- cbind(predictions, y_test)#
numIncorrect <- sum(apply(labels, 1, function(i) { any(i[1:10] != i[11:20]) }))#
cat(paste0("Incorrect classifications on test data: ", numIncorrect,#
           " (", round(numIncorrect/nrow(y_test)*100, 2), "%)\n"))
numIncorrect
labels <- cbind(predictions, y_test)#
numIncorrect <- sum(apply(labels, 1, function(i) { any(i[1:10] != i[11:20]) }))#
cat(paste0("Incorrect classifications on test data: ", numIncorrect," (", round(numIncorrect/nrow(y_test)*100, 2), "%)\n"))
numIncorrect <- sum(apply(labels, 1, function(i) { any(i[1:5] != i[6:10]) }))
numIncorrect
labels <- cbind(predictions, y_test)#
numIncorrect <- sum(apply(labels, 1, function(i) { any(i[1:5] != i[6:10]) }))#
cat(paste0("Incorrect classifications on test data: ", numIncorrect," (", round(numIncorrect/nrow(y_test)*100, 2), "%)\n"))
chosenRowsTrain <- sample(1:nrow(trainData), size=1000)
chosenRowsTrain
nrow(X_train)
# Loading library#
if(exists("configs") == F){#
	library(darch)#
	configs = data.frame(epochs_rbm=integer(), batch_rbm=integer(), ln_rate_rbm=numeric(), ln_scale_rbm=numeric(), cd_rbm=integer(), layers=charecters(), batch=integer(), ln_rate_bp=numeric(), ln_scale=numeric(), epochs_ft=integer())#
	predict_list <- list()#
	predict_norm_list <- list()#
	nn_list <- list()#
}#
#
onehot <- 1#
rbm.numEpochs <- 10#
rbm.batchSize <- 100#
rbm.learnRate <- .01#
rbm.learnRateScale <- .98#
rbm.numCD <- 2#
layers <- c(6,10,5)#
darch.batchSize <- 100#
bp.learnRate <- 1#
bp.learnRateScale <- .99#
darch.numEpochs <- 20#
#
# Load csv#
#
X_train <- read.csv(file="../X_train.csv", header=T, sep=",", row.names=1)#
X_test <- read.csv(file="../X_test.csv", header=T, sep=",", row.names=1)#
y_train_org <- read.csv(file="../y_train.csv", header=T, sep=",", row.names=1)#
y_test_org <- read.csv(file="../y_test.csv", header=T, sep=",", row.names=1)#
#
X_train <- X_train[1:939,]#
X_test <- X_test[1:313,]#
#
# trying one hot encoding#
onehot_test <- matrix(0L, nrow=dim(y_test_org)[1], ncol=max(y_test_org)+1)#
counter <- 1#
for(y in 1:dim(y_test_org)[1]){#
	onehot_test[counter,y_test_org[y,]+1] <- 1#
	counter <- counter+1#
}#
# trying one hot encoding#
onehot_train <- matrix(0L, nrow=dim(y_train_org)[1], ncol=max(y_train_org)+1)#
counter <- 1#
for(y in 1:dim(y_train_org)[1]){#
	onehot_train[counter, y_train_org[y,]+1] <- 1#
	counter <- counter+1#
}#
#
X_train <- as.matrix(X_train)#
X_test <- as.matrix(X_test)#
y_train <- as.matrix(y_train_org)#
y_test <- as.matrix(y_test_org)#
#
if(onehot){#
	y_test <- onehot_test#
	y_train <- onehot_train#
}#
# only take 1000 samples, otherwise training takes increasingly long#
#chosenRowsTrain <- sample(1:nrow(trainData), size=nrow(X_train))#
#trainDataSmall <- trainData[chosenRowsTrain,]#
#trainLabelsSmall <- trainLabels[chosenRowsTrain,]#
darch  <- darch(X_train, y_train,#
  rbm.numEpochs = 10,#
  rbm.consecutive = F, # each RBM is trained one epoch at a time#
  rbm.batchSize = 100,#
  rbm.lastLayer = -1, # don't train output layer#
  rbm.allData = T, # use bootstrap validation data as well for training#
  rbm.errorFunction = rmseError,#
  rbm.initialMomentum = .5,#
  rbm.finalMomentum = .7,#
  rbm.learnRate = .01,#
  rbm.learnRateScale = .98,#
  rbm.momentumRampLength = .8,#
  rbm.numCD = 2,#
  rbm.unitFunction = sigmoidUnitRbm,#
  rbm.weightDecay = .001,#
  layers = c(6,10,5),#
  darch.batchSize = 100,#
  darch.dither = T,#
  darch.initialMomentum = .4,#
  darch.finalMomentum = .9,#
  darch.momentumRampLength = .75,#
  bp.learnRate = 1,#
  bp.learnRateScale = .99,#
  darch.unitFunction = c(tanhUnit, softmaxUnit),#
  bootstrap = T,#
  darch.numEpochs = 20,#
  gputools = T, # try to use gputools#
  gputools.deviceId = 0#
)#
#
predictions <- predict(darch, newdata=X_test, type="class")#
labels <- cbind(predictions, y_test)#
numIncorrect <- sum(apply(labels, 1, function(i) { any(i[1:5] != i[6:10]) }))#
cat(paste0("Incorrect classifications on test data: ", numIncorrect," (", round(numIncorrect/nrow(y_test)*100, 2), "%)\n"))#
configs <- rbind(configs,data.frame(epochs_rbm= epochs_rbm, batch_rbm= batch_rbm, ln_rate_rbm= ln_rate_rbm, ln_scale_rbm= ln_scale_rbm, cd_rbm= cd_rbm, layers= paste(layers,collapse=" "), batch= batch, ln_rate_bp= ln_rate_bp, ln_scale= ln_scale, epochs_ft= epochs_ft))#
predict_list[[dim(configs)[1]]] <- predictions#
predict_norm_list[[dim(configs)[1]]] <- numIncorrect/nrow(y_test)#
nn_list[[dim(configs)[1]]] <- darch#
#
darch
configs
# Loading library#
if(exists("configs") == F){#
	library(darch)#
	configs = data.frame(epochs_rbm=integer(), batch_rbm=integer(), ln_rate_rbm=numeric(), ln_scale_rbm=numeric(), cd_rbm=integer(), layers=charecters(), batch=integer(), ln_rate_bp=numeric(), ln_scale=numeric(), epochs_ft=integer())#
	predict_list <- list()#
	predict_norm_list <- list()#
	nn_list <- list()#
}#
#
onehot <- 1#
rbm.numEpochs <- 10#
rbm.batchSize <- 100#
rbm.learnRate <- .01#
rbm.learnRateScale <- .98#
rbm.numCD <- 2#
layers <- c(6,10,5)#
darch.batchSize <- 100#
bp.learnRate <- 1#
bp.learnRateScale <- .99#
darch.numEpochs <- 20#
#
# Load csv#
#
X_train <- read.csv(file="../X_train.csv", header=T, sep=",", row.names=1)#
X_test <- read.csv(file="../X_test.csv", header=T, sep=",", row.names=1)#
y_train_org <- read.csv(file="../y_train.csv", header=T, sep=",", row.names=1)#
y_test_org <- read.csv(file="../y_test.csv", header=T, sep=",", row.names=1)#
#
X_train <- X_train[1:939,]#
X_test <- X_test[1:313,]#
#
# trying one hot encoding#
onehot_test <- matrix(0L, nrow=dim(y_test_org)[1], ncol=max(y_test_org)+1)#
counter <- 1#
for(y in 1:dim(y_test_org)[1]){#
	onehot_test[counter,y_test_org[y,]+1] <- 1#
	counter <- counter+1#
}#
# trying one hot encoding#
onehot_train <- matrix(0L, nrow=dim(y_train_org)[1], ncol=max(y_train_org)+1)#
counter <- 1#
for(y in 1:dim(y_train_org)[1]){#
	onehot_train[counter, y_train_org[y,]+1] <- 1#
	counter <- counter+1#
}#
#
X_train <- as.matrix(X_train)#
X_test <- as.matrix(X_test)#
y_train <- as.matrix(y_train_org)#
y_test <- as.matrix(y_test_org)#
#
if(onehot){#
	y_test <- onehot_test#
	y_train <- onehot_train#
}#
# only take 1000 samples, otherwise training takes increasingly long#
#chosenRowsTrain <- sample(1:nrow(trainData), size=nrow(X_train))#
#trainDataSmall <- trainData[chosenRowsTrain,]#
#trainLabelsSmall <- trainLabels[chosenRowsTrain,]#
darch  <- darch(X_train, y_train,#
  rbm.numEpochs = 10,#
  rbm.consecutive = F, # each RBM is trained one epoch at a time#
  rbm.batchSize = 100,#
  rbm.lastLayer = -1, # don't train output layer#
  rbm.allData = T, # use bootstrap validation data as well for training#
  rbm.errorFunction = rmseError,#
  rbm.initialMomentum = .5,#
  rbm.finalMomentum = .7,#
  rbm.learnRate = .01,#
  rbm.learnRateScale = .98,#
  rbm.momentumRampLength = .8,#
  rbm.numCD = 2,#
  rbm.unitFunction = sigmoidUnitRbm,#
  rbm.weightDecay = .001,#
  layers = c(6,10,5),#
  darch.batchSize = 100,#
  darch.dither = T,#
  darch.initialMomentum = .4,#
  darch.finalMomentum = .9,#
  darch.momentumRampLength = .75,#
  bp.learnRate = 1,#
  bp.learnRateScale = .99,#
  darch.unitFunction = c(tanhUnit, softmaxUnit),#
  bootstrap = T,#
  darch.numEpochs = 20,#
  gputools = T, # try to use gputools#
  gputools.deviceId = 0#
)#
#
predictions <- predict(darch, newdata=X_test, type="class")#
labels <- cbind(predictions, y_test)#
numIncorrect <- sum(apply(labels, 1, function(i) { any(i[1:5] != i[6:10]) }))#
cat(paste0("Incorrect classifications on test data: ", numIncorrect," (", round(numIncorrect/nrow(y_test)*100, 2), "%)\n"))#
configs <- rbind(configs,data.frame(epochs_rbm= epochs_rbm, batch_rbm= batch_rbm, ln_rate_rbm= ln_rate_rbm, ln_scale_rbm= ln_scale_rbm, cd_rbm= cd_rbm, layers= paste(layers,collapse=" "), batch= batch, ln_rate_bp= ln_rate_bp, ln_scale= ln_scale, epochs_ft= epochs_ft))#
predict_list[[dim(configs)[1]]] <- predictions#
predict_norm_list[[dim(configs)[1]]] <- numIncorrect/nrow(y_test)#
nn_list[[dim(configs)[1]]] <- darch#
#
darch#
configs
configs = data.frame(epochs_rbm=integer(), batch_rbm=integer(), ln_rate_rbm=numeric(), ln_scale_rbm=numeric(), cd_rbm=integer(), layers=charecter(), batch=integer(), ln_rate_bp=numeric(), ln_scale=numeric(), epochs_ft=integer())
configs = data.frame(epochs_rbm=integer(), batch_rbm=integer(), ln_rate_rbm=numeric(), ln_scale_rbm=numeric(), cd_rbm=integer(), layers=character(), batch=integer(), ln_rate_bp=numeric(), ln_scale=numeric(), epochs_ft=integer())
library(darch)#
	configs = data.frame(epochs_rbm=integer(), batch_rbm=integer(), ln_rate_rbm=numeric(), ln_scale_rbm=numeric(), cd_rbm=integer(), layers=character(), batch=integer(), ln_rate_bp=numeric(), ln_scale=numeric(), epochs_ft=integer())#
	predict_list <- list()#
	predict_norm_list <- list()#
	nn_list <- list()
cat("\014")
j
# run this only once#
if(exists("configs") == F){#
	library(deepnet)#
	configs = data.frame(hidden_rbm=integer(), numempochs_rbm=integer(), batchsize_rbm=integer(), lr_rbm=numeric(), cd=integer(), hidden_nn=character(), lr_nn=numeric(), numepochs_nn=integer(), batchsize_nn=numeric(), onehot=integer(), err_score=numeric(), err_score_norm=numeric())#
	predict_list <- list()#
	predict_norm_list <- list()#
	nn_list <- list()#
}#
onehot <- 1#
#
hidden_rbm <- 1000#
numepochs_rbm <- 10#
batchsize_rbm <- 500#
learningrate_rbm <- 0.001#
learningrate_scale_rbm <- 0.5#
cd <- 10#
#
hidden_nn <- c(100)#
learningrate_nn <- 0.001#
learningrate_scale_nn <- 0.5#
numepochs_nn <- 10#
batchsize_nn <- 500#
#
X_train <- read.csv(file="../X_train.csv", header=T, sep=",", row.names=1)#
X_test <- read.csv(file="../X_test.csv", header=T, sep=",", row.names=1)#
y_train_org <- read.csv(file="../y_train.csv", header=T, sep=",", row.names=1)#
y_test_org <- read.csv(file="../y_test.csv", header=T, sep=",", row.names=1)#
#
X_train <- X_train[1:939,]#
X_test <- X_test[1:313,]#
#
# trying one hot encoding#
onehot_test <- matrix(0L, nrow=dim(y_test_org)[1], ncol=max(y_test_org)+1)#
counter <- 1#
for(y in 1:dim(y_test_org)[1]){#
	onehot_test[counter,y_test_org[y,]+1] <- 1#
	counter <- counter+1#
}#
# trying one hot encoding#
onehot_train <- matrix(0L, nrow=dim(y_train_org)[1], ncol=max(y_train_org)+1)#
counter <- 1#
for(y in 1:dim(y_train_org)[1]){#
	onehot_train[counter, y_train_org[y,]+1] <- 1#
	counter <- counter+1#
}#
X_train <- as.matrix(X_train)#
X_test <- as.matrix(X_test)#
y_train <- as.matrix(y_train_org)#
y_test <- as.matrix(y_test_org)#
#
if(onehot){#
	y_test <- onehot_test#
	y_train <- onehot_train#
}#
#
rbm <- rbm.train(x=X_train, hidden=hidden_rbm, numepochs = numepochs_rbm, batchsize = batchsize_rbm, learningrate = learningrate_rbm, learningrate_scale = learningrate_scale_rbm, momentum = 0.5, visible_type = "bin", hidden_type = "bin", cd = cd)#
#
transformed_train <- rbm.up(rbm, X_train)#
transformed_test <- rbm.up(rbm, X_test)#
#
nn = nn.train(x=transformed_train, y_train, initW = NULL, initB = NULL, hidden = hidden_nn, activationfun = "sigm", learningrate = learningrate_nn, momentum = 0.5, learningrate_scale = learningrate_scale_nn, output = "sigm", numepochs = numepochs_nn, batchsize = batchsize_nn, hidden_dropout = 0, visible_dropout = 0)#
#
score <- 0#
score <- nn.test(nn, transformed_test, y_test, t = 0.5)#
#
pred <- nn.predict(nn, transformed_test)#
pred_norm <- matrix(0L, nrow=dim(pred)[1], ncol=dim(pred)[2])#
for(i in 1:dim(pred)[1]){#
	max_row <- which.max(pred[i,])#
	pred_norm[i,max_row] <- 1#
}#
#
score_norm <- 0#
for(i in 1:dim(pred_norm)[1]){#
	if(which.max(pred_norm[i,]) == which.max(y_test[i,])){#
		score_norm <- score_norm +1#
	}#
}#
score_norm <- 1- score_norm/dim(pred)[1]#
#
configs <- rbind(configs,data.frame(hidden_rbm=hidden_rbm, numempochs_rbm = numepochs_rbm, batchsize_rbm=batchsize_rbm, lr_rbm=learningrate_rbm, cd=cd, hidden_nn=paste(hidden_nn,collapse=" "), lr_nn=learningrate_nn, numepochs_nn=numepochs_nn, batchsize_nn=batchsize_nn, onehot=onehot, err_score=score, err_score_norm = score_norm))#
predict_list[[dim(configs)[1]]] <- pred#
predict_norm_list[[dim(configs)[1]]] <- pred_norm#
nn_list[[dim(configs)[1]]] <- nn#
#
configs
# Loading library#
if(exists("configs") == F){#
	library(darch)#
	configs = data.frame(epochs_rbm=integer(), batch_rbm=integer(), ln_rate_rbm=numeric(), ln_scale_rbm=numeric(), cd_rbm=integer(), layers=character(), batch=integer(), ln_rate_bp=numeric(), ln_scale=numeric(), epochs_ft=integer())#
	predict_list <- list()#
	predict_norm_list <- list()#
	nn_list <- list()#
}#
#
onehot <- 1#
rbm.numEpochs <- 10#
rbm.batchSize <- 100#
rbm.learnRate <- .01#
rbm.learnRateScale <- .98#
rbm.numCD <- 2#
layers <- c(6,10,5)#
darch.batchSize <- 100#
bp.learnRate <- 1#
bp.learnRateScale <- .99#
darch.numEpochs <- 20#
#
# Load csv#
#
X_train <- read.csv(file="../X_train.csv", header=T, sep=",", row.names=1)#
X_test <- read.csv(file="../X_test.csv", header=T, sep=",", row.names=1)#
y_train_org <- read.csv(file="../y_train.csv", header=T, sep=",", row.names=1)#
y_test_org <- read.csv(file="../y_test.csv", header=T, sep=",", row.names=1)#
#
X_train <- X_train[1:939,]#
X_test <- X_test[1:313,]#
#
# trying one hot encoding#
onehot_test <- matrix(0L, nrow=dim(y_test_org)[1], ncol=max(y_test_org)+1)#
counter <- 1#
for(y in 1:dim(y_test_org)[1]){#
	onehot_test[counter,y_test_org[y,]+1] <- 1#
	counter <- counter+1#
}#
# trying one hot encoding#
onehot_train <- matrix(0L, nrow=dim(y_train_org)[1], ncol=max(y_train_org)+1)#
counter <- 1#
for(y in 1:dim(y_train_org)[1]){#
	onehot_train[counter, y_train_org[y,]+1] <- 1#
	counter <- counter+1#
}#
#
X_train <- as.matrix(X_train)#
X_test <- as.matrix(X_test)#
y_train <- as.matrix(y_train_org)#
y_test <- as.matrix(y_test_org)#
#
if(onehot){#
	y_test <- onehot_test#
	y_train <- onehot_train#
}#
# only take 1000 samples, otherwise training takes increasingly long#
#chosenRowsTrain <- sample(1:nrow(trainData), size=nrow(X_train))#
#trainDataSmall <- trainData[chosenRowsTrain,]#
#trainLabelsSmall <- trainLabels[chosenRowsTrain,]#
darch  <- darch(X_train, y_train,#
  rbm.numEpochs = 10,#
  rbm.consecutive = F, # each RBM is trained one epoch at a time#
  rbm.batchSize = 100,#
  rbm.lastLayer = -1, # don't train output layer#
  rbm.allData = T, # use bootstrap validation data as well for training#
  rbm.errorFunction = rmseError,#
  rbm.initialMomentum = .5,#
  rbm.finalMomentum = .7,#
  rbm.learnRate = .01,#
  rbm.learnRateScale = .98,#
  rbm.momentumRampLength = .8,#
  rbm.numCD = 2,#
  rbm.unitFunction = sigmoidUnitRbm,#
  rbm.weightDecay = .001,#
  layers = c(6,10,5),#
  darch.batchSize = 100,#
  darch.dither = T,#
  darch.initialMomentum = .4,#
  darch.finalMomentum = .9,#
  darch.momentumRampLength = .75,#
  bp.learnRate = 1,#
  bp.learnRateScale = .99,#
  darch.unitFunction = c(tanhUnit, softmaxUnit),#
  bootstrap = T,#
  darch.numEpochs = 20,#
  gputools = T, # try to use gputools#
  gputools.deviceId = 0#
)#
#
predictions <- predict(darch, newdata=X_test, type="class")#
labels <- cbind(predictions, y_test)#
numIncorrect <- sum(apply(labels, 1, function(i) { any(i[1:5] != i[6:10]) }))#
cat(paste0("Incorrect classifications on test data: ", numIncorrect," (", round(numIncorrect/nrow(y_test)*100, 2), "%)\n"))#
configs <- rbind(configs,data.frame(epochs_rbm= epochs_rbm, batch_rbm= batch_rbm, ln_rate_rbm= ln_rate_rbm, ln_scale_rbm= ln_scale_rbm, cd_rbm= cd_rbm, layers= paste(layers,collapse=" "), batch= batch, ln_rate_bp= ln_rate_bp, ln_scale= ln_scale, epochs_ft= epochs_ft))#
predict_list[[dim(configs)[1]]] <- predictions#
predict_norm_list[[dim(configs)[1]]] <- numIncorrect/nrow(y_test)#
nn_list[[dim(configs)[1]]] <- darch#
#
darch#
configs
configs <- rbind(configs,data.frame(epochs_rbm= epochs_rbm, batch_rbm= batch_rbm, ln_rate_rbm= ln_rate_rbm, ln_scale_rbm= ln_scale_rbm, cd_rbm= cd_rbm, layers= paste(layers,collapse=" "), batch= batch, ln_rate_bp= ln_rate_bp, ln_scale_bp= ln_scale_bp, epochs_ft= epochs_ft))
# Loading library#
if(exists("configs") == F){#
	library(darch)#
	configs = data.frame(epochs_rbm=integer(), batch_rbm=integer(), ln_rate_rbm=numeric(), ln_scale_rbm=numeric(), cd_rbm=integer(), layers=character(), batch=integer(), ln_rate_bp=numeric(), ln_scale_bp=numeric(), epochs_ft=integer())#
	predict_list <- list()#
	predict_norm_list <- list()#
	nn_list <- list()#
}#
#
onehot <- 1#
epochs_rbm <- 10#
batch_rbm <- 100#
ln_rate_rbm <- .01#
ln_scale_rbm <- .98#
cd_rbm <- 2#
layers <- c(6,10,5)#
batch <- 100#
ln_rate_bp <- 1#
ln_scale_bp <- .99#
epochs_ft <- 20#
#
# Load csv#
#
X_train <- read.csv(file="../X_train.csv", header=T, sep=",", row.names=1)#
X_test <- read.csv(file="../X_test.csv", header=T, sep=",", row.names=1)#
y_train_org <- read.csv(file="../y_train.csv", header=T, sep=",", row.names=1)#
y_test_org <- read.csv(file="../y_test.csv", header=T, sep=",", row.names=1)#
#
X_train <- X_train[1:939,]#
X_test <- X_test[1:313,]#
#
# trying one hot encoding#
onehot_test <- matrix(0L, nrow=dim(y_test_org)[1], ncol=max(y_test_org)+1)#
counter <- 1#
for(y in 1:dim(y_test_org)[1]){#
	onehot_test[counter,y_test_org[y,]+1] <- 1#
	counter <- counter+1#
}#
# trying one hot encoding#
onehot_train <- matrix(0L, nrow=dim(y_train_org)[1], ncol=max(y_train_org)+1)#
counter <- 1#
for(y in 1:dim(y_train_org)[1]){#
	onehot_train[counter, y_train_org[y,]+1] <- 1#
	counter <- counter+1#
}#
#
X_train <- as.matrix(X_train)#
X_test <- as.matrix(X_test)#
y_train <- as.matrix(y_train_org)#
y_test <- as.matrix(y_test_org)#
#
if(onehot){#
	y_test <- onehot_test#
	y_train <- onehot_train#
}#
# only take 1000 samples, otherwise training takes increasingly long#
#chosenRowsTrain <- sample(1:nrow(trainData), size=nrow(X_train))#
#trainDataSmall <- trainData[chosenRowsTrain,]#
#trainLabelsSmall <- trainLabels[chosenRowsTrain,]#
darch  <- darch(X_train, y_train,#
  rbm.numEpochs = 10,#
  rbm.consecutive = F, # each RBM is trained one epoch at a time#
  rbm.batchSize = 100,#
  rbm.lastLayer = -1, # don't train output layer#
  rbm.allData = T, # use bootstrap validation data as well for training#
  rbm.errorFunction = rmseError,#
  rbm.initialMomentum = .5,#
  rbm.finalMomentum = .7,#
  rbm.learnRate = .01,#
  rbm.learnRateScale = .98,#
  rbm.momentumRampLength = .8,#
  rbm.numCD = 2,#
  rbm.unitFunction = sigmoidUnitRbm,#
  rbm.weightDecay = .001,#
  layers = c(6,10,5),#
  darch.batchSize = 100,#
  darch.dither = T,#
  darch.initialMomentum = .4,#
  darch.finalMomentum = .9,#
  darch.momentumRampLength = .75,#
  bp.learnRate = 1,#
  bp.learnRateScale = .99,#
  darch.unitFunction = c(tanhUnit, softmaxUnit),#
  bootstrap = T,#
  darch.numEpochs = 20,#
  gputools = T, # try to use gputools#
  gputools.deviceId = 0#
)#
#
predictions <- predict(darch, newdata=X_test, type="class")#
labels <- cbind(predictions, y_test)#
numIncorrect <- sum(apply(labels, 1, function(i) { any(i[1:5] != i[6:10]) }))#
cat(paste0("Incorrect classifications on test data: ", numIncorrect," (", round(numIncorrect/nrow(y_test)*100, 2), "%)\n"))#
configs <- rbind(configs,data.frame(epochs_rbm= epochs_rbm, batch_rbm= batch_rbm, ln_rate_rbm= ln_rate_rbm, ln_scale_rbm= ln_scale_rbm, cd_rbm= cd_rbm, layers= paste(layers,collapse=" "), batch= batch, ln_rate_bp= ln_rate_bp, ln_scale_bp= ln_scale_bp, epochs_ft= epochs_ft))#
predict_list[[dim(configs)[1]]] <- predictions#
predict_norm_list[[dim(configs)[1]]] <- numIncorrect/nrow(y_test)#
nn_list[[dim(configs)[1]]] <- darch#
#
darch#
configs
?darch
# Loading library#
if(exists("configs") == F){#
	library(darch)#
	configs = data.frame(epochs_rbm=integer(), batch_rbm=integer(), ln_rate_rbm=numeric(), ln_scale_rbm=numeric(), cd_rbm=integer(), layers=character(), batch=integer(), ln_rate_bp=numeric(), ln_scale_bp=numeric(), epochs_ft=integer(), classification_error=numeric())#
	predict_list <- list()#
	predict_norm_list <- list()#
	nn_list <- list()#
}#
#
onehot <- 1#
epochs_rbm <- 10#
batch_rbm <- 100#
ln_rate_rbm <- .01#
ln_scale_rbm <- .98#
cd_rbm <- 2#
layers <- c(6,10,5)#
batch <- 100#
ln_rate_bp <- 1#
ln_scale_bp <- .99#
epochs_ft <- 20#
#
# Load csv#
#
X_train <- read.csv(file="../X_train.csv", header=T, sep=",", row.names=1)#
X_test <- read.csv(file="../X_test.csv", header=T, sep=",", row.names=1)#
y_train_org <- read.csv(file="../y_train.csv", header=T, sep=",", row.names=1)#
y_test_org <- read.csv(file="../y_test.csv", header=T, sep=",", row.names=1)#
#
X_train <- X_train[1:939,]#
X_test <- X_test[1:313,]#
#
# trying one hot encoding#
onehot_test <- matrix(0L, nrow=dim(y_test_org)[1], ncol=max(y_test_org)+1)#
counter <- 1#
for(y in 1:dim(y_test_org)[1]){#
	onehot_test[counter,y_test_org[y,]+1] <- 1#
	counter <- counter+1#
}#
# trying one hot encoding#
onehot_train <- matrix(0L, nrow=dim(y_train_org)[1], ncol=max(y_train_org)+1)#
counter <- 1#
for(y in 1:dim(y_train_org)[1]){#
	onehot_train[counter, y_train_org[y,]+1] <- 1#
	counter <- counter+1#
}#
#
X_train <- as.matrix(X_train)#
X_test <- as.matrix(X_test)#
y_train <- as.matrix(y_train_org)#
y_test <- as.matrix(y_test_org)#
#
if(onehot){#
	y_test <- onehot_test#
	y_train <- onehot_train#
}#
# only take 1000 samples, otherwise training takes increasingly long#
#chosenRowsTrain <- sample(1:nrow(trainData), size=nrow(X_train))#
#trainDataSmall <- trainData[chosenRowsTrain,]#
#trainLabelsSmall <- trainLabels[chosenRowsTrain,]#
darch  <- darch(X_train, y_train,#
  rbm.numEpochs = 10,#
  rbm.consecutive = F, # each RBM is trained one epoch at a time#
  rbm.batchSize = 100,#
  rbm.lastLayer = -1, # don't train output layer#
  rbm.allData = T, # use bootstrap validation data as well for training#
  rbm.errorFunction = rmseError,#
  rbm.initialMomentum = .5,#
  rbm.finalMomentum = .7,#
  rbm.learnRate = .01,#
  rbm.learnRateScale = .98,#
  rbm.momentumRampLength = .8,#
  rbm.numCD = 2,#
  rbm.unitFunction = sigmoidUnitRbm,#
  rbm.weightDecay = .001,#
  layers = c(6,10,5),#
  darch.batchSize = 100,#
  darch.dither = T,#
  darch.initialMomentum = .4,#
  darch.finalMomentum = .9,#
  darch.momentumRampLength = .75,#
  bp.learnRate = 1,#
  bp.learnRateScale = .99,#
  darch.unitFunction = c(tanhUnit, softmaxUnit),#
  bootstrap = T,#
  darch.numEpochs = 20,#
  gputools = T, # try to use gputools#
  gputools.deviceId = 0#
)#
#
predictions <- predict(darch, newdata=X_test, type="class")#
labels <- cbind(predictions, y_test)#
numIncorrect <- sum(apply(labels, 1, function(i) { any(i[1:5] != i[6:10]) }))#
cat(paste0("Incorrect classifications on test data: ", numIncorrect," (", round(numIncorrect/nrow(y_test)*100, 2), "%)\n"))#
configs <- rbind(configs,data.frame(epochs_rbm= epochs_rbm, batch_rbm= batch_rbm, ln_rate_rbm= ln_rate_rbm, ln_scale_rbm= ln_scale_rbm, cd_rbm= cd_rbm, layers= paste(layers,collapse=" "), batch= batch, ln_rate_bp= ln_rate_bp, ln_scale_bp= ln_scale_bp, epochs_ft= epochs_ft, classification_error=numIncorrect/nrow(y_test)))#
predict_list[[dim(configs)[1]]] <- predictions#
predict_norm_list[[dim(configs)[1]]] <- labels#
nn_list[[dim(configs)[1]]] <- darch#
#
darch#
configs
# Loading library#
if(exists("configs") == F){#
	library(darch)#
	configs = data.frame(epochs_rbm=integer(), batch_rbm=integer(), ln_rate_rbm=numeric(), ln_scale_rbm=numeric(), cd_rbm=integer(), layers=character(), batch=integer(), ln_rate_bp=numeric(), ln_scale_bp=numeric(), epochs_ft=integer(), classification_error=numeric())#
	predict_list <- list()#
	predict_norm_list <- list()#
	nn_list <- list()#
}#
#
onehot <- 1#
epochs_rbm <- 10#
batch_rbm <- 100#
ln_rate_rbm <- .01#
ln_scale_rbm <- .98#
cd_rbm <- 2#
layers <- c(6,10,5)#
batch <- 100#
ln_rate_bp <- 1#
ln_scale_bp <- .99#
epochs_ft <- 20#
#
# Load csv#
#
X_train <- read.csv(file="../X_train.csv", header=T, sep=",", row.names=1)#
X_test <- read.csv(file="../X_test.csv", header=T, sep=",", row.names=1)#
y_train_org <- read.csv(file="../y_train.csv", header=T, sep=",", row.names=1)#
y_test_org <- read.csv(file="../y_test.csv", header=T, sep=",", row.names=1)#
#
X_train <- X_train[1:939,]#
X_test <- X_test[1:313,]#
#
# trying one hot encoding#
onehot_test <- matrix(0L, nrow=dim(y_test_org)[1], ncol=max(y_test_org)+1)#
counter <- 1#
for(y in 1:dim(y_test_org)[1]){#
	onehot_test[counter,y_test_org[y,]+1] <- 1#
	counter <- counter+1#
}#
# trying one hot encoding#
onehot_train <- matrix(0L, nrow=dim(y_train_org)[1], ncol=max(y_train_org)+1)#
counter <- 1#
for(y in 1:dim(y_train_org)[1]){#
	onehot_train[counter, y_train_org[y,]+1] <- 1#
	counter <- counter+1#
}#
#
X_train <- as.matrix(X_train)#
X_test <- as.matrix(X_test)#
y_train <- as.matrix(y_train_org)#
y_test <- as.matrix(y_test_org)#
#
if(onehot){#
	y_test <- onehot_test#
	y_train <- onehot_train#
}#
# only take 1000 samples, otherwise training takes increasingly long#
#chosenRowsTrain <- sample(1:nrow(trainData), size=nrow(X_train))#
#trainDataSmall <- trainData[chosenRowsTrain,]#
#trainLabelsSmall <- trainLabels[chosenRowsTrain,]#
darch  <- darch(X_train, y_train,#
  rbm.numEpochs = 10,#
  rbm.consecutive = F, # each RBM is trained one epoch at a time#
  rbm.batchSize = 100,#
  rbm.lastLayer = -1, # don't train output layer#
  rbm.allData = T, # use bootstrap validation data as well for training#
  rbm.errorFunction = rmseError,#
  rbm.initialMomentum = .5,#
  rbm.finalMomentum = .7,#
  rbm.learnRate = .01,#
  rbm.learnRateScale = .98,#
  rbm.momentumRampLength = .8,#
  rbm.numCD = 2,#
  rbm.unitFunction = sigmoidUnitRbm,#
  rbm.weightDecay = .001,#
  layers = c(6,10,5),#
  darch.batchSize = 100,#
  darch.dither = T,#
  darch.initialMomentum = .4,#
  darch.finalMomentum = .9,#
  darch.momentumRampLength = .75,#
  bp.learnRate = 1,#
  bp.learnRateScale = .99,#
  darch.unitFunction = c(tanhUnit, softmaxUnit),#
  bootstrap = T,#
  darch.numEpochs = 20,#
  gputools = T, # try to use gputools#
  gputools.deviceId = 0,#
  xValid = X_test,#
  yValid = y_test,#
)#
#
predictions <- predict(darch, newdata=X_test, type="class")#
labels <- cbind(predictions, y_test)#
numIncorrect <- sum(apply(labels, 1, function(i) { any(i[1:5] != i[6:10]) }))#
cat(paste0("Incorrect classifications on test data: ", numIncorrect," (", round(numIncorrect/nrow(y_test)*100, 2), "%)\n"))#
configs <- rbind(configs,data.frame(epochs_rbm= epochs_rbm, batch_rbm= batch_rbm, ln_rate_rbm= ln_rate_rbm, ln_scale_rbm= ln_scale_rbm, cd_rbm= cd_rbm, layers= paste(layers,collapse=" "), batch= batch, ln_rate_bp= ln_rate_bp, ln_scale_bp= ln_scale_bp, epochs_ft= epochs_ft, classification_error=numIncorrect/nrow(y_test)))#
predict_list[[dim(configs)[1]]] <- predictions#
predict_norm_list[[dim(configs)[1]]] <- labels#
nn_list[[dim(configs)[1]]] <- darch#
#
darch#
configs
# Loading library#
if(exists("configs") == F){#
	library(darch)#
	configs = data.frame(epochs_rbm=integer(), batch_rbm=integer(), ln_rate_rbm=numeric(), ln_scale_rbm=numeric(), cd_rbm=integer(), layers=character(), batch=integer(), ln_rate_bp=numeric(), ln_scale_bp=numeric(), epochs_ft=integer(), classification_error=numeric())#
	predict_list <- list()#
	predict_norm_list <- list()#
	nn_list <- list()#
}#
#
onehot <- 1#
epochs_rbm <- 10#
batch_rbm <- 100#
ln_rate_rbm <- .01#
ln_scale_rbm <- .98#
cd_rbm <- 2#
layers <- c(6,10,5)#
batch <- 100#
ln_rate_bp <- 1#
ln_scale_bp <- .99#
epochs_ft <- 20#
#
# Load csv#
#
X_train <- read.csv(file="../X_train.csv", header=T, sep=",", row.names=1)#
X_test <- read.csv(file="../X_test.csv", header=T, sep=",", row.names=1)#
y_train_org <- read.csv(file="../y_train.csv", header=T, sep=",", row.names=1)#
y_test_org <- read.csv(file="../y_test.csv", header=T, sep=",", row.names=1)#
#
X_train <- X_train[1:939,]#
X_test <- X_test[1:313,]#
#
# trying one hot encoding#
onehot_test <- matrix(0L, nrow=dim(y_test_org)[1], ncol=max(y_test_org)+1)#
counter <- 1#
for(y in 1:dim(y_test_org)[1]){#
	onehot_test[counter,y_test_org[y,]+1] <- 1#
	counter <- counter+1#
}#
# trying one hot encoding#
onehot_train <- matrix(0L, nrow=dim(y_train_org)[1], ncol=max(y_train_org)+1)#
counter <- 1#
for(y in 1:dim(y_train_org)[1]){#
	onehot_train[counter, y_train_org[y,]+1] <- 1#
	counter <- counter+1#
}#
#
X_train <- as.matrix(X_train)#
X_test <- as.matrix(X_test)#
y_train <- as.matrix(y_train_org)#
y_test <- as.matrix(y_test_org)#
#
if(onehot){#
	y_test <- onehot_test#
	y_train <- onehot_train#
}#
# only take 1000 samples, otherwise training takes increasingly long#
#chosenRowsTrain <- sample(1:nrow(trainData), size=nrow(X_train))#
#trainDataSmall <- trainData[chosenRowsTrain,]#
#trainLabelsSmall <- trainLabels[chosenRowsTrain,]#
darch  <- darch(X_train, y_train,#
  rbm.numEpochs = 10,#
  rbm.consecutive = F, # each RBM is trained one epoch at a time#
  rbm.batchSize = 100,#
  rbm.lastLayer = -1, # don't train output layer#
  rbm.allData = T, # use bootstrap validation data as well for training#
  rbm.errorFunction = rmseError,#
  rbm.initialMomentum = .5,#
  rbm.finalMomentum = .7,#
  rbm.learnRate = .01,#
  rbm.learnRateScale = .98,#
  rbm.momentumRampLength = .8,#
  rbm.numCD = 2,#
  rbm.unitFunction = sigmoidUnitRbm,#
  rbm.weightDecay = .001,#
  layers = c(6,10,5),#
  darch.batchSize = 100,#
  darch.dither = T,#
  darch.initialMomentum = .4,#
  darch.finalMomentum = .9,#
  darch.momentumRampLength = .75,#
  bp.learnRate = 1,#
  bp.learnRateScale = .99,#
  darch.unitFunction = c(tanhUnit, softmaxUnit),#
  bootstrap = T,#
  darch.numEpochs = 20,#
  gputools = T, # try to use gputools#
  gputools.deviceId = 0,#
  xValid = X_test,#
  yValid = y_test#
)#
#
predictions <- predict(darch, newdata=X_test, type="class")#
labels <- cbind(predictions, y_test)#
numIncorrect <- sum(apply(labels, 1, function(i) { any(i[1:5] != i[6:10]) }))#
cat(paste0("Incorrect classifications on test data: ", numIncorrect," (", round(numIncorrect/nrow(y_test)*100, 2), "%)\n"))#
configs <- rbind(configs,data.frame(epochs_rbm= epochs_rbm, batch_rbm= batch_rbm, ln_rate_rbm= ln_rate_rbm, ln_scale_rbm= ln_scale_rbm, cd_rbm= cd_rbm, layers= paste(layers,collapse=" "), batch= batch, ln_rate_bp= ln_rate_bp, ln_scale_bp= ln_scale_bp, epochs_ft= epochs_ft, classification_error=numIncorrect/nrow(y_test)))#
predict_list[[dim(configs)[1]]] <- predictions#
predict_norm_list[[dim(configs)[1]]] <- labels#
nn_list[[dim(configs)[1]]] <- darch#
#
darch#
configs
(1,2,3) == (1,2,3)
c(1,2,3) == c(1,2,3)
c(1,2,3) != c(1,2,3)
any(c(1,2,3) != c(1,2,3))
# Loading library#
if(exists("configs") == F){#
	library(darch)#
	configs = data.frame(epochs_rbm=integer(), batch_rbm=integer(), ln_rate_rbm=numeric(), ln_scale_rbm=numeric(), cd_rbm=integer(), layers=character(), batch=integer(), ln_rate_bp=numeric(), ln_scale_bp=numeric(), epochs_ft=integer(), classification_error=numeric())#
	predict_list <- list()#
	predict_norm_list <- list()#
	nn_list <- list()#
}#
#
onehot <- 1#
epochs_rbm <- 10#
batch_rbm <- 100#
ln_rate_rbm <- .01#
ln_scale_rbm <- .98#
cd_rbm <- 2#
layers <- c(6,10,5)#
batch <- 100#
ln_rate_bp <- 1#
ln_scale_bp <- .99#
epochs_ft <- 20#
#
# Load csv#
#
X_train <- read.csv(file="../X_train.csv", header=T, sep=",", row.names=1)#
X_test <- read.csv(file="../X_test.csv", header=T, sep=",", row.names=1)#
y_train_org <- read.csv(file="../y_train.csv", header=T, sep=",", row.names=1)#
y_test_org <- read.csv(file="../y_test.csv", header=T, sep=",", row.names=1)#
#
X_train <- X_train[1:939,]#
X_test <- X_test[1:313,]#
#
# trying one hot encoding#
onehot_test <- matrix(0L, nrow=dim(y_test_org)[1], ncol=max(y_test_org)+1)#
counter <- 1#
for(y in 1:dim(y_test_org)[1]){#
	onehot_test[counter,y_test_org[y,]+1] <- 1#
	counter <- counter+1#
}#
# trying one hot encoding#
onehot_train <- matrix(0L, nrow=dim(y_train_org)[1], ncol=max(y_train_org)+1)#
counter <- 1#
for(y in 1:dim(y_train_org)[1]){#
	onehot_train[counter, y_train_org[y,]+1] <- 1#
	counter <- counter+1#
}#
#
X_train <- as.matrix(X_train)#
X_test <- as.matrix(X_test)#
y_train <- as.matrix(y_train_org)#
y_test <- as.matrix(y_test_org)#
#
if(onehot){#
	y_test <- onehot_test#
	y_train <- onehot_train#
}#
# only take 1000 samples, otherwise training takes increasingly long#
#chosenRowsTrain <- sample(1:nrow(trainData), size=nrow(X_train))#
#trainDataSmall <- trainData[chosenRowsTrain,]#
#trainLabelsSmall <- trainLabels[chosenRowsTrain,]#
darch  <- darch(X_train, y_train,#
  rbm.numEpochs = 10,#
  rbm.consecutive = F, # each RBM is trained one epoch at a time#
  rbm.batchSize = 100,#
  rbm.lastLayer = -1, # don't train output layer#
  rbm.allData = T, # use bootstrap validation data as well for training#
  rbm.errorFunction = rmseError,#
  rbm.initialMomentum = .5,#
  rbm.finalMomentum = .7,#
  rbm.learnRate = .01,#
  rbm.learnRateScale = .98,#
  rbm.momentumRampLength = .8,#
  rbm.numCD = 2,#
  rbm.unitFunction = sigmoidUnitRbm,#
  rbm.weightDecay = .001,#
  layers = c(6,10,5),#
  darch.batchSize = 100,#
  darch.dither = T,#
  darch.initialMomentum = .4,#
  darch.finalMomentum = .9,#
  darch.momentumRampLength = .75,#
  bp.learnRate = 1,#
  bp.learnRateScale = .99,#
  darch.unitFunction = c(tanhUnit, softmaxUnit),#
  bootstrap = T,#
  darch.numEpochs = 20,#
  gputools = T, # try to use gputools#
  gputools.deviceId = 0,#
  xValid = X_test,#
  yValid = y_test#
)#
#
predictions <- predict(darch, newdata=X_test, type="class")#
labels <- cbind(predictions, y_test)#
numIncorrect <- sum(apply(labels, 1, function(i) { any(i[1:5] != i[6:10]) }))#
cat(paste0("Incorrect classifications on test data: ", numIncorrect," (", round(numIncorrect/nrow(y_test)*100, 2), "%)\n"))#
configs <- rbind(configs,data.frame(epochs_rbm= epochs_rbm, batch_rbm= batch_rbm, ln_rate_rbm= ln_rate_rbm, ln_scale_rbm= ln_scale_rbm, cd_rbm= cd_rbm, layers= paste(layers,collapse=" "), batch= batch, ln_rate_bp= ln_rate_bp, ln_scale_bp= ln_scale_bp, epochs_ft= epochs_ft, classification_error=numIncorrect/nrow(y_test)))#
predict_list[[dim(configs)[1]]] <- predictions#
predict_norm_list[[dim(configs)[1]]] <- labels#
nn_list[[dim(configs)[1]]] <- darch#
#
darch#
configs
# Loading library#
if(exists("configs") == F){#
	library(darch)#
	configs = data.frame(epochs_rbm=integer(), batch_rbm=integer(), ln_rate_rbm=numeric(), ln_scale_rbm=numeric(), cd_rbm=integer(), layers=character(), batch=integer(), ln_rate_bp=numeric(), ln_scale_bp=numeric(), epochs_ft=integer(), classification_error=numeric())#
	predict_list <- list()#
	predict_norm_list <- list()#
	nn_list <- list()#
}#
#
onehot <- 1#
epochs_rbm <- 10#
batch_rbm <- 100#
ln_rate_rbm <- .01#
ln_scale_rbm <- .98#
cd_rbm <- 2#
layers <- c(5,10,5)#
batch <- 100#
ln_rate_bp <- 1#
ln_scale_bp <- .99#
epochs_ft <- 20#
#
# Load csv#
#
X_train <- read.csv(file="../X_train.csv", header=T, sep=",", row.names=1)#
X_test <- read.csv(file="../X_test.csv", header=T, sep=",", row.names=1)#
y_train_org <- read.csv(file="../y_train.csv", header=T, sep=",", row.names=1)#
y_test_org <- read.csv(file="../y_test.csv", header=T, sep=",", row.names=1)#
#
X_train <- X_train[1:939,]#
X_test <- X_test[1:313,]#
#
# trying one hot encoding#
onehot_test <- matrix(0L, nrow=dim(y_test_org)[1], ncol=max(y_test_org)+1)#
counter <- 1#
for(y in 1:dim(y_test_org)[1]){#
	onehot_test[counter,y_test_org[y,]+1] <- 1#
	counter <- counter+1#
}#
# trying one hot encoding#
onehot_train <- matrix(0L, nrow=dim(y_train_org)[1], ncol=max(y_train_org)+1)#
counter <- 1#
for(y in 1:dim(y_train_org)[1]){#
	onehot_train[counter, y_train_org[y,]+1] <- 1#
	counter <- counter+1#
}#
#
X_train <- as.matrix(X_train)#
X_test <- as.matrix(X_test)#
y_train <- as.matrix(y_train_org)#
y_test <- as.matrix(y_test_org)#
#
if(onehot){#
	y_test <- onehot_test#
	y_train <- onehot_train#
}#
# only take 1000 samples, otherwise training takes increasingly long#
#chosenRowsTrain <- sample(1:nrow(trainData), size=nrow(X_train))#
#trainDataSmall <- trainData[chosenRowsTrain,]#
#trainLabelsSmall <- trainLabels[chosenRowsTrain,]#
darch  <- darch(X_train, y_train,#
  rbm.numEpochs = 10,#
  rbm.consecutive = F, # each RBM is trained one epoch at a time#
  rbm.batchSize = 100,#
  rbm.lastLayer = -1, # don't train output layer#
  rbm.allData = T, # use bootstrap validation data as well for training#
  rbm.errorFunction = rmseError,#
  rbm.initialMomentum = .5,#
  rbm.finalMomentum = .7,#
  rbm.learnRate = .01,#
  rbm.learnRateScale = .98,#
  rbm.momentumRampLength = .8,#
  rbm.numCD = 2,#
  rbm.unitFunction = sigmoidUnitRbm,#
  rbm.weightDecay = .001,#
  layers = c(6,10,5),#
  darch.batchSize = 100,#
  darch.dither = T,#
  darch.initialMomentum = .4,#
  darch.finalMomentum = .9,#
  darch.momentumRampLength = .75,#
  bp.learnRate = 1,#
  bp.learnRateScale = .99,#
  darch.unitFunction = c(tanhUnit, softmaxUnit),#
  bootstrap = T,#
  darch.numEpochs = 20,#
  gputools = T, # try to use gputools#
  gputools.deviceId = 0,#
  xValid = X_test,#
  yValid = y_test#
)#
#
predictions <- predict(darch, newdata=X_test, type="class")#
labels <- cbind(predictions, y_test)#
numIncorrect <- sum(apply(labels, 1, function(i) { any(i[1:5] != i[6:10]) }))#
cat(paste0("Incorrect classifications on test data: ", numIncorrect," (", round(numIncorrect/nrow(y_test)*100, 2), "%)\n"))#
configs <- rbind(configs,data.frame(epochs_rbm= epochs_rbm, batch_rbm= batch_rbm, ln_rate_rbm= ln_rate_rbm, ln_scale_rbm= ln_scale_rbm, cd_rbm= cd_rbm, layers= paste(layers,collapse=" "), batch= batch, ln_rate_bp= ln_rate_bp, ln_scale_bp= ln_scale_bp, epochs_ft= epochs_ft, classification_error=numIncorrect/nrow(y_test)))#
predict_list[[dim(configs)[1]]] <- predictions#
predict_norm_list[[dim(configs)[1]]] <- labels#
nn_list[[dim(configs)[1]]] <- darch#
#
darch#
configs
# Loading library#
if(exists("configs") == F){#
	library(darch)#
	configs = data.frame(epochs_rbm=integer(), batch_rbm=integer(), ln_rate_rbm=numeric(), ln_scale_rbm=numeric(), cd_rbm=integer(), layers=character(), batch=integer(), ln_rate_bp=numeric(), ln_scale_bp=numeric(), epochs_ft=integer(), classification_error=numeric())#
	predict_list <- list()#
	predict_norm_list <- list()#
	nn_list <- list()#
}#
#
onehot <- 1#
epochs_rbm <- 10#
batch_rbm <- 100#
ln_rate_rbm <- .01#
ln_scale_rbm <- .98#
cd_rbm <- 2#
layers <- c(5,100,5)#
batch <- 100#
ln_rate_bp <- 1#
ln_scale_bp <- .99#
epochs_ft <- 20#
#
# Load csv#
#
X_train <- read.csv(file="../X_train.csv", header=T, sep=",", row.names=1)#
X_test <- read.csv(file="../X_test.csv", header=T, sep=",", row.names=1)#
y_train_org <- read.csv(file="../y_train.csv", header=T, sep=",", row.names=1)#
y_test_org <- read.csv(file="../y_test.csv", header=T, sep=",", row.names=1)#
#
X_train <- X_train[1:939,]#
X_test <- X_test[1:313,]#
#
# trying one hot encoding#
onehot_test <- matrix(0L, nrow=dim(y_test_org)[1], ncol=max(y_test_org)+1)#
counter <- 1#
for(y in 1:dim(y_test_org)[1]){#
	onehot_test[counter,y_test_org[y,]+1] <- 1#
	counter <- counter+1#
}#
# trying one hot encoding#
onehot_train <- matrix(0L, nrow=dim(y_train_org)[1], ncol=max(y_train_org)+1)#
counter <- 1#
for(y in 1:dim(y_train_org)[1]){#
	onehot_train[counter, y_train_org[y,]+1] <- 1#
	counter <- counter+1#
}#
#
X_train <- as.matrix(X_train)#
X_test <- as.matrix(X_test)#
y_train <- as.matrix(y_train_org)#
y_test <- as.matrix(y_test_org)#
#
if(onehot){#
	y_test <- onehot_test#
	y_train <- onehot_train#
}#
# only take 1000 samples, otherwise training takes increasingly long#
#chosenRowsTrain <- sample(1:nrow(trainData), size=nrow(X_train))#
#trainDataSmall <- trainData[chosenRowsTrain,]#
#trainLabelsSmall <- trainLabels[chosenRowsTrain,]#
darch  <- darch(X_train, y_train,#
  rbm.numEpochs = 10,#
  rbm.consecutive = F, # each RBM is trained one epoch at a time#
  rbm.batchSize = 100,#
  rbm.lastLayer = -1, # don't train output layer#
  rbm.allData = T, # use bootstrap validation data as well for training#
  rbm.errorFunction = rmseError,#
  rbm.initialMomentum = .5,#
  rbm.finalMomentum = .7,#
  rbm.learnRate = .01,#
  rbm.learnRateScale = .98,#
  rbm.momentumRampLength = .8,#
  rbm.numCD = 2,#
  rbm.unitFunction = sigmoidUnitRbm,#
  rbm.weightDecay = .001,#
  layers = c(6,10,5),#
  darch.batchSize = 100,#
  darch.dither = T,#
  darch.initialMomentum = .4,#
  darch.finalMomentum = .9,#
  darch.momentumRampLength = .75,#
  bp.learnRate = 1,#
  bp.learnRateScale = .99,#
  darch.unitFunction = c(tanhUnit, softmaxUnit),#
  bootstrap = T,#
  darch.numEpochs = 20,#
  gputools = T, # try to use gputools#
  gputools.deviceId = 0,#
  xValid = X_test,#
  yValid = y_test#
)#
#
predictions <- predict(darch, newdata=X_test, type="class")#
labels <- cbind(predictions, y_test)#
numIncorrect <- sum(apply(labels, 1, function(i) { any(i[1:5] != i[6:10]) }))#
cat(paste0("Incorrect classifications on test data: ", numIncorrect," (", round(numIncorrect/nrow(y_test)*100, 2), "%)\n"))#
configs <- rbind(configs,data.frame(epochs_rbm= epochs_rbm, batch_rbm= batch_rbm, ln_rate_rbm= ln_rate_rbm, ln_scale_rbm= ln_scale_rbm, cd_rbm= cd_rbm, layers= paste(layers,collapse=" "), batch= batch, ln_rate_bp= ln_rate_bp, ln_scale_bp= ln_scale_bp, epochs_ft= epochs_ft, classification_error=numIncorrect/nrow(y_test)))#
predict_list[[dim(configs)[1]]] <- predictions#
predict_norm_list[[dim(configs)[1]]] <- labels#
nn_list[[dim(configs)[1]]] <- darch#
#
darch#
configs
# Loading library#
if(exists("configs") == F){#
	library(darch)#
	configs = data.frame(epochs_rbm=integer(), batch_rbm=integer(), ln_rate_rbm=numeric(), ln_scale_rbm=numeric(), cd_rbm=integer(), layers=character(), batch=integer(), ln_rate_bp=numeric(), ln_scale_bp=numeric(), epochs_ft=integer(), classification_error=numeric())#
	predict_list <- list()#
	predict_norm_list <- list()#
	nn_list <- list()#
}#
#
onehot <- 1#
epochs_rbm <- 10#
batch_rbm <- 100#
ln_rate_rbm <- .01#
ln_scale_rbm <- .98#
cd_rbm <- 2#
layers <- c(5,100,5)#
batch <- 100#
ln_rate_bp <- .01#
ln_scale_bp <- .99#
epochs_ft <- 20#
#
# Load csv#
#
X_train <- read.csv(file="../X_train.csv", header=T, sep=",", row.names=1)#
X_test <- read.csv(file="../X_test.csv", header=T, sep=",", row.names=1)#
y_train_org <- read.csv(file="../y_train.csv", header=T, sep=",", row.names=1)#
y_test_org <- read.csv(file="../y_test.csv", header=T, sep=",", row.names=1)#
#
X_train <- X_train[1:939,]#
X_test <- X_test[1:313,]#
#
# trying one hot encoding#
onehot_test <- matrix(0L, nrow=dim(y_test_org)[1], ncol=max(y_test_org)+1)#
counter <- 1#
for(y in 1:dim(y_test_org)[1]){#
	onehot_test[counter,y_test_org[y,]+1] <- 1#
	counter <- counter+1#
}#
# trying one hot encoding#
onehot_train <- matrix(0L, nrow=dim(y_train_org)[1], ncol=max(y_train_org)+1)#
counter <- 1#
for(y in 1:dim(y_train_org)[1]){#
	onehot_train[counter, y_train_org[y,]+1] <- 1#
	counter <- counter+1#
}#
#
X_train <- as.matrix(X_train)#
X_test <- as.matrix(X_test)#
y_train <- as.matrix(y_train_org)#
y_test <- as.matrix(y_test_org)#
#
if(onehot){#
	y_test <- onehot_test#
	y_train <- onehot_train#
}#
# only take 1000 samples, otherwise training takes increasingly long#
#chosenRowsTrain <- sample(1:nrow(trainData), size=nrow(X_train))#
#trainDataSmall <- trainData[chosenRowsTrain,]#
#trainLabelsSmall <- trainLabels[chosenRowsTrain,]#
darch  <- darch(X_train, y_train,#
  rbm.numEpochs = 10,#
  rbm.consecutive = F, # each RBM is trained one epoch at a time#
  rbm.batchSize = 100,#
  rbm.lastLayer = -1, # don't train output layer#
  rbm.allData = T, # use bootstrap validation data as well for training#
  rbm.errorFunction = rmseError,#
  rbm.initialMomentum = .5,#
  rbm.finalMomentum = .7,#
  rbm.learnRate = .01,#
  rbm.learnRateScale = .98,#
  rbm.momentumRampLength = .8,#
  rbm.numCD = 2,#
  rbm.unitFunction = sigmoidUnitRbm,#
  rbm.weightDecay = .001,#
  layers = c(6,10,5),#
  darch.batchSize = 100,#
  darch.dither = T,#
  darch.initialMomentum = .4,#
  darch.finalMomentum = .9,#
  darch.momentumRampLength = .75,#
  bp.learnRate = 1,#
  bp.learnRateScale = .99,#
  darch.unitFunction = c(tanhUnit, softmaxUnit),#
  bootstrap = T,#
  darch.numEpochs = 20,#
  gputools = T, # try to use gputools#
  gputools.deviceId = 0,#
  xValid = X_test,#
  yValid = y_test#
)#
#
predictions <- predict(darch, newdata=X_test, type="class")#
labels <- cbind(predictions, y_test)#
numIncorrect <- sum(apply(labels, 1, function(i) { any(i[1:5] != i[6:10]) }))#
cat(paste0("Incorrect classifications on test data: ", numIncorrect," (", round(numIncorrect/nrow(y_test)*100, 2), "%)\n"))#
configs <- rbind(configs,data.frame(epochs_rbm= epochs_rbm, batch_rbm= batch_rbm, ln_rate_rbm= ln_rate_rbm, ln_scale_rbm= ln_scale_rbm, cd_rbm= cd_rbm, layers= paste(layers,collapse=" "), batch= batch, ln_rate_bp= ln_rate_bp, ln_scale_bp= ln_scale_bp, epochs_ft= epochs_ft, classification_error=numIncorrect/nrow(y_test)))#
predict_list[[dim(configs)[1]]] <- predictions#
predict_norm_list[[dim(configs)[1]]] <- labels#
nn_list[[dim(configs)[1]]] <- darch#
#
darch#
configs
y_train
# Loading library#
if(exists("configs") == F){#
	library(darch)#
	configs = data.frame(epochs_rbm=integer(), batch_rbm=integer(), ln_rate_rbm=numeric(), ln_scale_rbm=numeric(), cd_rbm=integer(), layers=character(), batch=integer(), ln_rate_bp=numeric(), ln_scale_bp=numeric(), epochs_ft=integer(), classification_error=numeric())#
	predict_list <- list()#
	predict_norm_list <- list()#
	nn_list <- list()#
}#
#
onehot <- 1#
epochs_rbm <- 10#
batch_rbm <- 100#
ln_rate_rbm <- .01#
ln_scale_rbm <- .98#
cd_rbm <- 10#
layers <- c(5,100,5)#
batch <- 100#
ln_rate_bp <- .01#
ln_scale_bp <- .99#
epochs_ft <- 20#
#
# Load csv#
#
X_train <- read.csv(file="../X_train.csv", header=T, sep=",", row.names=1)#
X_test <- read.csv(file="../X_test.csv", header=T, sep=",", row.names=1)#
y_train_org <- read.csv(file="../y_train.csv", header=T, sep=",", row.names=1)#
y_test_org <- read.csv(file="../y_test.csv", header=T, sep=",", row.names=1)#
#
X_train <- X_train[1:939,]#
X_test <- X_test[1:313,]#
#
# trying one hot encoding#
onehot_test <- matrix(0L, nrow=dim(y_test_org)[1], ncol=max(y_test_org)+1)#
counter <- 1#
for(y in 1:dim(y_test_org)[1]){#
	onehot_test[counter,y_test_org[y,]+1] <- 1#
	counter <- counter+1#
}#
# trying one hot encoding#
onehot_train <- matrix(0L, nrow=dim(y_train_org)[1], ncol=max(y_train_org)+1)#
counter <- 1#
for(y in 1:dim(y_train_org)[1]){#
	onehot_train[counter, y_train_org[y,]+1] <- 1#
	counter <- counter+1#
}#
#
X_train <- as.matrix(X_train)#
X_test <- as.matrix(X_test)#
y_train <- as.matrix(y_train_org)#
y_test <- as.matrix(y_test_org)#
#
if(onehot){#
	y_test <- onehot_test#
	y_train <- onehot_train#
}#
# only take 1000 samples, otherwise training takes increasingly long#
#chosenRowsTrain <- sample(1:nrow(trainData), size=nrow(X_train))#
#trainDataSmall <- trainData[chosenRowsTrain,]#
#trainLabelsSmall <- trainLabels[chosenRowsTrain,]#
darch  <- darch(X_train, y_train,#
  rbm.numEpochs = 10,#
  rbm.consecutive = F, # each RBM is trained one epoch at a time#
  rbm.batchSize = 100,#
  rbm.lastLayer = -1, # don't train output layer#
  rbm.allData = T, # use bootstrap validation data as well for training#
  rbm.errorFunction = rmseError,#
  rbm.initialMomentum = .5,#
  rbm.finalMomentum = .7,#
  rbm.learnRate = .01,#
  rbm.learnRateScale = .98,#
  rbm.momentumRampLength = .8,#
  rbm.numCD = 2,#
  rbm.unitFunction = sigmoidUnitRbm,#
  rbm.weightDecay = .001,#
  layers = c(6,10,5),#
  darch.batchSize = 100,#
  darch.dither = T,#
  darch.initialMomentum = .4,#
  darch.finalMomentum = .9,#
  darch.momentumRampLength = .75,#
  bp.learnRate = 1,#
  bp.learnRateScale = .99,#
  darch.unitFunction = c(tanhUnit, softmaxUnit),#
  bootstrap = T,#
  darch.numEpochs = 20,#
  gputools = T, # try to use gputools#
  gputools.deviceId = 0,#
  xValid = X_test,#
  yValid = y_test#
)#
#
predictions <- predict(darch, newdata=X_test, type="class")#
labels <- cbind(predictions, y_test)#
numIncorrect <- sum(apply(labels, 1, function(i) { any(i[1:5] != i[6:10]) }))#
cat(paste0("Incorrect classifications on test data: ", numIncorrect," (", round(numIncorrect/nrow(y_test)*100, 2), "%)\n"))#
configs <- rbind(configs,data.frame(epochs_rbm= epochs_rbm, batch_rbm= batch_rbm, ln_rate_rbm= ln_rate_rbm, ln_scale_rbm= ln_scale_rbm, cd_rbm= cd_rbm, layers= paste(layers,collapse=" "), batch= batch, ln_rate_bp= ln_rate_bp, ln_scale_bp= ln_scale_bp, epochs_ft= epochs_ft, classification_error=numIncorrect/nrow(y_test)))#
predict_list[[dim(configs)[1]]] <- predictions#
predict_norm_list[[dim(configs)[1]]] <- labels#
nn_list[[dim(configs)[1]]] <- darch#
#
darch#
configs
# Loading library#
if(exists("configs") == F){#
	library(darch)#
	configs = data.frame(epochs_rbm=integer(), batch_rbm=integer(), ln_rate_rbm=numeric(), ln_scale_rbm=numeric(), cd_rbm=integer(), layers=character(), batch=integer(), ln_rate_bp=numeric(), ln_scale_bp=numeric(), epochs_ft=integer(), classification_error=numeric())#
	predict_list <- list()#
	predict_norm_list <- list()#
	nn_list <- list()#
}#
#
onehot <- 1#
epochs_rbm <- 10#
batch_rbm <- 100#
ln_rate_rbm <- .001#
ln_scale_rbm <- .8#
cd_rbm <- 10#
layers <- c(5,100,5)#
batch <- 100#
ln_rate_bp <- .01#
ln_scale_bp <- .8#
epochs_ft <- 20#
#
# Load csv#
#
X_train <- read.csv(file="../X_train.csv", header=T, sep=",", row.names=1)#
X_test <- read.csv(file="../X_test.csv", header=T, sep=",", row.names=1)#
y_train_org <- read.csv(file="../y_train.csv", header=T, sep=",", row.names=1)#
y_test_org <- read.csv(file="../y_test.csv", header=T, sep=",", row.names=1)#
#
X_train <- X_train[1:939,]#
X_test <- X_test[1:313,]#
#
# trying one hot encoding#
onehot_test <- matrix(0L, nrow=dim(y_test_org)[1], ncol=max(y_test_org)+1)#
counter <- 1#
for(y in 1:dim(y_test_org)[1]){#
	onehot_test[counter,y_test_org[y,]+1] <- 1#
	counter <- counter+1#
}#
# trying one hot encoding#
onehot_train <- matrix(0L, nrow=dim(y_train_org)[1], ncol=max(y_train_org)+1)#
counter <- 1#
for(y in 1:dim(y_train_org)[1]){#
	onehot_train[counter, y_train_org[y,]+1] <- 1#
	counter <- counter+1#
}#
#
X_train <- as.matrix(X_train)#
X_test <- as.matrix(X_test)#
y_train <- as.matrix(y_train_org)#
y_test <- as.matrix(y_test_org)#
#
if(onehot){#
	y_test <- onehot_test#
	y_train <- onehot_train#
}#
# only take 1000 samples, otherwise training takes increasingly long#
#chosenRowsTrain <- sample(1:nrow(trainData), size=nrow(X_train))#
#trainDataSmall <- trainData[chosenRowsTrain,]#
#trainLabelsSmall <- trainLabels[chosenRowsTrain,]#
darch  <- darch(X_train, y_train,#
  rbm.numEpochs = 10,#
  rbm.consecutive = F, # each RBM is trained one epoch at a time#
  rbm.batchSize = 100,#
  rbm.lastLayer = -1, # don't train output layer#
  rbm.allData = T, # use bootstrap validation data as well for training#
  rbm.errorFunction = rmseError,#
  rbm.initialMomentum = .5,#
  rbm.finalMomentum = .7,#
  rbm.learnRate = .01,#
  rbm.learnRateScale = .98,#
  rbm.momentumRampLength = .8,#
  rbm.numCD = 2,#
  rbm.unitFunction = sigmoidUnitRbm,#
  rbm.weightDecay = .001,#
  layers = c(6,10,5),#
  darch.batchSize = 100,#
  darch.dither = T,#
  darch.initialMomentum = .4,#
  darch.finalMomentum = .9,#
  darch.momentumRampLength = .75,#
  bp.learnRate = 1,#
  bp.learnRateScale = .99,#
  darch.unitFunction = c(tanhUnit, softmaxUnit),#
  bootstrap = T,#
  darch.numEpochs = 20,#
  gputools = T, # try to use gputools#
  gputools.deviceId = 0,#
  xValid = X_test,#
  yValid = y_test#
)#
#
predictions <- predict(darch, newdata=X_test, type="class")#
labels <- cbind(predictions, y_test)#
numIncorrect <- sum(apply(labels, 1, function(i) { any(i[1:5] != i[6:10]) }))#
cat(paste0("Incorrect classifications on test data: ", numIncorrect," (", round(numIncorrect/nrow(y_test)*100, 2), "%)\n"))#
configs <- rbind(configs,data.frame(epochs_rbm= epochs_rbm, batch_rbm= batch_rbm, ln_rate_rbm= ln_rate_rbm, ln_scale_rbm= ln_scale_rbm, cd_rbm= cd_rbm, layers= paste(layers,collapse=" "), batch= batch, ln_rate_bp= ln_rate_bp, ln_scale_bp= ln_scale_bp, epochs_ft= epochs_ft, classification_error=numIncorrect/nrow(y_test)))#
predict_list[[dim(configs)[1]]] <- predictions#
predict_norm_list[[dim(configs)[1]]] <- labels#
nn_list[[dim(configs)[1]]] <- darch#
#
darch#
configs
# Loading library#
if(exists("configs") == F){#
	library(darch)#
	configs = data.frame(epochs_rbm=integer(), batch_rbm=integer(), ln_rate_rbm=numeric(), ln_scale_rbm=numeric(), cd_rbm=integer(), layers=character(), batch=integer(), ln_rate_bp=numeric(), ln_scale_bp=numeric(), epochs_ft=integer(), classification_error=numeric())#
	predict_list <- list()#
	predict_norm_list <- list()#
	nn_list <- list()#
}#
#
onehot <- 1#
epochs_rbm <- 10#
batch_rbm <- 100#
ln_rate_rbm <- .001#
ln_scale_rbm <- .8#
cd_rbm <- 10#
layers <- c(5,100, 100,5)#
batch <- 100#
ln_rate_bp <- .01#
ln_scale_bp <- .8#
epochs_ft <- 20#
#
# Load csv#
#
X_train <- read.csv(file="../X_train.csv", header=T, sep=",", row.names=1)#
X_test <- read.csv(file="../X_test.csv", header=T, sep=",", row.names=1)#
y_train_org <- read.csv(file="../y_train.csv", header=T, sep=",", row.names=1)#
y_test_org <- read.csv(file="../y_test.csv", header=T, sep=",", row.names=1)#
#
X_train <- X_train[1:939,]#
X_test <- X_test[1:313,]#
#
# trying one hot encoding#
onehot_test <- matrix(0L, nrow=dim(y_test_org)[1], ncol=max(y_test_org)+1)#
counter <- 1#
for(y in 1:dim(y_test_org)[1]){#
	onehot_test[counter,y_test_org[y,]+1] <- 1#
	counter <- counter+1#
}#
# trying one hot encoding#
onehot_train <- matrix(0L, nrow=dim(y_train_org)[1], ncol=max(y_train_org)+1)#
counter <- 1#
for(y in 1:dim(y_train_org)[1]){#
	onehot_train[counter, y_train_org[y,]+1] <- 1#
	counter <- counter+1#
}#
#
X_train <- as.matrix(X_train)#
X_test <- as.matrix(X_test)#
y_train <- as.matrix(y_train_org)#
y_test <- as.matrix(y_test_org)#
#
if(onehot){#
	y_test <- onehot_test#
	y_train <- onehot_train#
}#
# only take 1000 samples, otherwise training takes increasingly long#
#chosenRowsTrain <- sample(1:nrow(trainData), size=nrow(X_train))#
#trainDataSmall <- trainData[chosenRowsTrain,]#
#trainLabelsSmall <- trainLabels[chosenRowsTrain,]#
darch  <- darch(X_train, y_train,#
  rbm.numEpochs = 10,#
  rbm.consecutive = F, # each RBM is trained one epoch at a time#
  rbm.batchSize = 100,#
  rbm.lastLayer = -1, # don't train output layer#
  rbm.allData = T, # use bootstrap validation data as well for training#
  rbm.errorFunction = rmseError,#
  rbm.initialMomentum = .5,#
  rbm.finalMomentum = .7,#
  rbm.learnRate = .01,#
  rbm.learnRateScale = .98,#
  rbm.momentumRampLength = .8,#
  rbm.numCD = 2,#
  rbm.unitFunction = sigmoidUnitRbm,#
  rbm.weightDecay = .001,#
  layers = c(6,10,5),#
  darch.batchSize = 100,#
  darch.dither = T,#
  darch.initialMomentum = .4,#
  darch.finalMomentum = .9,#
  darch.momentumRampLength = .75,#
  bp.learnRate = 1,#
  bp.learnRateScale = .99,#
  darch.unitFunction = c(tanhUnit, softmaxUnit),#
  bootstrap = T,#
  darch.numEpochs = 20,#
  gputools = T, # try to use gputools#
  gputools.deviceId = 0,#
  xValid = X_test,#
  yValid = y_test#
)#
#
predictions <- predict(darch, newdata=X_test, type="class")#
labels <- cbind(predictions, y_test)#
numIncorrect <- sum(apply(labels, 1, function(i) { any(i[1:5] != i[6:10]) }))#
cat(paste0("Incorrect classifications on test data: ", numIncorrect," (", round(numIncorrect/nrow(y_test)*100, 2), "%)\n"))#
configs <- rbind(configs,data.frame(epochs_rbm= epochs_rbm, batch_rbm= batch_rbm, ln_rate_rbm= ln_rate_rbm, ln_scale_rbm= ln_scale_rbm, cd_rbm= cd_rbm, layers= paste(layers,collapse=" "), batch= batch, ln_rate_bp= ln_rate_bp, ln_scale_bp= ln_scale_bp, epochs_ft= epochs_ft, classification_error=numIncorrect/nrow(y_test)))#
predict_list[[dim(configs)[1]]] <- predictions#
predict_norm_list[[dim(configs)[1]]] <- labels#
nn_list[[dim(configs)[1]]] <- darch#
#
darch#
configs
# Loading library#
if(exists("configs") == F){#
	library(darch)#
	configs = data.frame(epochs_rbm=integer(), batch_rbm=integer(), ln_rate_rbm=numeric(), ln_scale_rbm=numeric(), cd_rbm=integer(), layers=character(), batch=integer(), ln_rate_bp=numeric(), ln_scale_bp=numeric(), epochs_ft=integer(), classification_error=numeric())#
	predict_list <- list()#
	predict_norm_list <- list()#
	nn_list <- list()#
}#
#
onehot <- 1#
epochs_rbm <- 10#
batch_rbm <- 100#
ln_rate_rbm <- .1#
ln_scale_rbm <- .98#
cd_rbm <- 10#
layers <- c(5,10,5)#
batch <- 100#
ln_rate_bp <- .1#
ln_scale_bp <- .98#
epochs_ft <- 20#
#
# Load csv#
#
X_train <- read.csv(file="../X_train.csv", header=T, sep=",", row.names=1)#
X_test <- read.csv(file="../X_test.csv", header=T, sep=",", row.names=1)#
y_train_org <- read.csv(file="../y_train.csv", header=T, sep=",", row.names=1)#
y_test_org <- read.csv(file="../y_test.csv", header=T, sep=",", row.names=1)#
#
X_train <- X_train[1:939,]#
X_test <- X_test[1:313,]#
#
# trying one hot encoding#
onehot_test <- matrix(0L, nrow=dim(y_test_org)[1], ncol=max(y_test_org)+1)#
counter <- 1#
for(y in 1:dim(y_test_org)[1]){#
	onehot_test[counter,y_test_org[y,]+1] <- 1#
	counter <- counter+1#
}#
# trying one hot encoding#
onehot_train <- matrix(0L, nrow=dim(y_train_org)[1], ncol=max(y_train_org)+1)#
counter <- 1#
for(y in 1:dim(y_train_org)[1]){#
	onehot_train[counter, y_train_org[y,]+1] <- 1#
	counter <- counter+1#
}#
#
X_train <- as.matrix(X_train)#
X_test <- as.matrix(X_test)#
y_train <- as.matrix(y_train_org)#
y_test <- as.matrix(y_test_org)#
#
if(onehot){#
	y_test <- onehot_test#
	y_train <- onehot_train#
}#
# only take 1000 samples, otherwise training takes increasingly long#
#chosenRowsTrain <- sample(1:nrow(trainData), size=nrow(X_train))#
#trainDataSmall <- trainData[chosenRowsTrain,]#
#trainLabelsSmall <- trainLabels[chosenRowsTrain,]#
darch  <- darch(X_train, y_train,#
  rbm.numEpochs = epochs_rbm,#
  rbm.consecutive = F, # each RBM is trained one epoch at a time#
  rbm.batchSize = batch_rbm,#
  rbm.lastLayer = -1, # don't train output layer#
  rbm.allData = T, # use bootstrap validation data as well for training#
  rbm.errorFunction = rmseError,#
  rbm.initialMomentum = .5,#
  rbm.finalMomentum = .7,#
  rbm.learnRate = ln_rate_rbm,#
  rbm.learnRateScale = ln_scale_rbm,#
  rbm.momentumRampLength = .8,#
  rbm.numCD = cd_rbm,#
  rbm.unitFunction = sigmoidUnitRbm,#
  rbm.weightDecay = .001,#
  layers = c(6,10,5),#
  darch.batchSize = 100,#
  darch.dither = T,#
  darch.initialMomentum = .4,#
  darch.finalMomentum = .9,#
  darch.momentumRampLength = .75,#
  bp.learnRate = ln_rate_bp,#
  bp.learnRateScale = ln_scale_bp,#
  darch.unitFunction = c(tanhUnit, softmaxUnit),#
  bootstrap = T,#
  darch.numEpochs = epochsft,#
  gputools = T, # try to use gputools#
  gputools.deviceId = 0,#
  xValid = X_test,#
  yValid = y_test#
)#
#
predictions <- predict(darch, newdata=X_test, type="class")#
labels <- cbind(predictions, y_test)#
numIncorrect <- sum(apply(labels, 1, function(i) { any(i[1:5] != i[6:10]) }))#
cat(paste0("Incorrect classifications on test data: ", numIncorrect," (", round(numIncorrect/nrow(y_test)*100, 2), "%)\n"))#
configs <- rbind(configs,data.frame(epochs_rbm= epochs_rbm, batch_rbm= batch_rbm, ln_rate_rbm= ln_rate_rbm, ln_scale_rbm= ln_scale_rbm, cd_rbm= cd_rbm, layers= paste(layers,collapse=" "), batch= batch, ln_rate_bp= ln_rate_bp, ln_scale_bp= ln_scale_bp, epochs_ft= epochs_ft, classification_error=numIncorrect/nrow(y_test)))#
predict_list[[dim(configs)[1]]] <- predictions#
predict_norm_list[[dim(configs)[1]]] <- labels#
nn_list[[dim(configs)[1]]] <- darch#
#
darch#
configs
# Loading library#
if(exists("configs") == F){#
	library(darch)#
	configs = data.frame(epochs_rbm=integer(), batch_rbm=integer(), ln_rate_rbm=numeric(), ln_scale_rbm=numeric(), cd_rbm=integer(), layers=character(), batch=integer(), ln_rate_bp=numeric(), ln_scale_bp=numeric(), epochs_ft=integer(), classification_error=numeric())#
	predict_list <- list()#
	predict_norm_list <- list()#
	nn_list <- list()#
}#
#
onehot <- 1#
epochs_rbm <- 10#
batch_rbm <- 100#
ln_rate_rbm <- .1#
ln_scale_rbm <- .98#
cd_rbm <- 10#
layers <- c(5,10,5)#
batch <- 100#
ln_rate_bp <- .1#
ln_scale_bp <- .98#
epochs_ft <- 20#
#
# Load csv#
#
X_train <- read.csv(file="../X_train.csv", header=T, sep=",", row.names=1)#
X_test <- read.csv(file="../X_test.csv", header=T, sep=",", row.names=1)#
y_train_org <- read.csv(file="../y_train.csv", header=T, sep=",", row.names=1)#
y_test_org <- read.csv(file="../y_test.csv", header=T, sep=",", row.names=1)#
#
X_train <- X_train[1:939,]#
X_test <- X_test[1:313,]#
#
# trying one hot encoding#
onehot_test <- matrix(0L, nrow=dim(y_test_org)[1], ncol=max(y_test_org)+1)#
counter <- 1#
for(y in 1:dim(y_test_org)[1]){#
	onehot_test[counter,y_test_org[y,]+1] <- 1#
	counter <- counter+1#
}#
# trying one hot encoding#
onehot_train <- matrix(0L, nrow=dim(y_train_org)[1], ncol=max(y_train_org)+1)#
counter <- 1#
for(y in 1:dim(y_train_org)[1]){#
	onehot_train[counter, y_train_org[y,]+1] <- 1#
	counter <- counter+1#
}#
#
X_train <- as.matrix(X_train)#
X_test <- as.matrix(X_test)#
y_train <- as.matrix(y_train_org)#
y_test <- as.matrix(y_test_org)#
#
if(onehot){#
	y_test <- onehot_test#
	y_train <- onehot_train#
}#
# only take 1000 samples, otherwise training takes increasingly long#
#chosenRowsTrain <- sample(1:nrow(trainData), size=nrow(X_train))#
#trainDataSmall <- trainData[chosenRowsTrain,]#
#trainLabelsSmall <- trainLabels[chosenRowsTrain,]#
darch  <- darch(X_train, y_train,#
  rbm.numEpochs = epochs_rbm,#
  rbm.consecutive = F, # each RBM is trained one epoch at a time#
  rbm.batchSize = batch_rbm,#
  rbm.lastLayer = -1, # don't train output layer#
  rbm.allData = T, # use bootstrap validation data as well for training#
  rbm.errorFunction = rmseError,#
  rbm.initialMomentum = .5,#
  rbm.finalMomentum = .7,#
  rbm.learnRate = ln_rate_rbm,#
  rbm.learnRateScale = ln_scale_rbm,#
  rbm.momentumRampLength = .8,#
  rbm.numCD = cd_rbm,#
  rbm.unitFunction = sigmoidUnitRbm,#
  rbm.weightDecay = .001,#
  layers = c(6,10,5),#
  darch.batchSize = 100,#
  darch.dither = T,#
  darch.initialMomentum = .4,#
  darch.finalMomentum = .9,#
  darch.momentumRampLength = .75,#
  bp.learnRate = ln_rate_bp,#
  bp.learnRateScale = ln_scale_bp,#
  darch.unitFunction = c(tanhUnit, softmaxUnit),#
  bootstrap = T,#
  darch.numEpochs = epochs_ft,#
  gputools = T, # try to use gputools#
  gputools.deviceId = 0,#
  xValid = X_test,#
  yValid = y_test#
)#
#
predictions <- predict(darch, newdata=X_test, type="class")#
labels <- cbind(predictions, y_test)#
numIncorrect <- sum(apply(labels, 1, function(i) { any(i[1:5] != i[6:10]) }))#
cat(paste0("Incorrect classifications on test data: ", numIncorrect," (", round(numIncorrect/nrow(y_test)*100, 2), "%)\n"))#
configs <- rbind(configs,data.frame(epochs_rbm= epochs_rbm, batch_rbm= batch_rbm, ln_rate_rbm= ln_rate_rbm, ln_scale_rbm= ln_scale_rbm, cd_rbm= cd_rbm, layers= paste(layers,collapse=" "), batch= batch, ln_rate_bp= ln_rate_bp, ln_scale_bp= ln_scale_bp, epochs_ft= epochs_ft, classification_error=numIncorrect/nrow(y_test)))#
predict_list[[dim(configs)[1]]] <- predictions#
predict_norm_list[[dim(configs)[1]]] <- labels#
nn_list[[dim(configs)[1]]] <- darch#
#
darch#
configs
# Loading library#
if(exists("configs") == F){#
	library(darch)#
	configs = data.frame(epochs_rbm=integer(), batch_rbm=integer(), ln_rate_rbm=numeric(), ln_scale_rbm=numeric(), cd_rbm=integer(), layers=character(), batch=integer(), ln_rate_bp=numeric(), ln_scale_bp=numeric(), epochs_ft=integer(), classification_error=numeric())#
	predict_list <- list()#
	predict_norm_list <- list()#
	nn_list <- list()#
}#
#
onehot <- 1#
epochs_rbm <- 10#
batch_rbm <- 100#
ln_rate_rbm <- .1#
ln_scale_rbm <- .98#
cd_rbm <- 10#
layers <- c(5,100,5)#
batch <- 100#
ln_rate_bp <- .1#
ln_scale_bp <- .98#
epochs_ft <- 20#
#
# Load csv#
#
X_train <- read.csv(file="../X_train.csv", header=T, sep=",", row.names=1)#
X_test <- read.csv(file="../X_test.csv", header=T, sep=",", row.names=1)#
y_train_org <- read.csv(file="../y_train.csv", header=T, sep=",", row.names=1)#
y_test_org <- read.csv(file="../y_test.csv", header=T, sep=",", row.names=1)#
#
X_train <- X_train[1:939,]#
X_test <- X_test[1:313,]#
#
# trying one hot encoding#
onehot_test <- matrix(0L, nrow=dim(y_test_org)[1], ncol=max(y_test_org)+1)#
counter <- 1#
for(y in 1:dim(y_test_org)[1]){#
	onehot_test[counter,y_test_org[y,]+1] <- 1#
	counter <- counter+1#
}#
# trying one hot encoding#
onehot_train <- matrix(0L, nrow=dim(y_train_org)[1], ncol=max(y_train_org)+1)#
counter <- 1#
for(y in 1:dim(y_train_org)[1]){#
	onehot_train[counter, y_train_org[y,]+1] <- 1#
	counter <- counter+1#
}#
#
X_train <- as.matrix(X_train)#
X_test <- as.matrix(X_test)#
y_train <- as.matrix(y_train_org)#
y_test <- as.matrix(y_test_org)#
#
if(onehot){#
	y_test <- onehot_test#
	y_train <- onehot_train#
}#
# only take 1000 samples, otherwise training takes increasingly long#
#chosenRowsTrain <- sample(1:nrow(trainData), size=nrow(X_train))#
#trainDataSmall <- trainData[chosenRowsTrain,]#
#trainLabelsSmall <- trainLabels[chosenRowsTrain,]#
darch  <- darch(X_train, y_train,#
  rbm.numEpochs = epochs_rbm,#
  rbm.consecutive = F, # each RBM is trained one epoch at a time#
  rbm.batchSize = batch_rbm,#
  rbm.lastLayer = -1, # don't train output layer#
  rbm.allData = T, # use bootstrap validation data as well for training#
  rbm.errorFunction = rmseError,#
  rbm.initialMomentum = .5,#
  rbm.finalMomentum = .7,#
  rbm.learnRate = ln_rate_rbm,#
  rbm.learnRateScale = ln_scale_rbm,#
  rbm.momentumRampLength = .8,#
  rbm.numCD = cd_rbm,#
  rbm.unitFunction = sigmoidUnitRbm,#
  rbm.weightDecay = .001,#
  layers = layers,#
  darch.batchSize = 100,#
  darch.dither = T,#
  darch.initialMomentum = .4,#
  darch.finalMomentum = .9,#
  darch.momentumRampLength = .75,#
  bp.learnRate = ln_rate_bp,#
  bp.learnRateScale = ln_scale_bp,#
  darch.unitFunction = c(tanhUnit, softmaxUnit),#
  bootstrap = T,#
  darch.numEpochs = epochs_ft,#
  gputools = T, # try to use gputools#
  gputools.deviceId = 0,#
  xValid = X_test,#
  yValid = y_test#
)#
#
predictions <- predict(darch, newdata=X_test, type="class")#
labels <- cbind(predictions, y_test)#
numIncorrect <- sum(apply(labels, 1, function(i) { any(i[1:5] != i[6:10]) }))#
cat(paste0("Incorrect classifications on test data: ", numIncorrect," (", round(numIncorrect/nrow(y_test)*100, 2), "%)\n"))#
configs <- rbind(configs,data.frame(epochs_rbm= epochs_rbm, batch_rbm= batch_rbm, ln_rate_rbm= ln_rate_rbm, ln_scale_rbm= ln_scale_rbm, cd_rbm= cd_rbm, layers= paste(layers,collapse=" "), batch= batch, ln_rate_bp= ln_rate_bp, ln_scale_bp= ln_scale_bp, epochs_ft= epochs_ft, classification_error=numIncorrect/nrow(y_test)))#
predict_list[[dim(configs)[1]]] <- predictions#
predict_norm_list[[dim(configs)[1]]] <- labels#
nn_list[[dim(configs)[1]]] <- darch#
#
darch#
configs
# Loading library#
if(exists("configs") == F){#
	library(darch)#
	configs = data.frame(epochs_rbm=integer(), batch_rbm=integer(), ln_rate_rbm=numeric(), ln_scale_rbm=numeric(), cd_rbm=integer(), layers=character(), batch=integer(), ln_rate_bp=numeric(), ln_scale_bp=numeric(), epochs_ft=integer(), classification_error=numeric())#
	predict_list <- list()#
	predict_norm_list <- list()#
	nn_list <- list()#
}#
#
onehot <- 1#
epochs_rbm <- 10#
batch_rbm <- 100#
ln_rate_rbm <- .1#
ln_scale_rbm <- .98#
cd_rbm <- 10#
layers <- c(5,100,5)#
batch <- 100#
ln_rate_bp <- .1#
ln_scale_bp <- .98#
epochs_ft <- 20#
#
# Load csv#
#
X_train <- read.csv(file="../X_train.csv", header=T, sep=",", row.names=1)#
X_test <- read.csv(file="../X_test.csv", header=T, sep=",", row.names=1)#
y_train_org <- read.csv(file="../y_train.csv", header=T, sep=",", row.names=1)#
y_test_org <- read.csv(file="../y_test.csv", header=T, sep=",", row.names=1)#
#
X_train <- X_train[1:939,]#
X_test <- X_test[1:313,]#
#
# trying one hot encoding#
onehot_test <- matrix(0L, nrow=dim(y_test_org)[1], ncol=max(y_test_org)+1)#
counter <- 1#
for(y in 1:dim(y_test_org)[1]){#
	onehot_test[counter,y_test_org[y,]+1] <- 1#
	counter <- counter+1#
}#
# trying one hot encoding#
onehot_train <- matrix(0L, nrow=dim(y_train_org)[1], ncol=max(y_train_org)+1)#
counter <- 1#
for(y in 1:dim(y_train_org)[1]){#
	onehot_train[counter, y_train_org[y,]+1] <- 1#
	counter <- counter+1#
}#
#
X_train <- as.matrix(X_train)#
X_test <- as.matrix(X_test)#
y_train <- as.matrix(y_train_org)#
y_test <- as.matrix(y_test_org)#
#
if(onehot){#
	y_test <- onehot_test#
	y_train <- onehot_train#
}#
# only take 1000 samples, otherwise training takes increasingly long#
#chosenRowsTrain <- sample(1:nrow(trainData), size=nrow(X_train))#
#trainDataSmall <- trainData[chosenRowsTrain,]#
#trainLabelsSmall <- trainLabels[chosenRowsTrain,]#
darch  <- darch(X_train, y_train,#
  rbm.numEpochs = epochs_rbm,#
  rbm.consecutive = F, # each RBM is trained one epoch at a time#
  rbm.batchSize = batch_rbm,#
  rbm.lastLayer = -1, # don't train output layer#
  rbm.allData = T, # use bootstrap validation data as well for training#
  rbm.errorFunction = rmseError,#
  rbm.initialMomentum = .5,#
  rbm.finalMomentum = .7,#
  rbm.learnRate = ln_rate_rbm,#
  rbm.learnRateScale = ln_scale_rbm,#
  rbm.momentumRampLength = .8,#
  rbm.numCD = cd_rbm,#
  rbm.unitFunction = sigmoidUnitRbm,#
  rbm.weightDecay = .001,#
  layers = layers,#
  darch.batchSize = batch,#
  darch.dither = T,#
  darch.initialMomentum = .4,#
  darch.finalMomentum = .9,#
  darch.momentumRampLength = .75,#
  bp.learnRate = ln_rate_bp,#
  bp.learnRateScale = ln_scale_bp,#
  darch.unitFunction = c(tanhUnit, softmaxUnit),#
  bootstrap = T,#
  darch.numEpochs = epochs_ft,#
  gputools = T, # try to use gputools#
  gputools.deviceId = 0,#
  xValid = X_test,#
  yValid = y_test#
)#
#
predictions <- predict(darch, newdata=X_test, type="class")#
labels <- cbind(predictions, y_test)#
numIncorrect <- sum(apply(labels, 1, function(i) { any(i[1:5] != i[6:10]) }))#
cat(paste0("Incorrect classifications on test data: ", numIncorrect," (", round(numIncorrect/nrow(y_test)*100, 2), "%)\n"))#
configs <- rbind(configs,data.frame(epochs_rbm= epochs_rbm, batch_rbm= batch_rbm, ln_rate_rbm= ln_rate_rbm, ln_scale_rbm= ln_scale_rbm, cd_rbm= cd_rbm, layers= paste(layers,collapse=" "), batch= batch, ln_rate_bp= ln_rate_bp, ln_scale_bp= ln_scale_bp, epochs_ft= epochs_ft, classification_error=numIncorrect/nrow(y_test)))#
predict_list[[dim(configs)[1]]] <- predictions#
predict_norm_list[[dim(configs)[1]]] <- labels#
nn_list[[dim(configs)[1]]] <- darch#
#
darch#
configs
# Loading library#
if(exists("configs") == F){#
	library(darch)#
	configs = data.frame(epochs_rbm=integer(), batch_rbm=integer(), ln_rate_rbm=numeric(), ln_scale_rbm=numeric(), cd_rbm=integer(), layers=character(), batch=integer(), ln_rate_bp=numeric(), ln_scale_bp=numeric(), epochs_ft=integer(), classification_error=numeric())#
	predict_list <- list()#
	predict_norm_list <- list()#
	nn_list <- list()#
}#
#
onehot <- 1#
epochs_rbm <- 10#
batch_rbm <- 100#
ln_rate_rbm <- .1#
ln_scale_rbm <- .98#
cd_rbm <- 10#
layers <- c(5,10,5)#
batch <- 100#
ln_rate_bp <- .1#
ln_scale_bp <- .98#
epochs_ft <- 20#
#
# Load csv#
#
X_train <- read.csv(file="../X_train.csv", header=T, sep=",", row.names=1)#
X_test <- read.csv(file="../X_test.csv", header=T, sep=",", row.names=1)#
y_train_org <- read.csv(file="../y_train.csv", header=T, sep=",", row.names=1)#
y_test_org <- read.csv(file="../y_test.csv", header=T, sep=",", row.names=1)#
#
X_train <- X_train[1:939,]#
X_test <- X_test[1:313,]#
#
# trying one hot encoding#
onehot_test <- matrix(0L, nrow=dim(y_test_org)[1], ncol=max(y_test_org)+1)#
counter <- 1#
for(y in 1:dim(y_test_org)[1]){#
	onehot_test[counter,y_test_org[y,]+1] <- 1#
	counter <- counter+1#
}#
# trying one hot encoding#
onehot_train <- matrix(0L, nrow=dim(y_train_org)[1], ncol=max(y_train_org)+1)#
counter <- 1#
for(y in 1:dim(y_train_org)[1]){#
	onehot_train[counter, y_train_org[y,]+1] <- 1#
	counter <- counter+1#
}#
#
X_train <- as.matrix(X_train)#
X_test <- as.matrix(X_test)#
y_train <- as.matrix(y_train_org)#
y_test <- as.matrix(y_test_org)#
#
if(onehot){#
	y_test <- onehot_test#
	y_train <- onehot_train#
}#
# only take 1000 samples, otherwise training takes increasingly long#
#chosenRowsTrain <- sample(1:nrow(trainData), size=nrow(X_train))#
#trainDataSmall <- trainData[chosenRowsTrain,]#
#trainLabelsSmall <- trainLabels[chosenRowsTrain,]#
darch  <- darch(X_train, y_train,#
  rbm.numEpochs = epochs_rbm,#
  rbm.consecutive = F, # each RBM is trained one epoch at a time#
  rbm.batchSize = batch_rbm,#
  rbm.lastLayer = -1, # don't train output layer#
  rbm.allData = T, # use bootstrap validation data as well for training#
  rbm.errorFunction = rmseError,#
  rbm.initialMomentum = .5,#
  rbm.finalMomentum = .7,#
  rbm.learnRate = ln_rate_rbm,#
  rbm.learnRateScale = ln_scale_rbm,#
  rbm.momentumRampLength = .8,#
  rbm.numCD = cd_rbm,#
  rbm.unitFunction = sigmoidUnitRbm,#
  rbm.weightDecay = .001,#
  layers = layers,#
  darch.batchSize = batch,#
  darch.dither = T,#
  darch.initialMomentum = .4,#
  darch.finalMomentum = .9,#
  darch.momentumRampLength = .75,#
  bp.learnRate = ln_rate_bp,#
  bp.learnRateScale = ln_scale_bp,#
  darch.unitFunction = c(tanhUnit, softmaxUnit),#
  bootstrap = T,#
  darch.numEpochs = epochs_ft,#
  gputools = T, # try to use gputools#
  gputools.deviceId = 0,#
  xValid = X_test,#
  yValid = y_test#
)#
#
predictions <- predict(darch, newdata=X_test, type="class")#
labels <- cbind(predictions, y_test)#
numIncorrect <- sum(apply(labels, 1, function(i) { any(i[1:5] != i[6:10]) }))#
cat(paste0("Incorrect classifications on test data: ", numIncorrect," (", round(numIncorrect/nrow(y_test)*100, 2), "%)\n"))#
configs <- rbind(configs,data.frame(epochs_rbm= epochs_rbm, batch_rbm= batch_rbm, ln_rate_rbm= ln_rate_rbm, ln_scale_rbm= ln_scale_rbm, cd_rbm= cd_rbm, layers= paste(layers,collapse=" "), batch= batch, ln_rate_bp= ln_rate_bp, ln_scale_bp= ln_scale_bp, epochs_ft= epochs_ft, classification_error=numIncorrect/nrow(y_test)))#
predict_list[[dim(configs)[1]]] <- predictions#
predict_norm_list[[dim(configs)[1]]] <- labels#
nn_list[[dim(configs)[1]]] <- darch#
#
darch#
configs
# Loading library#
if(exists("configs") == F){#
	library(darch)#
	configs = data.frame(epochs_rbm=integer(), batch_rbm=integer(), ln_rate_rbm=numeric(), ln_scale_rbm=numeric(), cd_rbm=integer(), layers=character(), batch=integer(), ln_rate_bp=numeric(), ln_scale_bp=numeric(), epochs_ft=integer(), classification_error=numeric())#
	predict_list <- list()#
	predict_norm_list <- list()#
	nn_list <- list()#
}#
#
onehot <- 1#
epochs_rbm <- 10#
batch_rbm <- 100#
ln_rate_rbm <- .1#
ln_scale_rbm <- .98#
cd_rbm <- 10#
layers <- c(5,100,10,5)#
batch <- 100#
ln_rate_bp <- .1#
ln_scale_bp <- .98#
epochs_ft <- 20#
#
# Load csv#
#
X_train <- read.csv(file="../X_train.csv", header=T, sep=",", row.names=1)#
X_test <- read.csv(file="../X_test.csv", header=T, sep=",", row.names=1)#
y_train_org <- read.csv(file="../y_train.csv", header=T, sep=",", row.names=1)#
y_test_org <- read.csv(file="../y_test.csv", header=T, sep=",", row.names=1)#
#
X_train <- X_train[1:939,]#
X_test <- X_test[1:313,]#
#
# trying one hot encoding#
onehot_test <- matrix(0L, nrow=dim(y_test_org)[1], ncol=max(y_test_org)+1)#
counter <- 1#
for(y in 1:dim(y_test_org)[1]){#
	onehot_test[counter,y_test_org[y,]+1] <- 1#
	counter <- counter+1#
}#
# trying one hot encoding#
onehot_train <- matrix(0L, nrow=dim(y_train_org)[1], ncol=max(y_train_org)+1)#
counter <- 1#
for(y in 1:dim(y_train_org)[1]){#
	onehot_train[counter, y_train_org[y,]+1] <- 1#
	counter <- counter+1#
}#
#
X_train <- as.matrix(X_train)#
X_test <- as.matrix(X_test)#
y_train <- as.matrix(y_train_org)#
y_test <- as.matrix(y_test_org)#
#
if(onehot){#
	y_test <- onehot_test#
	y_train <- onehot_train#
}#
# only take 1000 samples, otherwise training takes increasingly long#
#chosenRowsTrain <- sample(1:nrow(trainData), size=nrow(X_train))#
#trainDataSmall <- trainData[chosenRowsTrain,]#
#trainLabelsSmall <- trainLabels[chosenRowsTrain,]#
darch  <- darch(X_train, y_train,#
  rbm.numEpochs = epochs_rbm,#
  rbm.consecutive = F, # each RBM is trained one epoch at a time#
  rbm.batchSize = batch_rbm,#
  rbm.lastLayer = -1, # don't train output layer#
  rbm.allData = T, # use bootstrap validation data as well for training#
  rbm.errorFunction = rmseError,#
  rbm.initialMomentum = .5,#
  rbm.finalMomentum = .7,#
  rbm.learnRate = ln_rate_rbm,#
  rbm.learnRateScale = ln_scale_rbm,#
  rbm.momentumRampLength = .8,#
  rbm.numCD = cd_rbm,#
  rbm.unitFunction = sigmoidUnitRbm,#
  rbm.weightDecay = .001,#
  layers = layers,#
  darch.batchSize = batch,#
  darch.dither = T,#
  darch.initialMomentum = .4,#
  darch.finalMomentum = .9,#
  darch.momentumRampLength = .75,#
  bp.learnRate = ln_rate_bp,#
  bp.learnRateScale = ln_scale_bp,#
  darch.unitFunction = c(tanhUnit, softmaxUnit),#
  bootstrap = T,#
  darch.numEpochs = epochs_ft,#
  gputools = T, # try to use gputools#
  gputools.deviceId = 0,#
  xValid = X_test,#
  yValid = y_test#
)#
#
predictions <- predict(darch, newdata=X_test, type="class")#
labels <- cbind(predictions, y_test)#
numIncorrect <- sum(apply(labels, 1, function(i) { any(i[1:5] != i[6:10]) }))#
cat(paste0("Incorrect classifications on test data: ", numIncorrect," (", round(numIncorrect/nrow(y_test)*100, 2), "%)\n"))#
configs <- rbind(configs,data.frame(epochs_rbm= epochs_rbm, batch_rbm= batch_rbm, ln_rate_rbm= ln_rate_rbm, ln_scale_rbm= ln_scale_rbm, cd_rbm= cd_rbm, layers= paste(layers,collapse=" "), batch= batch, ln_rate_bp= ln_rate_bp, ln_scale_bp= ln_scale_bp, epochs_ft= epochs_ft, classification_error=numIncorrect/nrow(y_test)))#
predict_list[[dim(configs)[1]]] <- predictions#
predict_norm_list[[dim(configs)[1]]] <- labels#
nn_list[[dim(configs)[1]]] <- darch#
#
darch#
configs
# Loading library#
if(exists("configs") == F){#
	library(darch)#
	configs = data.frame(epochs_rbm=integer(), batch_rbm=integer(), ln_rate_rbm=numeric(), ln_scale_rbm=numeric(), cd_rbm=integer(), layers=character(), batch=integer(), ln_rate_bp=numeric(), ln_scale_bp=numeric(), epochs_ft=integer(), classification_error=numeric())#
	predict_list <- list()#
	predict_norm_list <- list()#
	nn_list <- list()#
}#
#
onehot <- 1#
epochs_rbm <- 10#
batch_rbm <- 100#
ln_rate_rbm <- .01#
ln_scale_rbm <- .98#
cd_rbm <- 10#
layers <- c(5,100,10,5)#
batch <- 100#
ln_rate_bp <- .1#
ln_scale_bp <- .98#
epochs_ft <- 20#
#
# Load csv#
#
X_train <- read.csv(file="../X_train.csv", header=T, sep=",", row.names=1)#
X_test <- read.csv(file="../X_test.csv", header=T, sep=",", row.names=1)#
y_train_org <- read.csv(file="../y_train.csv", header=T, sep=",", row.names=1)#
y_test_org <- read.csv(file="../y_test.csv", header=T, sep=",", row.names=1)#
#
X_train <- X_train[1:939,]#
X_test <- X_test[1:313,]#
#
# trying one hot encoding#
onehot_test <- matrix(0L, nrow=dim(y_test_org)[1], ncol=max(y_test_org)+1)#
counter <- 1#
for(y in 1:dim(y_test_org)[1]){#
	onehot_test[counter,y_test_org[y,]+1] <- 1#
	counter <- counter+1#
}#
# trying one hot encoding#
onehot_train <- matrix(0L, nrow=dim(y_train_org)[1], ncol=max(y_train_org)+1)#
counter <- 1#
for(y in 1:dim(y_train_org)[1]){#
	onehot_train[counter, y_train_org[y,]+1] <- 1#
	counter <- counter+1#
}#
#
X_train <- as.matrix(X_train)#
X_test <- as.matrix(X_test)#
y_train <- as.matrix(y_train_org)#
y_test <- as.matrix(y_test_org)#
#
if(onehot){#
	y_test <- onehot_test#
	y_train <- onehot_train#
}#
# only take 1000 samples, otherwise training takes increasingly long#
#chosenRowsTrain <- sample(1:nrow(trainData), size=nrow(X_train))#
#trainDataSmall <- trainData[chosenRowsTrain,]#
#trainLabelsSmall <- trainLabels[chosenRowsTrain,]#
darch  <- darch(X_train, y_train,#
  rbm.numEpochs = epochs_rbm,#
  rbm.consecutive = F, # each RBM is trained one epoch at a time#
  rbm.batchSize = batch_rbm,#
  rbm.lastLayer = -1, # don't train output layer#
  rbm.allData = T, # use bootstrap validation data as well for training#
  rbm.errorFunction = rmseError,#
  rbm.initialMomentum = .5,#
  rbm.finalMomentum = .7,#
  rbm.learnRate = ln_rate_rbm,#
  rbm.learnRateScale = ln_scale_rbm,#
  rbm.momentumRampLength = .8,#
  rbm.numCD = cd_rbm,#
  rbm.unitFunction = sigmoidUnitRbm,#
  rbm.weightDecay = .001,#
  layers = layers,#
  darch.batchSize = batch,#
  darch.dither = T,#
  darch.initialMomentum = .4,#
  darch.finalMomentum = .9,#
  darch.momentumRampLength = .75,#
  bp.learnRate = ln_rate_bp,#
  bp.learnRateScale = ln_scale_bp,#
  darch.unitFunction = c(tanhUnit, softmaxUnit),#
  bootstrap = T,#
  darch.numEpochs = epochs_ft,#
  gputools = T, # try to use gputools#
  gputools.deviceId = 0,#
  xValid = X_test,#
  yValid = y_test#
)#
#
predictions <- predict(darch, newdata=X_test, type="class")#
labels <- cbind(predictions, y_test)#
numIncorrect <- sum(apply(labels, 1, function(i) { any(i[1:5] != i[6:10]) }))#
cat(paste0("Incorrect classifications on test data: ", numIncorrect," (", round(numIncorrect/nrow(y_test)*100, 2), "%)\n"))#
configs <- rbind(configs,data.frame(epochs_rbm= epochs_rbm, batch_rbm= batch_rbm, ln_rate_rbm= ln_rate_rbm, ln_scale_rbm= ln_scale_rbm, cd_rbm= cd_rbm, layers= paste(layers,collapse=" "), batch= batch, ln_rate_bp= ln_rate_bp, ln_scale_bp= ln_scale_bp, epochs_ft= epochs_ft, classification_error=numIncorrect/nrow(y_test)))#
predict_list[[dim(configs)[1]]] <- predictions#
predict_norm_list[[dim(configs)[1]]] <- labels#
nn_list[[dim(configs)[1]]] <- darch#
#
darch#
configs
# Loading library#
if(exists("configs") == F){#
	library(darch)#
	configs = data.frame(epochs_rbm=integer(), batch_rbm=integer(), ln_rate_rbm=numeric(), ln_scale_rbm=numeric(), cd_rbm=integer(), layers=character(), batch=integer(), ln_rate_bp=numeric(), ln_scale_bp=numeric(), epochs_ft=integer(), classification_error=numeric())#
	predict_list <- list()#
	predict_norm_list <- list()#
	nn_list <- list()#
}#
#
onehot <- 1#
epochs_rbm <- 10#
batch_rbm <- 100#
ln_rate_rbm <- .01#
ln_scale_rbm <- .98#
cd_rbm <- 10#
layers <- c(5,100,10,5)#
batch <- 100#
ln_rate_bp <- .01#
ln_scale_bp <- .98#
epochs_ft <- 20#
#
# Load csv#
#
X_train <- read.csv(file="../X_train.csv", header=T, sep=",", row.names=1)#
X_test <- read.csv(file="../X_test.csv", header=T, sep=",", row.names=1)#
y_train_org <- read.csv(file="../y_train.csv", header=T, sep=",", row.names=1)#
y_test_org <- read.csv(file="../y_test.csv", header=T, sep=",", row.names=1)#
#
X_train <- X_train[1:939,]#
X_test <- X_test[1:313,]#
#
# trying one hot encoding#
onehot_test <- matrix(0L, nrow=dim(y_test_org)[1], ncol=max(y_test_org)+1)#
counter <- 1#
for(y in 1:dim(y_test_org)[1]){#
	onehot_test[counter,y_test_org[y,]+1] <- 1#
	counter <- counter+1#
}#
# trying one hot encoding#
onehot_train <- matrix(0L, nrow=dim(y_train_org)[1], ncol=max(y_train_org)+1)#
counter <- 1#
for(y in 1:dim(y_train_org)[1]){#
	onehot_train[counter, y_train_org[y,]+1] <- 1#
	counter <- counter+1#
}#
#
X_train <- as.matrix(X_train)#
X_test <- as.matrix(X_test)#
y_train <- as.matrix(y_train_org)#
y_test <- as.matrix(y_test_org)#
#
if(onehot){#
	y_test <- onehot_test#
	y_train <- onehot_train#
}#
# only take 1000 samples, otherwise training takes increasingly long#
#chosenRowsTrain <- sample(1:nrow(trainData), size=nrow(X_train))#
#trainDataSmall <- trainData[chosenRowsTrain,]#
#trainLabelsSmall <- trainLabels[chosenRowsTrain,]#
darch  <- darch(X_train, y_train,#
  rbm.numEpochs = epochs_rbm,#
  rbm.consecutive = F, # each RBM is trained one epoch at a time#
  rbm.batchSize = batch_rbm,#
  rbm.lastLayer = -1, # don't train output layer#
  rbm.allData = T, # use bootstrap validation data as well for training#
  rbm.errorFunction = rmseError,#
  rbm.initialMomentum = .5,#
  rbm.finalMomentum = .7,#
  rbm.learnRate = ln_rate_rbm,#
  rbm.learnRateScale = ln_scale_rbm,#
  rbm.momentumRampLength = .8,#
  rbm.numCD = cd_rbm,#
  rbm.unitFunction = sigmoidUnitRbm,#
  rbm.weightDecay = .001,#
  layers = layers,#
  darch.batchSize = batch,#
  darch.dither = T,#
  darch.initialMomentum = .4,#
  darch.finalMomentum = .9,#
  darch.momentumRampLength = .75,#
  bp.learnRate = ln_rate_bp,#
  bp.learnRateScale = ln_scale_bp,#
  darch.unitFunction = c(tanhUnit, softmaxUnit),#
  bootstrap = T,#
  darch.numEpochs = epochs_ft,#
  gputools = T, # try to use gputools#
  gputools.deviceId = 0,#
  xValid = X_test,#
  yValid = y_test#
)#
#
predictions <- predict(darch, newdata=X_test, type="class")#
labels <- cbind(predictions, y_test)#
numIncorrect <- sum(apply(labels, 1, function(i) { any(i[1:5] != i[6:10]) }))#
cat(paste0("Incorrect classifications on test data: ", numIncorrect," (", round(numIncorrect/nrow(y_test)*100, 2), "%)\n"))#
configs <- rbind(configs,data.frame(epochs_rbm= epochs_rbm, batch_rbm= batch_rbm, ln_rate_rbm= ln_rate_rbm, ln_scale_rbm= ln_scale_rbm, cd_rbm= cd_rbm, layers= paste(layers,collapse=" "), batch= batch, ln_rate_bp= ln_rate_bp, ln_scale_bp= ln_scale_bp, epochs_ft= epochs_ft, classification_error=numIncorrect/nrow(y_test)))#
predict_list[[dim(configs)[1]]] <- predictions#
predict_norm_list[[dim(configs)[1]]] <- labels#
nn_list[[dim(configs)[1]]] <- darch#
#
darch#
configs
predictions
# Loading library#
if(exists("configs") == F){#
	library(darch)#
	configs = data.frame(epochs_rbm=integer(), batch_rbm=integer(), ln_rate_rbm=numeric(), ln_scale_rbm=numeric(), cd_rbm=integer(), layers=character(), batch=integer(), ln_rate_bp=numeric(), ln_scale_bp=numeric(), epochs_ft=integer(), classification_error=numeric())#
	predict_list <- list()#
	predict_norm_list <- list()#
	nn_list <- list()#
}#
#
onehot <- 1#
epochs_rbm <- 10#
batch_rbm <- 100#
ln_rate_rbm <- .01#
ln_scale_rbm <- .98#
cd_rbm <- 10#
layers <- c(5,100,100,5)#
batch <- 100#
ln_rate_bp <- .01#
ln_scale_bp <- .98#
epochs_ft <- 20#
#
# Load csv#
#
X_train <- read.csv(file="../X_train.csv", header=T, sep=",", row.names=1)#
X_test <- read.csv(file="../X_test.csv", header=T, sep=",", row.names=1)#
y_train_org <- read.csv(file="../y_train.csv", header=T, sep=",", row.names=1)#
y_test_org <- read.csv(file="../y_test.csv", header=T, sep=",", row.names=1)#
#
X_train <- X_train[1:939,]#
X_test <- X_test[1:313,]#
#
# trying one hot encoding#
onehot_test <- matrix(0L, nrow=dim(y_test_org)[1], ncol=max(y_test_org)+1)#
counter <- 1#
for(y in 1:dim(y_test_org)[1]){#
	onehot_test[counter,y_test_org[y,]+1] <- 1#
	counter <- counter+1#
}#
# trying one hot encoding#
onehot_train <- matrix(0L, nrow=dim(y_train_org)[1], ncol=max(y_train_org)+1)#
counter <- 1#
for(y in 1:dim(y_train_org)[1]){#
	onehot_train[counter, y_train_org[y,]+1] <- 1#
	counter <- counter+1#
}#
#
X_train <- as.matrix(X_train)#
X_test <- as.matrix(X_test)#
y_train <- as.matrix(y_train_org)#
y_test <- as.matrix(y_test_org)#
#
if(onehot){#
	y_test <- onehot_test#
	y_train <- onehot_train#
}#
# only take 1000 samples, otherwise training takes increasingly long#
#chosenRowsTrain <- sample(1:nrow(trainData), size=nrow(X_train))#
#trainDataSmall <- trainData[chosenRowsTrain,]#
#trainLabelsSmall <- trainLabels[chosenRowsTrain,]#
darch  <- darch(X_train, y_train,#
  rbm.numEpochs = epochs_rbm,#
  rbm.consecutive = F, # each RBM is trained one epoch at a time#
  rbm.batchSize = batch_rbm,#
  rbm.lastLayer = -1, # don't train output layer#
  rbm.allData = T, # use bootstrap validation data as well for training#
  rbm.errorFunction = rmseError,#
  rbm.initialMomentum = .5,#
  rbm.finalMomentum = .7,#
  rbm.learnRate = ln_rate_rbm,#
  rbm.learnRateScale = ln_scale_rbm,#
  rbm.momentumRampLength = .8,#
  rbm.numCD = cd_rbm,#
  rbm.unitFunction = sigmoidUnitRbm,#
  rbm.weightDecay = .001,#
  layers = layers,#
  darch.batchSize = batch,#
  darch.dither = T,#
  darch.initialMomentum = .4,#
  darch.finalMomentum = .9,#
  darch.momentumRampLength = .75,#
  bp.learnRate = ln_rate_bp,#
  bp.learnRateScale = ln_scale_bp,#
  darch.unitFunction = c(tanhUnit, softmaxUnit),#
  bootstrap = T,#
  darch.numEpochs = epochs_ft,#
  gputools = T, # try to use gputools#
  gputools.deviceId = 0,#
  xValid = X_test,#
  yValid = y_test#
)#
#
predictions <- predict(darch, newdata=X_test, type="class")#
labels <- cbind(predictions, y_test)#
numIncorrect <- sum(apply(labels, 1, function(i) { any(i[1:5] != i[6:10]) }))#
cat(paste0("Incorrect classifications on test data: ", numIncorrect," (", round(numIncorrect/nrow(y_test)*100, 2), "%)\n"))#
configs <- rbind(configs,data.frame(epochs_rbm= epochs_rbm, batch_rbm= batch_rbm, ln_rate_rbm= ln_rate_rbm, ln_scale_rbm= ln_scale_rbm, cd_rbm= cd_rbm, layers= paste(layers,collapse=" "), batch= batch, ln_rate_bp= ln_rate_bp, ln_scale_bp= ln_scale_bp, epochs_ft= epochs_ft, classification_error=numIncorrect/nrow(y_test)))#
predict_list[[dim(configs)[1]]] <- predictions#
predict_norm_list[[dim(configs)[1]]] <- labels#
nn_list[[dim(configs)[1]]] <- darch#
#
darch#
configs
# Loading library#
if(exists("configs") == F){#
	library(darch)#
	configs = data.frame(epochs_rbm=integer(), batch_rbm=integer(), ln_rate_rbm=numeric(), ln_scale_rbm=numeric(), cd_rbm=integer(), layers=character(), batch=integer(), ln_rate_bp=numeric(), ln_scale_bp=numeric(), epochs_ft=integer(), classification_error=numeric())#
	predict_list <- list()#
	predict_norm_list <- list()#
	nn_list <- list()#
}#
#
onehot <- 1#
epochs_rbm <- 10#
batch_rbm <- 100#
ln_rate_rbm <- .01#
ln_scale_rbm <- .98#
cd_rbm <- 10#
layers <- c(5,1000,100,5)#
batch <- 100#
ln_rate_bp <- .01#
ln_scale_bp <- .98#
epochs_ft <- 20#
#
# Load csv#
#
X_train <- read.csv(file="../X_train.csv", header=T, sep=",", row.names=1)#
X_test <- read.csv(file="../X_test.csv", header=T, sep=",", row.names=1)#
y_train_org <- read.csv(file="../y_train.csv", header=T, sep=",", row.names=1)#
y_test_org <- read.csv(file="../y_test.csv", header=T, sep=",", row.names=1)#
#
X_train <- X_train[1:939,]#
X_test <- X_test[1:313,]#
#
# trying one hot encoding#
onehot_test <- matrix(0L, nrow=dim(y_test_org)[1], ncol=max(y_test_org)+1)#
counter <- 1#
for(y in 1:dim(y_test_org)[1]){#
	onehot_test[counter,y_test_org[y,]+1] <- 1#
	counter <- counter+1#
}#
# trying one hot encoding#
onehot_train <- matrix(0L, nrow=dim(y_train_org)[1], ncol=max(y_train_org)+1)#
counter <- 1#
for(y in 1:dim(y_train_org)[1]){#
	onehot_train[counter, y_train_org[y,]+1] <- 1#
	counter <- counter+1#
}#
#
X_train <- as.matrix(X_train)#
X_test <- as.matrix(X_test)#
y_train <- as.matrix(y_train_org)#
y_test <- as.matrix(y_test_org)#
#
if(onehot){#
	y_test <- onehot_test#
	y_train <- onehot_train#
}#
# only take 1000 samples, otherwise training takes increasingly long#
#chosenRowsTrain <- sample(1:nrow(trainData), size=nrow(X_train))#
#trainDataSmall <- trainData[chosenRowsTrain,]#
#trainLabelsSmall <- trainLabels[chosenRowsTrain,]#
darch  <- darch(X_train, y_train,#
  rbm.numEpochs = epochs_rbm,#
  rbm.consecutive = F, # each RBM is trained one epoch at a time#
  rbm.batchSize = batch_rbm,#
  rbm.lastLayer = -1, # don't train output layer#
  rbm.allData = T, # use bootstrap validation data as well for training#
  rbm.errorFunction = rmseError,#
  rbm.initialMomentum = .5,#
  rbm.finalMomentum = .7,#
  rbm.learnRate = ln_rate_rbm,#
  rbm.learnRateScale = ln_scale_rbm,#
  rbm.momentumRampLength = .8,#
  rbm.numCD = cd_rbm,#
  rbm.unitFunction = sigmoidUnitRbm,#
  rbm.weightDecay = .001,#
  layers = layers,#
  darch.batchSize = batch,#
  darch.dither = T,#
  darch.initialMomentum = .4,#
  darch.finalMomentum = .9,#
  darch.momentumRampLength = .75,#
  bp.learnRate = ln_rate_bp,#
  bp.learnRateScale = ln_scale_bp,#
  darch.unitFunction = c(tanhUnit, softmaxUnit),#
  bootstrap = T,#
  darch.numEpochs = epochs_ft,#
  gputools = T, # try to use gputools#
  gputools.deviceId = 0,#
  xValid = X_test,#
  yValid = y_test#
)#
#
predictions <- predict(darch, newdata=X_test, type="class")#
labels <- cbind(predictions, y_test)#
numIncorrect <- sum(apply(labels, 1, function(i) { any(i[1:5] != i[6:10]) }))#
cat(paste0("Incorrect classifications on test data: ", numIncorrect," (", round(numIncorrect/nrow(y_test)*100, 2), "%)\n"))#
configs <- rbind(configs,data.frame(epochs_rbm= epochs_rbm, batch_rbm= batch_rbm, ln_rate_rbm= ln_rate_rbm, ln_scale_rbm= ln_scale_rbm, cd_rbm= cd_rbm, layers= paste(layers,collapse=" "), batch= batch, ln_rate_bp= ln_rate_bp, ln_scale_bp= ln_scale_bp, epochs_ft= epochs_ft, classification_error=numIncorrect/nrow(y_test)))#
predict_list[[dim(configs)[1]]] <- predictions#
predict_norm_list[[dim(configs)[1]]] <- labels#
nn_list[[dim(configs)[1]]] <- darch#
#
darch#
configs
# Loading library#
if(exists("configs") == F){#
	library(darch)#
	configs = data.frame(epochs_rbm=integer(), batch_rbm=integer(), ln_rate_rbm=numeric(), ln_scale_rbm=numeric(), cd_rbm=integer(), layers=character(), batch=integer(), ln_rate_bp=numeric(), ln_scale_bp=numeric(), epochs_ft=integer(), classification_error=numeric())#
	predict_list <- list()#
	predict_norm_list <- list()#
	nn_list <- list()#
}#
#
onehot <- 1#
epochs_rbm <- 10#
batch_rbm <- 100#
ln_rate_rbm <- .001#
ln_scale_rbm <- .98#
cd_rbm <- 10#
layers <- c(5,1000,100,5)#
batch <- 100#
ln_rate_bp <- .01#
ln_scale_bp <- .98#
epochs_ft <- 20#
#
# Load csv#
#
X_train <- read.csv(file="../X_train.csv", header=T, sep=",", row.names=1)#
X_test <- read.csv(file="../X_test.csv", header=T, sep=",", row.names=1)#
y_train_org <- read.csv(file="../y_train.csv", header=T, sep=",", row.names=1)#
y_test_org <- read.csv(file="../y_test.csv", header=T, sep=",", row.names=1)#
#
X_train <- X_train[1:939,]#
X_test <- X_test[1:313,]#
#
# trying one hot encoding#
onehot_test <- matrix(0L, nrow=dim(y_test_org)[1], ncol=max(y_test_org)+1)#
counter <- 1#
for(y in 1:dim(y_test_org)[1]){#
	onehot_test[counter,y_test_org[y,]+1] <- 1#
	counter <- counter+1#
}#
# trying one hot encoding#
onehot_train <- matrix(0L, nrow=dim(y_train_org)[1], ncol=max(y_train_org)+1)#
counter <- 1#
for(y in 1:dim(y_train_org)[1]){#
	onehot_train[counter, y_train_org[y,]+1] <- 1#
	counter <- counter+1#
}#
#
X_train <- as.matrix(X_train)#
X_test <- as.matrix(X_test)#
y_train <- as.matrix(y_train_org)#
y_test <- as.matrix(y_test_org)#
#
if(onehot){#
	y_test <- onehot_test#
	y_train <- onehot_train#
}#
# only take 1000 samples, otherwise training takes increasingly long#
#chosenRowsTrain <- sample(1:nrow(trainData), size=nrow(X_train))#
#trainDataSmall <- trainData[chosenRowsTrain,]#
#trainLabelsSmall <- trainLabels[chosenRowsTrain,]#
darch  <- darch(X_train, y_train,#
  rbm.numEpochs = epochs_rbm,#
  rbm.consecutive = F, # each RBM is trained one epoch at a time#
  rbm.batchSize = batch_rbm,#
  rbm.lastLayer = -1, # don't train output layer#
  rbm.allData = T, # use bootstrap validation data as well for training#
  rbm.errorFunction = rmseError,#
  rbm.initialMomentum = .5,#
  rbm.finalMomentum = .7,#
  rbm.learnRate = ln_rate_rbm,#
  rbm.learnRateScale = ln_scale_rbm,#
  rbm.momentumRampLength = .8,#
  rbm.numCD = cd_rbm,#
  rbm.unitFunction = sigmoidUnitRbm,#
  rbm.weightDecay = .001,#
  layers = layers,#
  darch.batchSize = batch,#
  darch.dither = T,#
  darch.initialMomentum = .4,#
  darch.finalMomentum = .9,#
  darch.momentumRampLength = .75,#
  bp.learnRate = ln_rate_bp,#
  bp.learnRateScale = ln_scale_bp,#
  darch.unitFunction = c(tanhUnit, softmaxUnit),#
  bootstrap = T,#
  darch.numEpochs = epochs_ft,#
  gputools = T, # try to use gputools#
  gputools.deviceId = 0,#
  xValid = X_test,#
  yValid = y_test#
)#
#
predictions <- predict(darch, newdata=X_test, type="class")#
labels <- cbind(predictions, y_test)#
numIncorrect <- sum(apply(labels, 1, function(i) { any(i[1:5] != i[6:10]) }))#
cat(paste0("Incorrect classifications on test data: ", numIncorrect," (", round(numIncorrect/nrow(y_test)*100, 2), "%)\n"))#
configs <- rbind(configs,data.frame(epochs_rbm= epochs_rbm, batch_rbm= batch_rbm, ln_rate_rbm= ln_rate_rbm, ln_scale_rbm= ln_scale_rbm, cd_rbm= cd_rbm, layers= paste(layers,collapse=" "), batch= batch, ln_rate_bp= ln_rate_bp, ln_scale_bp= ln_scale_bp, epochs_ft= epochs_ft, classification_error=numIncorrect/nrow(y_test)))#
predict_list[[dim(configs)[1]]] <- predictions#
predict_norm_list[[dim(configs)[1]]] <- labels#
nn_list[[dim(configs)[1]]] <- darch#
#
darch#
configs
# Loading library#
if(exists("configs") == F){#
	library(darch)#
	configs = data.frame(epochs_rbm=integer(), batch_rbm=integer(), ln_rate_rbm=numeric(), ln_scale_rbm=numeric(), cd_rbm=integer(), layers=character(), batch=integer(), ln_rate_bp=numeric(), ln_scale_bp=numeric(), epochs_ft=integer(), classification_error=numeric())#
	predict_list <- list()#
	predict_norm_list <- list()#
	nn_list <- list()#
}#
#
onehot <- 1#
epochs_rbm <- 20#
batch_rbm <- 100#
ln_rate_rbm <- .001#
ln_scale_rbm <- .98#
cd_rbm <- 10#
layers <- c(5,1000,100,5)#
batch <- 100#
ln_rate_bp <- .01#
ln_scale_bp <- .98#
epochs_ft <- 20#
#
# Load csv#
#
X_train <- read.csv(file="../X_train.csv", header=T, sep=",", row.names=1)#
X_test <- read.csv(file="../X_test.csv", header=T, sep=",", row.names=1)#
y_train_org <- read.csv(file="../y_train.csv", header=T, sep=",", row.names=1)#
y_test_org <- read.csv(file="../y_test.csv", header=T, sep=",", row.names=1)#
#
X_train <- X_train[1:939,]#
X_test <- X_test[1:313,]#
#
# trying one hot encoding#
onehot_test <- matrix(0L, nrow=dim(y_test_org)[1], ncol=max(y_test_org)+1)#
counter <- 1#
for(y in 1:dim(y_test_org)[1]){#
	onehot_test[counter,y_test_org[y,]+1] <- 1#
	counter <- counter+1#
}#
# trying one hot encoding#
onehot_train <- matrix(0L, nrow=dim(y_train_org)[1], ncol=max(y_train_org)+1)#
counter <- 1#
for(y in 1:dim(y_train_org)[1]){#
	onehot_train[counter, y_train_org[y,]+1] <- 1#
	counter <- counter+1#
}#
#
X_train <- as.matrix(X_train)#
X_test <- as.matrix(X_test)#
y_train <- as.matrix(y_train_org)#
y_test <- as.matrix(y_test_org)#
#
if(onehot){#
	y_test <- onehot_test#
	y_train <- onehot_train#
}#
# only take 1000 samples, otherwise training takes increasingly long#
#chosenRowsTrain <- sample(1:nrow(trainData), size=nrow(X_train))#
#trainDataSmall <- trainData[chosenRowsTrain,]#
#trainLabelsSmall <- trainLabels[chosenRowsTrain,]#
darch  <- darch(X_train, y_train,#
  rbm.numEpochs = epochs_rbm,#
  rbm.consecutive = F, # each RBM is trained one epoch at a time#
  rbm.batchSize = batch_rbm,#
  rbm.lastLayer = -1, # don't train output layer#
  rbm.allData = T, # use bootstrap validation data as well for training#
  rbm.errorFunction = rmseError,#
  rbm.initialMomentum = .5,#
  rbm.finalMomentum = .7,#
  rbm.learnRate = ln_rate_rbm,#
  rbm.learnRateScale = ln_scale_rbm,#
  rbm.momentumRampLength = .8,#
  rbm.numCD = cd_rbm,#
  rbm.unitFunction = sigmoidUnitRbm,#
  rbm.weightDecay = .001,#
  layers = layers,#
  darch.batchSize = batch,#
  darch.dither = T,#
  darch.initialMomentum = .4,#
  darch.finalMomentum = .9,#
  darch.momentumRampLength = .75,#
  bp.learnRate = ln_rate_bp,#
  bp.learnRateScale = ln_scale_bp,#
  darch.unitFunction = c(tanhUnit, softmaxUnit),#
  bootstrap = T,#
  darch.numEpochs = epochs_ft,#
  gputools = T, # try to use gputools#
  gputools.deviceId = 0,#
  xValid = X_test,#
  yValid = y_test#
)#
#
predictions <- predict(darch, newdata=X_test, type="class")#
labels <- cbind(predictions, y_test)#
numIncorrect <- sum(apply(labels, 1, function(i) { any(i[1:5] != i[6:10]) }))#
cat(paste0("Incorrect classifications on test data: ", numIncorrect," (", round(numIncorrect/nrow(y_test)*100, 2), "%)\n"))#
configs <- rbind(configs,data.frame(epochs_rbm= epochs_rbm, batch_rbm= batch_rbm, ln_rate_rbm= ln_rate_rbm, ln_scale_rbm= ln_scale_rbm, cd_rbm= cd_rbm, layers= paste(layers,collapse=" "), batch= batch, ln_rate_bp= ln_rate_bp, ln_scale_bp= ln_scale_bp, epochs_ft= epochs_ft, classification_error=numIncorrect/nrow(y_test)))#
predict_list[[dim(configs)[1]]] <- predictions#
predict_norm_list[[dim(configs)[1]]] <- labels#
nn_list[[dim(configs)[1]]] <- darch#
#
darch#
configs
# Loading library#
if(exists("configs") == F){#
	library(darch)#
	configs = data.frame(epochs_rbm=integer(), batch_rbm=integer(), ln_rate_rbm=numeric(), ln_scale_rbm=numeric(), cd_rbm=integer(), layers=character(), batch=integer(), ln_rate_bp=numeric(), ln_scale_bp=numeric(), epochs_ft=integer(), classification_error=numeric())#
	predict_list <- list()#
	predict_norm_list <- list()#
	nn_list <- list()#
}#
#
onehot <- 1#
epochs_rbm <- 20#
batch_rbm <- 100#
ln_rate_rbm <- .001#
ln_scale_rbm <- .98#
cd_rbm <- 10#
layers <- c(5,1000,5)#
units <- c(tanhUnit, softmaxUnit)#
batch <- 100#
ln_rate_bp <- .01#
ln_scale_bp <- .98#
epochs_ft <- 20#
#
# Load csv#
#
X_train <- read.csv(file="../X_train.csv", header=T, sep=",", row.names=1)#
X_test <- read.csv(file="../X_test.csv", header=T, sep=",", row.names=1)#
y_train_org <- read.csv(file="../y_train.csv", header=T, sep=",", row.names=1)#
y_test_org <- read.csv(file="../y_test.csv", header=T, sep=",", row.names=1)#
#
X_train <- X_train[1:939,]#
X_test <- X_test[1:313,]#
#
# trying one hot encoding#
onehot_test <- matrix(0L, nrow=dim(y_test_org)[1], ncol=max(y_test_org)+1)#
counter <- 1#
for(y in 1:dim(y_test_org)[1]){#
	onehot_test[counter,y_test_org[y,]+1] <- 1#
	counter <- counter+1#
}#
# trying one hot encoding#
onehot_train <- matrix(0L, nrow=dim(y_train_org)[1], ncol=max(y_train_org)+1)#
counter <- 1#
for(y in 1:dim(y_train_org)[1]){#
	onehot_train[counter, y_train_org[y,]+1] <- 1#
	counter <- counter+1#
}#
#
X_train <- as.matrix(X_train)#
X_test <- as.matrix(X_test)#
y_train <- as.matrix(y_train_org)#
y_test <- as.matrix(y_test_org)#
#
if(onehot){#
	y_test <- onehot_test#
	y_train <- onehot_train#
}#
# only take 1000 samples, otherwise training takes increasingly long#
#chosenRowsTrain <- sample(1:nrow(trainData), size=nrow(X_train))#
#trainDataSmall <- trainData[chosenRowsTrain,]#
#trainLabelsSmall <- trainLabels[chosenRowsTrain,]#
darch  <- darch(X_train, y_train,#
  rbm.numEpochs = epochs_rbm,#
  rbm.consecutive = F, # each RBM is trained one epoch at a time#
  rbm.batchSize = batch_rbm,#
  rbm.lastLayer = -1, # don't train output layer#
  rbm.allData = T, # use bootstrap validation data as well for training#
  rbm.errorFunction = rmseError,#
  rbm.initialMomentum = .5,#
  rbm.finalMomentum = .7,#
  rbm.learnRate = ln_rate_rbm,#
  rbm.learnRateScale = ln_scale_rbm,#
  rbm.momentumRampLength = .8,#
  rbm.numCD = cd_rbm,#
  rbm.unitFunction = sigmoidUnitRbm,#
  rbm.weightDecay = .001,#
  layers = layers,#
  darch.batchSize = batch,#
  darch.dither = T,#
  darch.initialMomentum = .4,#
  darch.finalMomentum = .9,#
  darch.momentumRampLength = .75,#
  bp.learnRate = ln_rate_bp,#
  bp.learnRateScale = ln_scale_bp,#
  darch.unitFunction = units,#
  bootstrap = T,#
  darch.numEpochs = epochs_ft,#
  gputools = T, # try to use gputools#
  gputools.deviceId = 0,#
  xValid = X_test,#
  yValid = y_test#
)#
#
predictions <- predict(darch, newdata=X_test, type="class")#
labels <- cbind(predictions, y_test)#
numIncorrect <- sum(apply(labels, 1, function(i) { any(i[1:5] != i[6:10]) }))#
cat(paste0("Incorrect classifications on test data: ", numIncorrect," (", round(numIncorrect/nrow(y_test)*100, 2), "%)\n"))#
configs <- rbind(configs,data.frame(epochs_rbm= epochs_rbm, batch_rbm= batch_rbm, ln_rate_rbm= ln_rate_rbm, ln_scale_rbm= ln_scale_rbm, cd_rbm= cd_rbm, layers= paste(layers,collapse=" "), batch= batch, ln_rate_bp= ln_rate_bp, ln_scale_bp= ln_scale_bp, epochs_ft= epochs_ft, classification_error=numIncorrect/nrow(y_test)))#
predict_list[[dim(configs)[1]]] <- predictions#
predict_norm_list[[dim(configs)[1]]] <- labels#
nn_list[[dim(configs)[1]]] <- darch#
#
darch#
configs
# Loading library#
if(exists("configs") == F){#
	library(darch)#
	configs = data.frame(epochs_rbm=integer(), batch_rbm=integer(), ln_rate_rbm=numeric(), ln_scale_rbm=numeric(), cd_rbm=integer(), layers=character(), batch=integer(), ln_rate_bp=numeric(), ln_scale_bp=numeric(), epochs_ft=integer(), classification_error=numeric())#
	predict_list <- list()#
	predict_norm_list <- list()#
	nn_list <- list()#
}#
#
onehot <- 1#
epochs_rbm <- 20#
batch_rbm <- 100#
ln_rate_rbm <- .001#
ln_scale_rbm <- .98#
cd_rbm <- 10#
layers <- c(5,1000,5)#
units <- c(tanhUnit, softmaxUnit)#
batch <- 100#
ln_rate_bp <- .001#
ln_scale_bp <- .98#
epochs_ft <- 20#
#
# Load csv#
#
X_train <- read.csv(file="../X_train.csv", header=T, sep=",", row.names=1)#
X_test <- read.csv(file="../X_test.csv", header=T, sep=",", row.names=1)#
y_train_org <- read.csv(file="../y_train.csv", header=T, sep=",", row.names=1)#
y_test_org <- read.csv(file="../y_test.csv", header=T, sep=",", row.names=1)#
#
X_train <- X_train[1:939,]#
X_test <- X_test[1:313,]#
#
# trying one hot encoding#
onehot_test <- matrix(0L, nrow=dim(y_test_org)[1], ncol=max(y_test_org)+1)#
counter <- 1#
for(y in 1:dim(y_test_org)[1]){#
	onehot_test[counter,y_test_org[y,]+1] <- 1#
	counter <- counter+1#
}#
# trying one hot encoding#
onehot_train <- matrix(0L, nrow=dim(y_train_org)[1], ncol=max(y_train_org)+1)#
counter <- 1#
for(y in 1:dim(y_train_org)[1]){#
	onehot_train[counter, y_train_org[y,]+1] <- 1#
	counter <- counter+1#
}#
#
X_train <- as.matrix(X_train)#
X_test <- as.matrix(X_test)#
y_train <- as.matrix(y_train_org)#
y_test <- as.matrix(y_test_org)#
#
if(onehot){#
	y_test <- onehot_test#
	y_train <- onehot_train#
}#
# only take 1000 samples, otherwise training takes increasingly long#
#chosenRowsTrain <- sample(1:nrow(trainData), size=nrow(X_train))#
#trainDataSmall <- trainData[chosenRowsTrain,]#
#trainLabelsSmall <- trainLabels[chosenRowsTrain,]#
darch  <- darch(X_train, y_train,#
  rbm.numEpochs = epochs_rbm,#
  rbm.consecutive = F, # each RBM is trained one epoch at a time#
  rbm.batchSize = batch_rbm,#
  rbm.lastLayer = -1, # don't train output layer#
  rbm.allData = T, # use bootstrap validation data as well for training#
  rbm.errorFunction = rmseError,#
  rbm.initialMomentum = .5,#
  rbm.finalMomentum = .7,#
  rbm.learnRate = ln_rate_rbm,#
  rbm.learnRateScale = ln_scale_rbm,#
  rbm.momentumRampLength = .8,#
  rbm.numCD = cd_rbm,#
  rbm.unitFunction = sigmoidUnitRbm,#
  rbm.weightDecay = .001,#
  layers = layers,#
  darch.batchSize = batch,#
  darch.dither = T,#
  darch.initialMomentum = .4,#
  darch.finalMomentum = .9,#
  darch.momentumRampLength = .75,#
  bp.learnRate = ln_rate_bp,#
  bp.learnRateScale = ln_scale_bp,#
  darch.unitFunction = units,#
  bootstrap = T,#
  darch.numEpochs = epochs_ft,#
  gputools = T, # try to use gputools#
  gputools.deviceId = 0,#
  xValid = X_test,#
  yValid = y_test#
)#
#
predictions <- predict(darch, newdata=X_test, type="class")#
labels <- cbind(predictions, y_test)#
numIncorrect <- sum(apply(labels, 1, function(i) { any(i[1:5] != i[6:10]) }))#
cat(paste0("Incorrect classifications on test data: ", numIncorrect," (", round(numIncorrect/nrow(y_test)*100, 2), "%)\n"))#
configs <- rbind(configs,data.frame(epochs_rbm= epochs_rbm, batch_rbm= batch_rbm, ln_rate_rbm= ln_rate_rbm, ln_scale_rbm= ln_scale_rbm, cd_rbm= cd_rbm, layers= paste(layers,collapse=" "), batch= batch, ln_rate_bp= ln_rate_bp, ln_scale_bp= ln_scale_bp, epochs_ft= epochs_ft, classification_error=numIncorrect/nrow(y_test)))#
predict_list[[dim(configs)[1]]] <- predictions#
predict_norm_list[[dim(configs)[1]]] <- labels#
nn_list[[dim(configs)[1]]] <- darch#
#
darch#
configs
# Loading library#
if(exists("configs") == F){#
	library(darch)#
	configs = data.frame(epochs_rbm=integer(), batch_rbm=integer(), ln_rate_rbm=numeric(), ln_scale_rbm=numeric(), cd_rbm=integer(), layers=character(), batch=integer(), ln_rate_bp=numeric(), ln_scale_bp=numeric(), epochs_ft=integer(), classification_error=numeric())#
	predict_list <- list()#
	predict_norm_list <- list()#
	nn_list <- list()#
}#
#
onehot <- 1#
epochs_rbm <- 20#
batch_rbm <- 100#
ln_rate_rbm <- .001#
ln_scale_rbm <- .98#
cd_rbm <- 10#
layers <- c(5,100,5)#
units <- c(tanhUnit, softmaxUnit)#
batch <- 100#
ln_rate_bp <- .001#
ln_scale_bp <- .98#
epochs_ft <- 20#
#
# Load csv#
#
X_train <- read.csv(file="../X_train.csv", header=T, sep=",", row.names=1)#
X_test <- read.csv(file="../X_test.csv", header=T, sep=",", row.names=1)#
y_train_org <- read.csv(file="../y_train.csv", header=T, sep=",", row.names=1)#
y_test_org <- read.csv(file="../y_test.csv", header=T, sep=",", row.names=1)#
#
X_train <- X_train[1:939,]#
X_test <- X_test[1:313,]#
#
# trying one hot encoding#
onehot_test <- matrix(0L, nrow=dim(y_test_org)[1], ncol=max(y_test_org)+1)#
counter <- 1#
for(y in 1:dim(y_test_org)[1]){#
	onehot_test[counter,y_test_org[y,]+1] <- 1#
	counter <- counter+1#
}#
# trying one hot encoding#
onehot_train <- matrix(0L, nrow=dim(y_train_org)[1], ncol=max(y_train_org)+1)#
counter <- 1#
for(y in 1:dim(y_train_org)[1]){#
	onehot_train[counter, y_train_org[y,]+1] <- 1#
	counter <- counter+1#
}#
#
X_train <- as.matrix(X_train)#
X_test <- as.matrix(X_test)#
y_train <- as.matrix(y_train_org)#
y_test <- as.matrix(y_test_org)#
#
if(onehot){#
	y_test <- onehot_test#
	y_train <- onehot_train#
}#
# only take 1000 samples, otherwise training takes increasingly long#
#chosenRowsTrain <- sample(1:nrow(trainData), size=nrow(X_train))#
#trainDataSmall <- trainData[chosenRowsTrain,]#
#trainLabelsSmall <- trainLabels[chosenRowsTrain,]#
darch  <- darch(X_train, y_train,#
  rbm.numEpochs = epochs_rbm,#
  rbm.consecutive = F, # each RBM is trained one epoch at a time#
  rbm.batchSize = batch_rbm,#
  rbm.lastLayer = -1, # don't train output layer#
  rbm.allData = T, # use bootstrap validation data as well for training#
  rbm.errorFunction = rmseError,#
  rbm.initialMomentum = .5,#
  rbm.finalMomentum = .7,#
  rbm.learnRate = ln_rate_rbm,#
  rbm.learnRateScale = ln_scale_rbm,#
  rbm.momentumRampLength = .8,#
  rbm.numCD = cd_rbm,#
  rbm.unitFunction = sigmoidUnitRbm,#
  rbm.weightDecay = .001,#
  layers = layers,#
  darch.batchSize = batch,#
  darch.dither = T,#
  darch.initialMomentum = .4,#
  darch.finalMomentum = .9,#
  darch.momentumRampLength = .75,#
  bp.learnRate = ln_rate_bp,#
  bp.learnRateScale = ln_scale_bp,#
  darch.unitFunction = units,#
  bootstrap = T,#
  darch.numEpochs = epochs_ft,#
  gputools = T, # try to use gputools#
  gputools.deviceId = 0,#
  xValid = X_test,#
  yValid = y_test#
)#
#
predictions <- predict(darch, newdata=X_test, type="class")#
labels <- cbind(predictions, y_test)#
numIncorrect <- sum(apply(labels, 1, function(i) { any(i[1:5] != i[6:10]) }))#
cat(paste0("Incorrect classifications on test data: ", numIncorrect," (", round(numIncorrect/nrow(y_test)*100, 2), "%)\n"))#
configs <- rbind(configs,data.frame(epochs_rbm= epochs_rbm, batch_rbm= batch_rbm, ln_rate_rbm= ln_rate_rbm, ln_scale_rbm= ln_scale_rbm, cd_rbm= cd_rbm, layers= paste(layers,collapse=" "), batch= batch, ln_rate_bp= ln_rate_bp, ln_scale_bp= ln_scale_bp, epochs_ft= epochs_ft, classification_error=numIncorrect/nrow(y_test)))#
predict_list[[dim(configs)[1]]] <- predictions#
predict_norm_list[[dim(configs)[1]]] <- labels#
nn_list[[dim(configs)[1]]] <- darch#
#
darch#
configs
# Loading library#
if(exists("configs") == F){#
	library(darch)#
	configs = data.frame(epochs_rbm=integer(), batch_rbm=integer(), ln_rate_rbm=numeric(), ln_scale_rbm=numeric(), cd_rbm=integer(), layers=character(), batch=integer(), ln_rate_bp=numeric(), ln_scale_bp=numeric(), epochs_ft=integer(), classification_error=numeric())#
	predict_list <- list()#
	predict_norm_list <- list()#
	nn_list <- list()#
}#
#
onehot <- 1#
epochs_rbm <- 20#
batch_rbm <- 100#
ln_rate_rbm <- .001#
ln_scale_rbm <- .8#
cd_rbm <- 10#
layers <- c(5,100,5)#
units <- c(tanhUnit, softmaxUnit)#
batch <- 100#
ln_rate_bp <- .001#
ln_scale_bp <- .8#
epochs_ft <- 20#
#
# Load csv#
#
X_train <- read.csv(file="../X_train.csv", header=T, sep=",", row.names=1)#
X_test <- read.csv(file="../X_test.csv", header=T, sep=",", row.names=1)#
y_train_org <- read.csv(file="../y_train.csv", header=T, sep=",", row.names=1)#
y_test_org <- read.csv(file="../y_test.csv", header=T, sep=",", row.names=1)#
#
X_train <- X_train[1:939,]#
X_test <- X_test[1:313,]#
#
# trying one hot encoding#
onehot_test <- matrix(0L, nrow=dim(y_test_org)[1], ncol=max(y_test_org)+1)#
counter <- 1#
for(y in 1:dim(y_test_org)[1]){#
	onehot_test[counter,y_test_org[y,]+1] <- 1#
	counter <- counter+1#
}#
# trying one hot encoding#
onehot_train <- matrix(0L, nrow=dim(y_train_org)[1], ncol=max(y_train_org)+1)#
counter <- 1#
for(y in 1:dim(y_train_org)[1]){#
	onehot_train[counter, y_train_org[y,]+1] <- 1#
	counter <- counter+1#
}#
#
X_train <- as.matrix(X_train)#
X_test <- as.matrix(X_test)#
y_train <- as.matrix(y_train_org)#
y_test <- as.matrix(y_test_org)#
#
if(onehot){#
	y_test <- onehot_test#
	y_train <- onehot_train#
}#
# only take 1000 samples, otherwise training takes increasingly long#
#chosenRowsTrain <- sample(1:nrow(trainData), size=nrow(X_train))#
#trainDataSmall <- trainData[chosenRowsTrain,]#
#trainLabelsSmall <- trainLabels[chosenRowsTrain,]#
darch  <- darch(X_train, y_train,#
  rbm.numEpochs = epochs_rbm,#
  rbm.consecutive = F, # each RBM is trained one epoch at a time#
  rbm.batchSize = batch_rbm,#
  rbm.lastLayer = -1, # don't train output layer#
  rbm.allData = T, # use bootstrap validation data as well for training#
  rbm.errorFunction = rmseError,#
  rbm.initialMomentum = .5,#
  rbm.finalMomentum = .7,#
  rbm.learnRate = ln_rate_rbm,#
  rbm.learnRateScale = ln_scale_rbm,#
  rbm.momentumRampLength = .8,#
  rbm.numCD = cd_rbm,#
  rbm.unitFunction = sigmoidUnitRbm,#
  rbm.weightDecay = .001,#
  layers = layers,#
  darch.batchSize = batch,#
  darch.dither = T,#
  darch.initialMomentum = .4,#
  darch.finalMomentum = .9,#
  darch.momentumRampLength = .75,#
  bp.learnRate = ln_rate_bp,#
  bp.learnRateScale = ln_scale_bp,#
  darch.unitFunction = units,#
  bootstrap = T,#
  darch.numEpochs = epochs_ft,#
  gputools = T, # try to use gputools#
  gputools.deviceId = 0,#
  xValid = X_test,#
  yValid = y_test#
)#
#
predictions <- predict(darch, newdata=X_test, type="class")#
labels <- cbind(predictions, y_test)#
numIncorrect <- sum(apply(labels, 1, function(i) { any(i[1:5] != i[6:10]) }))#
cat(paste0("Incorrect classifications on test data: ", numIncorrect," (", round(numIncorrect/nrow(y_test)*100, 2), "%)\n"))#
configs <- rbind(configs,data.frame(epochs_rbm= epochs_rbm, batch_rbm= batch_rbm, ln_rate_rbm= ln_rate_rbm, ln_scale_rbm= ln_scale_rbm, cd_rbm= cd_rbm, layers= paste(layers,collapse=" "), batch= batch, ln_rate_bp= ln_rate_bp, ln_scale_bp= ln_scale_bp, epochs_ft= epochs_ft, classification_error=numIncorrect/nrow(y_test)))#
predict_list[[dim(configs)[1]]] <- predictions#
predict_norm_list[[dim(configs)[1]]] <- labels#
nn_list[[dim(configs)[1]]] <- darch#
#
darch#
configs
predictions
# Loading library#
if(exists("configs") == F){#
	library(darch)#
	configs = data.frame(epochs_rbm=integer(), batch_rbm=integer(), ln_rate_rbm=numeric(), ln_scale_rbm=numeric(), cd_rbm=integer(), layers=character(), batch=integer(), ln_rate_bp=numeric(), ln_scale_bp=numeric(), epochs_ft=integer(), classification_error=numeric())#
	predict_list <- list()#
	predict_norm_list <- list()#
	nn_list <- list()#
}#
#
onehot <- 1#
epochs_rbm <- 20#
batch_rbm <- 200#
ln_rate_rbm <- .001#
ln_scale_rbm <- .8#
cd_rbm <- 10#
layers <- c(5,100,5)#
units <- c(tanhUnit, softmaxUnit)#
batch <- 100#
ln_rate_bp <- .001#
ln_scale_bp <- .8#
epochs_ft <- 20#
#
# Load csv#
#
X_train <- read.csv(file="../X_train.csv", header=T, sep=",", row.names=1)#
X_test <- read.csv(file="../X_test.csv", header=T, sep=",", row.names=1)#
y_train_org <- read.csv(file="../y_train.csv", header=T, sep=",", row.names=1)#
y_test_org <- read.csv(file="../y_test.csv", header=T, sep=",", row.names=1)#
#
X_train <- X_train[1:939,]#
X_test <- X_test[1:313,]#
#
# trying one hot encoding#
onehot_test <- matrix(0L, nrow=dim(y_test_org)[1], ncol=max(y_test_org)+1)#
counter <- 1#
for(y in 1:dim(y_test_org)[1]){#
	onehot_test[counter,y_test_org[y,]+1] <- 1#
	counter <- counter+1#
}#
# trying one hot encoding#
onehot_train <- matrix(0L, nrow=dim(y_train_org)[1], ncol=max(y_train_org)+1)#
counter <- 1#
for(y in 1:dim(y_train_org)[1]){#
	onehot_train[counter, y_train_org[y,]+1] <- 1#
	counter <- counter+1#
}#
#
X_train <- as.matrix(X_train)#
X_test <- as.matrix(X_test)#
y_train <- as.matrix(y_train_org)#
y_test <- as.matrix(y_test_org)#
#
if(onehot){#
	y_test <- onehot_test#
	y_train <- onehot_train#
}#
# only take 1000 samples, otherwise training takes increasingly long#
#chosenRowsTrain <- sample(1:nrow(trainData), size=nrow(X_train))#
#trainDataSmall <- trainData[chosenRowsTrain,]#
#trainLabelsSmall <- trainLabels[chosenRowsTrain,]#
darch  <- darch(X_train, y_train,#
  rbm.numEpochs = epochs_rbm,#
  rbm.consecutive = F, # each RBM is trained one epoch at a time#
  rbm.batchSize = batch_rbm,#
  rbm.lastLayer = -1, # don't train output layer#
  rbm.allData = T, # use bootstrap validation data as well for training#
  rbm.errorFunction = rmseError,#
  rbm.initialMomentum = .5,#
  rbm.finalMomentum = .7,#
  rbm.learnRate = ln_rate_rbm,#
  rbm.learnRateScale = ln_scale_rbm,#
  rbm.momentumRampLength = .8,#
  rbm.numCD = cd_rbm,#
  rbm.unitFunction = sigmoidUnitRbm,#
  rbm.weightDecay = .001,#
  layers = layers,#
  darch.batchSize = batch,#
  darch.dither = T,#
  darch.initialMomentum = .4,#
  darch.finalMomentum = .9,#
  darch.momentumRampLength = .75,#
  bp.learnRate = ln_rate_bp,#
  bp.learnRateScale = ln_scale_bp,#
  darch.unitFunction = units,#
  bootstrap = T,#
  darch.numEpochs = epochs_ft,#
  gputools = T, # try to use gputools#
  gputools.deviceId = 0,#
  xValid = X_test,#
  yValid = y_test#
)#
#
predictions <- predict(darch, newdata=X_test, type="class")#
labels <- cbind(predictions, y_test)#
numIncorrect <- sum(apply(labels, 1, function(i) { any(i[1:5] != i[6:10]) }))#
cat(paste0("Incorrect classifications on test data: ", numIncorrect," (", round(numIncorrect/nrow(y_test)*100, 2), "%)\n"))#
configs <- rbind(configs,data.frame(epochs_rbm= epochs_rbm, batch_rbm= batch_rbm, ln_rate_rbm= ln_rate_rbm, ln_scale_rbm= ln_scale_rbm, cd_rbm= cd_rbm, layers= paste(layers,collapse=" "), batch= batch, ln_rate_bp= ln_rate_bp, ln_scale_bp= ln_scale_bp, epochs_ft= epochs_ft, classification_error=numIncorrect/nrow(y_test)))#
predict_list[[dim(configs)[1]]] <- predictions#
predict_norm_list[[dim(configs)[1]]] <- labels#
nn_list[[dim(configs)[1]]] <- darch#
#
darch#
configs
# Loading library#
if(exists("configs") == F){#
	library(darch)#
	configs = data.frame(epochs_rbm=integer(), batch_rbm=integer(), ln_rate_rbm=numeric(), ln_scale_rbm=numeric(), cd_rbm=integer(), layers=character(), batch=integer(), ln_rate_bp=numeric(), ln_scale_bp=numeric(), epochs_ft=integer(), classification_error=numeric())#
	predict_list <- list()#
	predict_norm_list <- list()#
	nn_list <- list()#
}#
#
onehot <- 1#
epochs_rbm <- 20#
batch_rbm <- 200#
ln_rate_rbm <- .01#
ln_scale_rbm <- .8#
cd_rbm <- 10#
layers <- c(5,100,5)#
units <- c(tanhUnit, softmaxUnit)#
batch <- 100#
ln_rate_bp <- .001#
ln_scale_bp <- .8#
epochs_ft <- 20#
#
# Load csv#
#
X_train <- read.csv(file="../X_train.csv", header=T, sep=",", row.names=1)#
X_test <- read.csv(file="../X_test.csv", header=T, sep=",", row.names=1)#
y_train_org <- read.csv(file="../y_train.csv", header=T, sep=",", row.names=1)#
y_test_org <- read.csv(file="../y_test.csv", header=T, sep=",", row.names=1)#
#
X_train <- X_train[1:939,]#
X_test <- X_test[1:313,]#
#
# trying one hot encoding#
onehot_test <- matrix(0L, nrow=dim(y_test_org)[1], ncol=max(y_test_org)+1)#
counter <- 1#
for(y in 1:dim(y_test_org)[1]){#
	onehot_test[counter,y_test_org[y,]+1] <- 1#
	counter <- counter+1#
}#
# trying one hot encoding#
onehot_train <- matrix(0L, nrow=dim(y_train_org)[1], ncol=max(y_train_org)+1)#
counter <- 1#
for(y in 1:dim(y_train_org)[1]){#
	onehot_train[counter, y_train_org[y,]+1] <- 1#
	counter <- counter+1#
}#
#
X_train <- as.matrix(X_train)#
X_test <- as.matrix(X_test)#
y_train <- as.matrix(y_train_org)#
y_test <- as.matrix(y_test_org)#
#
if(onehot){#
	y_test <- onehot_test#
	y_train <- onehot_train#
}#
# only take 1000 samples, otherwise training takes increasingly long#
#chosenRowsTrain <- sample(1:nrow(trainData), size=nrow(X_train))#
#trainDataSmall <- trainData[chosenRowsTrain,]#
#trainLabelsSmall <- trainLabels[chosenRowsTrain,]#
darch  <- darch(X_train, y_train,#
  rbm.numEpochs = epochs_rbm,#
  rbm.consecutive = F, # each RBM is trained one epoch at a time#
  rbm.batchSize = batch_rbm,#
  rbm.lastLayer = -1, # don't train output layer#
  rbm.allData = T, # use bootstrap validation data as well for training#
  rbm.errorFunction = rmseError,#
  rbm.initialMomentum = .5,#
  rbm.finalMomentum = .7,#
  rbm.learnRate = ln_rate_rbm,#
  rbm.learnRateScale = ln_scale_rbm,#
  rbm.momentumRampLength = .8,#
  rbm.numCD = cd_rbm,#
  rbm.unitFunction = sigmoidUnitRbm,#
  rbm.weightDecay = .001,#
  layers = layers,#
  darch.batchSize = batch,#
  darch.dither = T,#
  darch.initialMomentum = .4,#
  darch.finalMomentum = .9,#
  darch.momentumRampLength = .75,#
  bp.learnRate = ln_rate_bp,#
  bp.learnRateScale = ln_scale_bp,#
  darch.unitFunction = units,#
  bootstrap = T,#
  darch.numEpochs = epochs_ft,#
  gputools = T, # try to use gputools#
  gputools.deviceId = 0,#
  xValid = X_test,#
  yValid = y_test#
)#
#
predictions <- predict(darch, newdata=X_test, type="class")#
labels <- cbind(predictions, y_test)#
numIncorrect <- sum(apply(labels, 1, function(i) { any(i[1:5] != i[6:10]) }))#
cat(paste0("Incorrect classifications on test data: ", numIncorrect," (", round(numIncorrect/nrow(y_test)*100, 2), "%)\n"))#
configs <- rbind(configs,data.frame(epochs_rbm= epochs_rbm, batch_rbm= batch_rbm, ln_rate_rbm= ln_rate_rbm, ln_scale_rbm= ln_scale_rbm, cd_rbm= cd_rbm, layers= paste(layers,collapse=" "), batch= batch, ln_rate_bp= ln_rate_bp, ln_scale_bp= ln_scale_bp, epochs_ft= epochs_ft, classification_error=numIncorrect/nrow(y_test)))#
predict_list[[dim(configs)[1]]] <- predictions#
predict_norm_list[[dim(configs)[1]]] <- labels#
nn_list[[dim(configs)[1]]] <- darch#
#
darch#
configs
predictions
# Loading library#
if(exists("configs") == F){#
	library(darch)#
	configs = data.frame(epochs_rbm=integer(), batch_rbm=integer(), ln_rate_rbm=numeric(), ln_scale_rbm=numeric(), cd_rbm=integer(), layers=character(), batch=integer(), ln_rate_bp=numeric(), ln_scale_bp=numeric(), epochs_ft=integer(), classification_error=numeric())#
	predict_list <- list()#
	predict_norm_list <- list()#
	nn_list <- list()#
}#
#
onehot <- 1#
epochs_rbm <- 20#
batch_rbm <- 200#
ln_rate_rbm <- .01#
ln_scale_rbm <- 1#
cd_rbm <- 10#
layers <- c(5,100,5)#
units <- c(tanhUnit, softmaxUnit)#
batch <- 100#
ln_rate_bp <- .001#
ln_scale_bp <- 1#
epochs_ft <- 20#
#
# Load csv#
#
X_train <- read.csv(file="../X_train.csv", header=T, sep=",", row.names=1)#
X_test <- read.csv(file="../X_test.csv", header=T, sep=",", row.names=1)#
y_train_org <- read.csv(file="../y_train.csv", header=T, sep=",", row.names=1)#
y_test_org <- read.csv(file="../y_test.csv", header=T, sep=",", row.names=1)#
#
X_train <- X_train[1:939,]#
X_test <- X_test[1:313,]#
#
# trying one hot encoding#
onehot_test <- matrix(0L, nrow=dim(y_test_org)[1], ncol=max(y_test_org)+1)#
counter <- 1#
for(y in 1:dim(y_test_org)[1]){#
	onehot_test[counter,y_test_org[y,]+1] <- 1#
	counter <- counter+1#
}#
# trying one hot encoding#
onehot_train <- matrix(0L, nrow=dim(y_train_org)[1], ncol=max(y_train_org)+1)#
counter <- 1#
for(y in 1:dim(y_train_org)[1]){#
	onehot_train[counter, y_train_org[y,]+1] <- 1#
	counter <- counter+1#
}#
#
X_train <- as.matrix(X_train)#
X_test <- as.matrix(X_test)#
y_train <- as.matrix(y_train_org)#
y_test <- as.matrix(y_test_org)#
#
if(onehot){#
	y_test <- onehot_test#
	y_train <- onehot_train#
}#
# only take 1000 samples, otherwise training takes increasingly long#
#chosenRowsTrain <- sample(1:nrow(trainData), size=nrow(X_train))#
#trainDataSmall <- trainData[chosenRowsTrain,]#
#trainLabelsSmall <- trainLabels[chosenRowsTrain,]#
darch  <- darch(X_train, y_train,#
  rbm.numEpochs = epochs_rbm,#
  rbm.consecutive = F, # each RBM is trained one epoch at a time#
  rbm.batchSize = batch_rbm,#
  rbm.lastLayer = -1, # don't train output layer#
  rbm.allData = T, # use bootstrap validation data as well for training#
  rbm.errorFunction = rmseError,#
  rbm.initialMomentum = .5,#
  rbm.finalMomentum = .7,#
  rbm.learnRate = ln_rate_rbm,#
  rbm.learnRateScale = ln_scale_rbm,#
  rbm.momentumRampLength = .8,#
  rbm.numCD = cd_rbm,#
  rbm.unitFunction = sigmoidUnitRbm,#
  rbm.weightDecay = .001,#
  layers = layers,#
  darch.batchSize = batch,#
  darch.dither = T,#
  darch.initialMomentum = .4,#
  darch.finalMomentum = .9,#
  darch.momentumRampLength = .75,#
  bp.learnRate = ln_rate_bp,#
  bp.learnRateScale = ln_scale_bp,#
  darch.unitFunction = units,#
  bootstrap = T,#
  darch.numEpochs = epochs_ft,#
  gputools = T, # try to use gputools#
  gputools.deviceId = 0,#
  xValid = X_test,#
  yValid = y_test#
)#
#
predictions <- predict(darch, newdata=X_test, type="class")#
labels <- cbind(predictions, y_test)#
numIncorrect <- sum(apply(labels, 1, function(i) { any(i[1:5] != i[6:10]) }))#
cat(paste0("Incorrect classifications on test data: ", numIncorrect," (", round(numIncorrect/nrow(y_test)*100, 2), "%)\n"))#
configs <- rbind(configs,data.frame(epochs_rbm= epochs_rbm, batch_rbm= batch_rbm, ln_rate_rbm= ln_rate_rbm, ln_scale_rbm= ln_scale_rbm, cd_rbm= cd_rbm, layers= paste(layers,collapse=" "), batch= batch, ln_rate_bp= ln_rate_bp, ln_scale_bp= ln_scale_bp, epochs_ft= epochs_ft, classification_error=numIncorrect/nrow(y_test)))#
predict_list[[dim(configs)[1]]] <- predictions#
predict_norm_list[[dim(configs)[1]]] <- labels#
nn_list[[dim(configs)[1]]] <- darch#
#
darch#
configs
# Loading library#
if(exists("configs") == F){#
	library(darch)#
	configs = data.frame(epochs_rbm=integer(), batch_rbm=integer(), ln_rate_rbm=numeric(), ln_scale_rbm=numeric(), cd_rbm=integer(), layers=character(), batch=integer(), ln_rate_bp=numeric(), ln_scale_bp=numeric(), epochs_ft=integer(), classification_error=numeric())#
	predict_list <- list()#
	predict_norm_list <- list()#
	nn_list <- list()#
}#
#
onehot <- 1#
epochs_rbm <- 20#
batch_rbm <- 200#
ln_rate_rbm <- .01#
ln_scale_rbm <- 1#
cd_rbm <- 10#
layers <- c(5,100,5)#
units <- c(tanhUnit, softmaxUnit)#
batch <- 100#
ln_rate_bp <- .01#
ln_scale_bp <- 1#
epochs_ft <- 20#
#
# Load csv#
#
X_train <- read.csv(file="../X_train.csv", header=T, sep=",", row.names=1)#
X_test <- read.csv(file="../X_test.csv", header=T, sep=",", row.names=1)#
y_train_org <- read.csv(file="../y_train.csv", header=T, sep=",", row.names=1)#
y_test_org <- read.csv(file="../y_test.csv", header=T, sep=",", row.names=1)#
#
X_train <- X_train[1:939,]#
X_test <- X_test[1:313,]#
#
# trying one hot encoding#
onehot_test <- matrix(0L, nrow=dim(y_test_org)[1], ncol=max(y_test_org)+1)#
counter <- 1#
for(y in 1:dim(y_test_org)[1]){#
	onehot_test[counter,y_test_org[y,]+1] <- 1#
	counter <- counter+1#
}#
# trying one hot encoding#
onehot_train <- matrix(0L, nrow=dim(y_train_org)[1], ncol=max(y_train_org)+1)#
counter <- 1#
for(y in 1:dim(y_train_org)[1]){#
	onehot_train[counter, y_train_org[y,]+1] <- 1#
	counter <- counter+1#
}#
#
X_train <- as.matrix(X_train)#
X_test <- as.matrix(X_test)#
y_train <- as.matrix(y_train_org)#
y_test <- as.matrix(y_test_org)#
#
if(onehot){#
	y_test <- onehot_test#
	y_train <- onehot_train#
}#
# only take 1000 samples, otherwise training takes increasingly long#
#chosenRowsTrain <- sample(1:nrow(trainData), size=nrow(X_train))#
#trainDataSmall <- trainData[chosenRowsTrain,]#
#trainLabelsSmall <- trainLabels[chosenRowsTrain,]#
darch  <- darch(X_train, y_train,#
  rbm.numEpochs = epochs_rbm,#
  rbm.consecutive = F, # each RBM is trained one epoch at a time#
  rbm.batchSize = batch_rbm,#
  rbm.lastLayer = -1, # don't train output layer#
  rbm.allData = T, # use bootstrap validation data as well for training#
  rbm.errorFunction = rmseError,#
  rbm.initialMomentum = .5,#
  rbm.finalMomentum = .7,#
  rbm.learnRate = ln_rate_rbm,#
  rbm.learnRateScale = ln_scale_rbm,#
  rbm.momentumRampLength = .8,#
  rbm.numCD = cd_rbm,#
  rbm.unitFunction = sigmoidUnitRbm,#
  rbm.weightDecay = .001,#
  layers = layers,#
  darch.batchSize = batch,#
  darch.dither = T,#
  darch.initialMomentum = .4,#
  darch.finalMomentum = .9,#
  darch.momentumRampLength = .75,#
  bp.learnRate = ln_rate_bp,#
  bp.learnRateScale = ln_scale_bp,#
  darch.unitFunction = units,#
  bootstrap = T,#
  darch.numEpochs = epochs_ft,#
  gputools = T, # try to use gputools#
  gputools.deviceId = 0,#
  xValid = X_test,#
  yValid = y_test#
)#
#
predictions <- predict(darch, newdata=X_test, type="class")#
labels <- cbind(predictions, y_test)#
numIncorrect <- sum(apply(labels, 1, function(i) { any(i[1:5] != i[6:10]) }))#
cat(paste0("Incorrect classifications on test data: ", numIncorrect," (", round(numIncorrect/nrow(y_test)*100, 2), "%)\n"))#
configs <- rbind(configs,data.frame(epochs_rbm= epochs_rbm, batch_rbm= batch_rbm, ln_rate_rbm= ln_rate_rbm, ln_scale_rbm= ln_scale_rbm, cd_rbm= cd_rbm, layers= paste(layers,collapse=" "), batch= batch, ln_rate_bp= ln_rate_bp, ln_scale_bp= ln_scale_bp, epochs_ft= epochs_ft, classification_error=numIncorrect/nrow(y_test)))#
predict_list[[dim(configs)[1]]] <- predictions#
predict_norm_list[[dim(configs)[1]]] <- labels#
nn_list[[dim(configs)[1]]] <- darch#
#
darch#
configs
# Loading library#
if(exists("configs") == F){#
	library(darch)#
	configs = data.frame(epochs_rbm=integer(), batch_rbm=integer(), ln_rate_rbm=numeric(), ln_scale_rbm=numeric(), cd_rbm=integer(), layers=character(), batch=integer(), ln_rate_bp=numeric(), ln_scale_bp=numeric(), epochs_ft=integer(), classification_error=numeric())#
	predict_list <- list()#
	predict_norm_list <- list()#
	nn_list <- list()#
}#
#
onehot <- 1#
epochs_rbm <- 20#
batch_rbm <- 200#
ln_rate_rbm <- .01#
ln_scale_rbm <- 1#
cd_rbm <- 10#
layers <- c(5,100,5)#
units <- c(tanhUnit, softmaxUnit)#
batch <- 100#
ln_rate_bp <- .01#
ln_scale_bp <- 0.9#
epochs_ft <- 20#
#
# Load csv#
#
X_train <- read.csv(file="../X_train.csv", header=T, sep=",", row.names=1)#
X_test <- read.csv(file="../X_test.csv", header=T, sep=",", row.names=1)#
y_train_org <- read.csv(file="../y_train.csv", header=T, sep=",", row.names=1)#
y_test_org <- read.csv(file="../y_test.csv", header=T, sep=",", row.names=1)#
#
X_train <- X_train[1:939,]#
X_test <- X_test[1:313,]#
#
# trying one hot encoding#
onehot_test <- matrix(0L, nrow=dim(y_test_org)[1], ncol=max(y_test_org)+1)#
counter <- 1#
for(y in 1:dim(y_test_org)[1]){#
	onehot_test[counter,y_test_org[y,]+1] <- 1#
	counter <- counter+1#
}#
# trying one hot encoding#
onehot_train <- matrix(0L, nrow=dim(y_train_org)[1], ncol=max(y_train_org)+1)#
counter <- 1#
for(y in 1:dim(y_train_org)[1]){#
	onehot_train[counter, y_train_org[y,]+1] <- 1#
	counter <- counter+1#
}#
#
X_train <- as.matrix(X_train)#
X_test <- as.matrix(X_test)#
y_train <- as.matrix(y_train_org)#
y_test <- as.matrix(y_test_org)#
#
if(onehot){#
	y_test <- onehot_test#
	y_train <- onehot_train#
}#
# only take 1000 samples, otherwise training takes increasingly long#
#chosenRowsTrain <- sample(1:nrow(trainData), size=nrow(X_train))#
#trainDataSmall <- trainData[chosenRowsTrain,]#
#trainLabelsSmall <- trainLabels[chosenRowsTrain,]#
darch  <- darch(X_train, y_train,#
  rbm.numEpochs = epochs_rbm,#
  rbm.consecutive = F, # each RBM is trained one epoch at a time#
  rbm.batchSize = batch_rbm,#
  rbm.lastLayer = -1, # don't train output layer#
  rbm.allData = T, # use bootstrap validation data as well for training#
  rbm.errorFunction = rmseError,#
  rbm.initialMomentum = .5,#
  rbm.finalMomentum = .7,#
  rbm.learnRate = ln_rate_rbm,#
  rbm.learnRateScale = ln_scale_rbm,#
  rbm.momentumRampLength = .8,#
  rbm.numCD = cd_rbm,#
  rbm.unitFunction = sigmoidUnitRbm,#
  rbm.weightDecay = .001,#
  layers = layers,#
  darch.batchSize = batch,#
  darch.dither = T,#
  darch.initialMomentum = .4,#
  darch.finalMomentum = .9,#
  darch.momentumRampLength = .75,#
  bp.learnRate = ln_rate_bp,#
  bp.learnRateScale = ln_scale_bp,#
  darch.unitFunction = units,#
  bootstrap = T,#
  darch.numEpochs = epochs_ft,#
  gputools = T, # try to use gputools#
  gputools.deviceId = 0,#
  xValid = X_test,#
  yValid = y_test#
)#
#
predictions <- predict(darch, newdata=X_test, type="class")#
labels <- cbind(predictions, y_test)#
numIncorrect <- sum(apply(labels, 1, function(i) { any(i[1:5] != i[6:10]) }))#
cat(paste0("Incorrect classifications on test data: ", numIncorrect," (", round(numIncorrect/nrow(y_test)*100, 2), "%)\n"))#
configs <- rbind(configs,data.frame(epochs_rbm= epochs_rbm, batch_rbm= batch_rbm, ln_rate_rbm= ln_rate_rbm, ln_scale_rbm= ln_scale_rbm, cd_rbm= cd_rbm, layers= paste(layers,collapse=" "), batch= batch, ln_rate_bp= ln_rate_bp, ln_scale_bp= ln_scale_bp, epochs_ft= epochs_ft, classification_error=numIncorrect/nrow(y_test)))#
predict_list[[dim(configs)[1]]] <- predictions#
predict_norm_list[[dim(configs)[1]]] <- labels#
nn_list[[dim(configs)[1]]] <- darch#
#
darch#
configs
ln_scale_bp <- 1
# Loading library#
if(exists("configs") == F){#
	library(darch)#
	configs = data.frame(epochs_rbm=integer(), batch_rbm=integer(), ln_rate_rbm=numeric(), ln_scale_rbm=numeric(), cd_rbm=integer(), layers=character(), batch=integer(), ln_rate_bp=numeric(), ln_scale_bp=numeric(), epochs_ft=integer(), classification_error=numeric())#
	predict_list <- list()#
	predict_norm_list <- list()#
	nn_list <- list()#
}#
#
onehot <- 1#
epochs_rbm <- 20#
batch_rbm <- 200#
ln_rate_rbm <- .01#
ln_scale_rbm <- 1#
cd_rbm <- 10#
layers <- c(5,100,5)#
units <- c(tanhUnit, softmaxUnit)#
batch <- 100#
ln_rate_bp <- .01#
ln_scale_bp <- 1#
epochs_ft <- 20#
#
# Load csv#
#
X_train <- read.csv(file="../X_train.csv", header=T, sep=",", row.names=1)#
X_test <- read.csv(file="../X_test.csv", header=T, sep=",", row.names=1)#
y_train_org <- read.csv(file="../y_train.csv", header=T, sep=",", row.names=1)#
y_test_org <- read.csv(file="../y_test.csv", header=T, sep=",", row.names=1)#
#
X_train <- X_train[1:939,]#
X_test <- X_test[1:313,]#
#
# trying one hot encoding#
onehot_test <- matrix(0L, nrow=dim(y_test_org)[1], ncol=max(y_test_org)+1)#
counter <- 1#
for(y in 1:dim(y_test_org)[1]){#
	onehot_test[counter,y_test_org[y,]+1] <- 1#
	counter <- counter+1#
}#
# trying one hot encoding#
onehot_train <- matrix(0L, nrow=dim(y_train_org)[1], ncol=max(y_train_org)+1)#
counter <- 1#
for(y in 1:dim(y_train_org)[1]){#
	onehot_train[counter, y_train_org[y,]+1] <- 1#
	counter <- counter+1#
}#
#
X_train <- as.matrix(X_train)#
X_test <- as.matrix(X_test)#
y_train <- as.matrix(y_train_org)#
y_test <- as.matrix(y_test_org)#
#
if(onehot){#
	y_test <- onehot_test#
	y_train <- onehot_train#
}#
# only take 1000 samples, otherwise training takes increasingly long#
#chosenRowsTrain <- sample(1:nrow(trainData), size=nrow(X_train))#
#trainDataSmall <- trainData[chosenRowsTrain,]#
#trainLabelsSmall <- trainLabels[chosenRowsTrain,]#
darch  <- darch(X_train, y_train,#
  rbm.numEpochs = epochs_rbm,#
  rbm.consecutive = F, # each RBM is trained one epoch at a time#
  rbm.batchSize = batch_rbm,#
  rbm.lastLayer = -1, # don't train output layer#
  rbm.allData = T, # use bootstrap validation data as well for training#
  rbm.errorFunction = rmseError,#
  rbm.initialMomentum = .5,#
  rbm.finalMomentum = .7,#
  rbm.learnRate = ln_rate_rbm,#
  rbm.learnRateScale = ln_scale_rbm,#
  rbm.momentumRampLength = .8,#
  rbm.numCD = cd_rbm,#
  rbm.unitFunction = sigmoidUnitRbm,#
  rbm.weightDecay = .001,#
  layers = layers,#
  darch.batchSize = batch,#
  darch.dither = T,#
  darch.initialMomentum = .4,#
  darch.finalMomentum = .9,#
  darch.momentumRampLength = .75,#
  bp.learnRate = ln_rate_bp,#
  bp.learnRateScale = ln_scale_bp,#
  darch.unitFunction = units,#
  bootstrap = T,#
  darch.numEpochs = epochs_ft,#
  gputools = T, # try to use gputools#
  gputools.deviceId = 0,#
  xValid = X_test,#
  yValid = y_test#
)#
#
predictions <- predict(darch, newdata=X_test, type="class")#
labels <- cbind(predictions, y_test)#
numIncorrect <- sum(apply(labels, 1, function(i) { any(i[1:5] != i[6:10]) }))#
cat(paste0("Incorrect classifications on test data: ", numIncorrect," (", round(numIncorrect/nrow(y_test)*100, 2), "%)\n"))#
configs <- rbind(configs,data.frame(epochs_rbm= epochs_rbm, batch_rbm= batch_rbm, ln_rate_rbm= ln_rate_rbm, ln_scale_rbm= ln_scale_rbm, cd_rbm= cd_rbm, layers= paste(layers,collapse=" "), batch= batch, ln_rate_bp= ln_rate_bp, ln_scale_bp= ln_scale_bp, epochs_ft= epochs_ft, classification_error=numIncorrect/nrow(y_test)))#
predict_list[[dim(configs)[1]]] <- predictions#
predict_norm_list[[dim(configs)[1]]] <- labels#
nn_list[[dim(configs)[1]]] <- darch#
#
darch#
configs
# Loading library#
if(exists("configs") == F){#
	library(darch)#
	configs = data.frame(epochs_rbm=integer(), batch_rbm=integer(), ln_rate_rbm=numeric(), ln_scale_rbm=numeric(), cd_rbm=integer(), layers=character(), batch=integer(), ln_rate_bp=numeric(), ln_scale_bp=numeric(), epochs_ft=integer(), classification_error=numeric())#
	predict_list <- list()#
	predict_norm_list <- list()#
	nn_list <- list()#
}#
#
onehot <- 1#
epochs_rbm <- 20#
batch_rbm <- 200#
ln_rate_rbm <- .01#
ln_scale_rbm <- 1#
cd_rbm <- 10#
layers <- c(5,100,5)#
units <- c(tanhUnit, softmaxUnit)#
batch <- 100#
ln_rate_bp <- .1#
ln_scale_bp <- 1#
epochs_ft <- 20#
#
# Load csv#
#
X_train <- read.csv(file="../X_train.csv", header=T, sep=",", row.names=1)#
X_test <- read.csv(file="../X_test.csv", header=T, sep=",", row.names=1)#
y_train_org <- read.csv(file="../y_train.csv", header=T, sep=",", row.names=1)#
y_test_org <- read.csv(file="../y_test.csv", header=T, sep=",", row.names=1)#
#
X_train <- X_train[1:939,]#
X_test <- X_test[1:313,]#
#
# trying one hot encoding#
onehot_test <- matrix(0L, nrow=dim(y_test_org)[1], ncol=max(y_test_org)+1)#
counter <- 1#
for(y in 1:dim(y_test_org)[1]){#
	onehot_test[counter,y_test_org[y,]+1] <- 1#
	counter <- counter+1#
}#
# trying one hot encoding#
onehot_train <- matrix(0L, nrow=dim(y_train_org)[1], ncol=max(y_train_org)+1)#
counter <- 1#
for(y in 1:dim(y_train_org)[1]){#
	onehot_train[counter, y_train_org[y,]+1] <- 1#
	counter <- counter+1#
}#
#
X_train <- as.matrix(X_train)#
X_test <- as.matrix(X_test)#
y_train <- as.matrix(y_train_org)#
y_test <- as.matrix(y_test_org)#
#
if(onehot){#
	y_test <- onehot_test#
	y_train <- onehot_train#
}#
# only take 1000 samples, otherwise training takes increasingly long#
#chosenRowsTrain <- sample(1:nrow(trainData), size=nrow(X_train))#
#trainDataSmall <- trainData[chosenRowsTrain,]#
#trainLabelsSmall <- trainLabels[chosenRowsTrain,]#
darch  <- darch(X_train, y_train,#
  rbm.numEpochs = epochs_rbm,#
  rbm.consecutive = F, # each RBM is trained one epoch at a time#
  rbm.batchSize = batch_rbm,#
  rbm.lastLayer = -1, # don't train output layer#
  rbm.allData = T, # use bootstrap validation data as well for training#
  rbm.errorFunction = rmseError,#
  rbm.initialMomentum = .5,#
  rbm.finalMomentum = .7,#
  rbm.learnRate = ln_rate_rbm,#
  rbm.learnRateScale = ln_scale_rbm,#
  rbm.momentumRampLength = .8,#
  rbm.numCD = cd_rbm,#
  rbm.unitFunction = sigmoidUnitRbm,#
  rbm.weightDecay = .001,#
  layers = layers,#
  darch.batchSize = batch,#
  darch.dither = T,#
  darch.initialMomentum = .4,#
  darch.finalMomentum = .9,#
  darch.momentumRampLength = .75,#
  bp.learnRate = ln_rate_bp,#
  bp.learnRateScale = ln_scale_bp,#
  darch.unitFunction = units,#
  bootstrap = T,#
  darch.numEpochs = epochs_ft,#
  gputools = T, # try to use gputools#
  gputools.deviceId = 0,#
  xValid = X_test,#
  yValid = y_test#
)#
#
predictions <- predict(darch, newdata=X_test, type="class")#
labels <- cbind(predictions, y_test)#
numIncorrect <- sum(apply(labels, 1, function(i) { any(i[1:5] != i[6:10]) }))#
cat(paste0("Incorrect classifications on test data: ", numIncorrect," (", round(numIncorrect/nrow(y_test)*100, 2), "%)\n"))#
configs <- rbind(configs,data.frame(epochs_rbm= epochs_rbm, batch_rbm= batch_rbm, ln_rate_rbm= ln_rate_rbm, ln_scale_rbm= ln_scale_rbm, cd_rbm= cd_rbm, layers= paste(layers,collapse=" "), batch= batch, ln_rate_bp= ln_rate_bp, ln_scale_bp= ln_scale_bp, epochs_ft= epochs_ft, classification_error=numIncorrect/nrow(y_test)))#
predict_list[[dim(configs)[1]]] <- predictions#
predict_norm_list[[dim(configs)[1]]] <- labels#
nn_list[[dim(configs)[1]]] <- darch#
#
darch#
configs
# MNIST example with pre-training#
dataFolder = "data/"#
downloadMNIST = T#
# Make sure to prove the correct folder if you have already downloaded the#
# MNIST data somewhere, or otherwise set downloadMNIST to TRUE#
provideMNIST(dataFolder, downloadMNIST)#
# Load MNIST data#
load(paste0(dataFolder, "train.RData")) # trainData, trainLabels#
load(paste0(dataFolder, "test.RData")) # testData, testLabels
# only take 1000 samples, otherwise training takes increasingly long#
chosenRowsTrain <- sample(1:nrow(trainData), size=1000)#
trainDataSmall <- trainData[chosenRowsTrain,]#
trainLabelsSmall <- trainLabels[chosenRowsTrain,]
trainDataSmall
# run this only once#
if(exists("configs") == F){#
	library(deepnet)#
	configs = data.frame(hidden_rbm=integer(), numempochs_rbm=integer(), batchsize_rbm=integer(), lr_rbm=numeric(), cd=integer(), hidden_nn=character(), lr_nn=numeric(), numepochs_nn=integer(), batchsize_nn=numeric(), onehot=integer(), err_score=numeric(), err_score_norm=numeric())#
	predict_list <- list()#
	predict_norm_list <- list()#
	nn_list <- list()#
}#
onehot <- 1#
#
hidden_rbm <- 100#
numepochs_rbm <- 10#
batchsize_rbm <- 100#
learningrate_rbm <- 0.1#
learningrate_scale_rbm <- 1#
cd <- 3#
#
hidden_nn <- c(100)#
learningrate_nn <- 0.1#
learningrate_scale_nn <- 1#
numepochs_nn <- 10#
batchsize_nn <- 100#
#
X_train <- read.csv(file="../X_train.csv", header=T, sep=",", row.names=1)#
X_test <- read.csv(file="../X_test.csv", header=T, sep=",", row.names=1)#
y_train_org <- read.csv(file="../y_train.csv", header=T, sep=",", row.names=1)#
y_test_org <- read.csv(file="../y_test.csv", header=T, sep=",", row.names=1)#
#
X_train <- X_train[1:939,]#
X_test <- X_test[1:313,]#
#
# trying one hot encoding#
onehot_test <- matrix(0L, nrow=dim(y_test_org)[1], ncol=max(y_test_org)+1)#
counter <- 1#
for(y in 1:dim(y_test_org)[1]){#
	onehot_test[counter,y_test_org[y,]+1] <- 1#
	counter <- counter+1#
}#
# trying one hot encoding#
onehot_train <- matrix(0L, nrow=dim(y_train_org)[1], ncol=max(y_train_org)+1)#
counter <- 1#
for(y in 1:dim(y_train_org)[1]){#
	onehot_train[counter, y_train_org[y,]+1] <- 1#
	counter <- counter+1#
}#
X_train <- as.matrix(X_train)#
X_test <- as.matrix(X_test)#
y_train <- as.matrix(y_train_org)#
y_test <- as.matrix(y_test_org)#
#
if(onehot){#
	y_test <- onehot_test#
	y_train <- onehot_train#
}#
#
rbm <- rbm.train(x=X_train, hidden=hidden_rbm, numepochs = numepochs_rbm, batchsize = batchsize_rbm, learningrate = learningrate_rbm, learningrate_scale = learningrate_scale_rbm, momentum = 0.5, visible_type = "bin", hidden_type = "bin", cd = cd)#
#
transformed_train <- rbm.up(rbm, X_train)#
transformed_test <- rbm.up(rbm, X_test)#
#
nn = nn.train(x=transformed_train, y_train, initW = NULL, initB = NULL, hidden = hidden_nn, activationfun = "sigm", learningrate = learningrate_nn, momentum = 0.5, learningrate_scale = learningrate_scale_nn, output = "sigm", numepochs = numepochs_nn, batchsize = batchsize_nn, hidden_dropout = 0, visible_dropout = 0)#
#
score <- 0#
score <- nn.test(nn, transformed_test, y_test, t = 0.5)#
#
pred <- nn.predict(nn, transformed_test)#
pred_norm <- matrix(0L, nrow=dim(pred)[1], ncol=dim(pred)[2])#
for(i in 1:dim(pred)[1]){#
	max_row <- which.max(pred[i,])#
	pred_norm[i,max_row] <- 1#
}#
#
score_norm <- 0#
for(i in 1:dim(pred_norm)[1]){#
	if(which.max(pred_norm[i,]) == which.max(y_test[i,])){#
		score_norm <- score_norm +1#
	}#
}#
score_norm <- 1- score_norm/dim(pred)[1]#
#
configs <- rbind(configs,data.frame(hidden_rbm=hidden_rbm, numempochs_rbm = numepochs_rbm, batchsize_rbm=batchsize_rbm, lr_rbm=learningrate_rbm, cd=cd, hidden_nn=paste(hidden_nn,collapse=" "), lr_nn=learningrate_nn, numepochs_nn=numepochs_nn, batchsize_nn=batchsize_nn, onehot=onehot, err_score=score, err_score_norm = score_norm))#
predict_list[[dim(configs)[1]]] <- pred#
predict_norm_list[[dim(configs)[1]]] <- pred_norm#
nn_list[[dim(configs)[1]]] <- nn#
#
configs
# run this only once#
if(exists("configs") == F){#
	library(deepnet)#
	configs = data.frame(hidden_rbm=integer(), numempochs_rbm=integer(), batchsize_rbm=integer(), lr_rbm=numeric(), cd=integer(), hidden_nn=character(), lr_nn=numeric(), numepochs_nn=integer(), batchsize_nn=numeric(), onehot=integer(), err_score=numeric(), err_score_norm=numeric())#
	predict_list <- list()#
	predict_norm_list <- list()#
	nn_list <- list()#
}#
onehot <- 1#
#
hidden_rbm <- 100#
numepochs_rbm <- 10#
batchsize_rbm <- 100#
learningrate_rbm <- 0.01#
learningrate_scale_rbm <- 1#
cd <- 3#
#
hidden_nn <- c(100)#
learningrate_nn <- 0.1#
learningrate_scale_nn <- 1#
numepochs_nn <- 10#
batchsize_nn <- 100#
#
X_train <- read.csv(file="../X_train.csv", header=T, sep=",", row.names=1)#
X_test <- read.csv(file="../X_test.csv", header=T, sep=",", row.names=1)#
y_train_org <- read.csv(file="../y_train.csv", header=T, sep=",", row.names=1)#
y_test_org <- read.csv(file="../y_test.csv", header=T, sep=",", row.names=1)#
#
X_train <- X_train[1:939,]#
X_test <- X_test[1:313,]#
#
# trying one hot encoding#
onehot_test <- matrix(0L, nrow=dim(y_test_org)[1], ncol=max(y_test_org)+1)#
counter <- 1#
for(y in 1:dim(y_test_org)[1]){#
	onehot_test[counter,y_test_org[y,]+1] <- 1#
	counter <- counter+1#
}#
# trying one hot encoding#
onehot_train <- matrix(0L, nrow=dim(y_train_org)[1], ncol=max(y_train_org)+1)#
counter <- 1#
for(y in 1:dim(y_train_org)[1]){#
	onehot_train[counter, y_train_org[y,]+1] <- 1#
	counter <- counter+1#
}#
X_train <- as.matrix(X_train)#
X_test <- as.matrix(X_test)#
y_train <- as.matrix(y_train_org)#
y_test <- as.matrix(y_test_org)#
#
if(onehot){#
	y_test <- onehot_test#
	y_train <- onehot_train#
}#
#
rbm <- rbm.train(x=X_train, hidden=hidden_rbm, numepochs = numepochs_rbm, batchsize = batchsize_rbm, learningrate = learningrate_rbm, learningrate_scale = learningrate_scale_rbm, momentum = 0.5, visible_type = "bin", hidden_type = "bin", cd = cd)#
#
transformed_train <- rbm.up(rbm, X_train)#
transformed_test <- rbm.up(rbm, X_test)#
#
nn = nn.train(x=transformed_train, y_train, initW = NULL, initB = NULL, hidden = hidden_nn, activationfun = "sigm", learningrate = learningrate_nn, momentum = 0.5, learningrate_scale = learningrate_scale_nn, output = "sigm", numepochs = numepochs_nn, batchsize = batchsize_nn, hidden_dropout = 0, visible_dropout = 0)#
#
score <- 0#
score <- nn.test(nn, transformed_test, y_test, t = 0.5)#
#
pred <- nn.predict(nn, transformed_test)#
pred_norm <- matrix(0L, nrow=dim(pred)[1], ncol=dim(pred)[2])#
for(i in 1:dim(pred)[1]){#
	max_row <- which.max(pred[i,])#
	pred_norm[i,max_row] <- 1#
}#
#
score_norm <- 0#
for(i in 1:dim(pred_norm)[1]){#
	if(which.max(pred_norm[i,]) == which.max(y_test[i,])){#
		score_norm <- score_norm +1#
	}#
}#
score_norm <- 1- score_norm/dim(pred)[1]#
#
configs <- rbind(configs,data.frame(hidden_rbm=hidden_rbm, numempochs_rbm = numepochs_rbm, batchsize_rbm=batchsize_rbm, lr_rbm=learningrate_rbm, cd=cd, hidden_nn=paste(hidden_nn,collapse=" "), lr_nn=learningrate_nn, numepochs_nn=numepochs_nn, batchsize_nn=batchsize_nn, onehot=onehot, err_score=score, err_score_norm = score_norm))#
predict_list[[dim(configs)[1]]] <- pred#
predict_norm_list[[dim(configs)[1]]] <- pred_norm#
nn_list[[dim(configs)[1]]] <- nn#
#
configs
# run this only once#
if(exists("configs") == F){#
	library(deepnet)#
	configs = data.frame(hidden_rbm=integer(), numempochs_rbm=integer(), batchsize_rbm=integer(), lr_rbm=numeric(), cd=integer(), hidden_nn=character(), lr_nn=numeric(), numepochs_nn=integer(), batchsize_nn=numeric(), onehot=integer(), err_score=numeric(), err_score_norm=numeric())#
	predict_list <- list()#
	predict_norm_list <- list()#
	nn_list <- list()#
}#
onehot <- 1#
#
hidden_rbm <- 100#
numepochs_rbm <- 10#
batchsize_rbm <- 100#
learningrate_rbm <- 0.01#
learningrate_scale_rbm <- 1#
cd <- 3#
#
hidden_nn <- c(100)#
learningrate_nn <- 0.01#
learningrate_scale_nn <- 1#
numepochs_nn <- 10#
batchsize_nn <- 100#
#
X_train <- read.csv(file="../X_train.csv", header=T, sep=",", row.names=1)#
X_test <- read.csv(file="../X_test.csv", header=T, sep=",", row.names=1)#
y_train_org <- read.csv(file="../y_train.csv", header=T, sep=",", row.names=1)#
y_test_org <- read.csv(file="../y_test.csv", header=T, sep=",", row.names=1)#
#
X_train <- X_train[1:939,]#
X_test <- X_test[1:313,]#
#
# trying one hot encoding#
onehot_test <- matrix(0L, nrow=dim(y_test_org)[1], ncol=max(y_test_org)+1)#
counter <- 1#
for(y in 1:dim(y_test_org)[1]){#
	onehot_test[counter,y_test_org[y,]+1] <- 1#
	counter <- counter+1#
}#
# trying one hot encoding#
onehot_train <- matrix(0L, nrow=dim(y_train_org)[1], ncol=max(y_train_org)+1)#
counter <- 1#
for(y in 1:dim(y_train_org)[1]){#
	onehot_train[counter, y_train_org[y,]+1] <- 1#
	counter <- counter+1#
}#
X_train <- as.matrix(X_train)#
X_test <- as.matrix(X_test)#
y_train <- as.matrix(y_train_org)#
y_test <- as.matrix(y_test_org)#
#
if(onehot){#
	y_test <- onehot_test#
	y_train <- onehot_train#
}#
#
rbm <- rbm.train(x=X_train, hidden=hidden_rbm, numepochs = numepochs_rbm, batchsize = batchsize_rbm, learningrate = learningrate_rbm, learningrate_scale = learningrate_scale_rbm, momentum = 0.5, visible_type = "bin", hidden_type = "bin", cd = cd)#
#
transformed_train <- rbm.up(rbm, X_train)#
transformed_test <- rbm.up(rbm, X_test)#
#
nn = nn.train(x=transformed_train, y_train, initW = NULL, initB = NULL, hidden = hidden_nn, activationfun = "sigm", learningrate = learningrate_nn, momentum = 0.5, learningrate_scale = learningrate_scale_nn, output = "sigm", numepochs = numepochs_nn, batchsize = batchsize_nn, hidden_dropout = 0, visible_dropout = 0)#
#
score <- 0#
score <- nn.test(nn, transformed_test, y_test, t = 0.5)#
#
pred <- nn.predict(nn, transformed_test)#
pred_norm <- matrix(0L, nrow=dim(pred)[1], ncol=dim(pred)[2])#
for(i in 1:dim(pred)[1]){#
	max_row <- which.max(pred[i,])#
	pred_norm[i,max_row] <- 1#
}#
#
score_norm <- 0#
for(i in 1:dim(pred_norm)[1]){#
	if(which.max(pred_norm[i,]) == which.max(y_test[i,])){#
		score_norm <- score_norm +1#
	}#
}#
score_norm <- 1- score_norm/dim(pred)[1]#
#
configs <- rbind(configs,data.frame(hidden_rbm=hidden_rbm, numempochs_rbm = numepochs_rbm, batchsize_rbm=batchsize_rbm, lr_rbm=learningrate_rbm, cd=cd, hidden_nn=paste(hidden_nn,collapse=" "), lr_nn=learningrate_nn, numepochs_nn=numepochs_nn, batchsize_nn=batchsize_nn, onehot=onehot, err_score=score, err_score_norm = score_norm))#
predict_list[[dim(configs)[1]]] <- pred#
predict_norm_list[[dim(configs)[1]]] <- pred_norm#
nn_list[[dim(configs)[1]]] <- nn#
#
configs
# run this only once#
if(exists("configs") == F){#
	library(deepnet)#
	configs = data.frame(hidden_rbm=integer(), numempochs_rbm=integer(), batchsize_rbm=integer(), lr_rbm=numeric(), cd=integer(), hidden_nn=character(), lr_nn=numeric(), numepochs_nn=integer(), batchsize_nn=numeric(), onehot=integer(), err_score=numeric(), err_score_norm=numeric())#
	predict_list <- list()#
	predict_norm_list <- list()#
	nn_list <- list()#
}#
onehot <- 1#
#
hidden_rbm <- 100#
numepochs_rbm <- 10#
batchsize_rbm <- 100#
learningrate_rbm <- 0.01#
learningrate_scale_rbm <- 0.9#
cd <- 3#
#
hidden_nn <- c(100)#
learningrate_nn <- 0.01#
learningrate_scale_nn <- 1#
numepochs_nn <- 10#
batchsize_nn <- 100#
#
X_train <- read.csv(file="../X_train.csv", header=T, sep=",", row.names=1)#
X_test <- read.csv(file="../X_test.csv", header=T, sep=",", row.names=1)#
y_train_org <- read.csv(file="../y_train.csv", header=T, sep=",", row.names=1)#
y_test_org <- read.csv(file="../y_test.csv", header=T, sep=",", row.names=1)#
#
X_train <- X_train[1:939,]#
X_test <- X_test[1:313,]#
#
# trying one hot encoding#
onehot_test <- matrix(0L, nrow=dim(y_test_org)[1], ncol=max(y_test_org)+1)#
counter <- 1#
for(y in 1:dim(y_test_org)[1]){#
	onehot_test[counter,y_test_org[y,]+1] <- 1#
	counter <- counter+1#
}#
# trying one hot encoding#
onehot_train <- matrix(0L, nrow=dim(y_train_org)[1], ncol=max(y_train_org)+1)#
counter <- 1#
for(y in 1:dim(y_train_org)[1]){#
	onehot_train[counter, y_train_org[y,]+1] <- 1#
	counter <- counter+1#
}#
X_train <- as.matrix(X_train)#
X_test <- as.matrix(X_test)#
y_train <- as.matrix(y_train_org)#
y_test <- as.matrix(y_test_org)#
#
if(onehot){#
	y_test <- onehot_test#
	y_train <- onehot_train#
}#
#
rbm <- rbm.train(x=X_train, hidden=hidden_rbm, numepochs = numepochs_rbm, batchsize = batchsize_rbm, learningrate = learningrate_rbm, learningrate_scale = learningrate_scale_rbm, momentum = 0.5, visible_type = "bin", hidden_type = "bin", cd = cd)#
#
transformed_train <- rbm.up(rbm, X_train)#
transformed_test <- rbm.up(rbm, X_test)#
#
nn = nn.train(x=transformed_train, y_train, initW = NULL, initB = NULL, hidden = hidden_nn, activationfun = "sigm", learningrate = learningrate_nn, momentum = 0.5, learningrate_scale = learningrate_scale_nn, output = "sigm", numepochs = numepochs_nn, batchsize = batchsize_nn, hidden_dropout = 0, visible_dropout = 0)#
#
score <- 0#
score <- nn.test(nn, transformed_test, y_test, t = 0.5)#
#
pred <- nn.predict(nn, transformed_test)#
pred_norm <- matrix(0L, nrow=dim(pred)[1], ncol=dim(pred)[2])#
for(i in 1:dim(pred)[1]){#
	max_row <- which.max(pred[i,])#
	pred_norm[i,max_row] <- 1#
}#
#
score_norm <- 0#
for(i in 1:dim(pred_norm)[1]){#
	if(which.max(pred_norm[i,]) == which.max(y_test[i,])){#
		score_norm <- score_norm +1#
	}#
}#
score_norm <- 1- score_norm/dim(pred)[1]#
#
configs <- rbind(configs,data.frame(hidden_rbm=hidden_rbm, numempochs_rbm = numepochs_rbm, batchsize_rbm=batchsize_rbm, lr_rbm=learningrate_rbm, cd=cd, hidden_nn=paste(hidden_nn,collapse=" "), lr_nn=learningrate_nn, numepochs_nn=numepochs_nn, batchsize_nn=batchsize_nn, onehot=onehot, err_score=score, err_score_norm = score_norm))#
predict_list[[dim(configs)[1]]] <- pred#
predict_norm_list[[dim(configs)[1]]] <- pred_norm#
nn_list[[dim(configs)[1]]] <- nn#
#
configs
# run this only once#
if(exists("configs") == F){#
	library(deepnet)#
	configs = data.frame(hidden_rbm=integer(), numempochs_rbm=integer(), batchsize_rbm=integer(), lr_rbm=numeric(), cd=integer(), hidden_nn=character(), lr_nn=numeric(), numepochs_nn=integer(), batchsize_nn=numeric(), onehot=integer(), err_score=numeric(), err_score_norm=numeric())#
	predict_list <- list()#
	predict_norm_list <- list()#
	nn_list <- list()#
}#
onehot <- 1#
#
hidden_rbm <- 100#
numepochs_rbm <- 10#
batchsize_rbm <- 100#
learningrate_rbm <- 0.01#
learningrate_scale_rbm <- 0.9#
cd <- 3#
#
hidden_nn <- c(100)#
learningrate_nn <- 0.01#
learningrate_scale_nn <- 0.9#
numepochs_nn <- 10#
batchsize_nn <- 100#
#
X_train <- read.csv(file="../X_train.csv", header=T, sep=",", row.names=1)#
X_test <- read.csv(file="../X_test.csv", header=T, sep=",", row.names=1)#
y_train_org <- read.csv(file="../y_train.csv", header=T, sep=",", row.names=1)#
y_test_org <- read.csv(file="../y_test.csv", header=T, sep=",", row.names=1)#
#
X_train <- X_train[1:939,]#
X_test <- X_test[1:313,]#
#
# trying one hot encoding#
onehot_test <- matrix(0L, nrow=dim(y_test_org)[1], ncol=max(y_test_org)+1)#
counter <- 1#
for(y in 1:dim(y_test_org)[1]){#
	onehot_test[counter,y_test_org[y,]+1] <- 1#
	counter <- counter+1#
}#
# trying one hot encoding#
onehot_train <- matrix(0L, nrow=dim(y_train_org)[1], ncol=max(y_train_org)+1)#
counter <- 1#
for(y in 1:dim(y_train_org)[1]){#
	onehot_train[counter, y_train_org[y,]+1] <- 1#
	counter <- counter+1#
}#
X_train <- as.matrix(X_train)#
X_test <- as.matrix(X_test)#
y_train <- as.matrix(y_train_org)#
y_test <- as.matrix(y_test_org)#
#
if(onehot){#
	y_test <- onehot_test#
	y_train <- onehot_train#
}#
#
rbm <- rbm.train(x=X_train, hidden=hidden_rbm, numepochs = numepochs_rbm, batchsize = batchsize_rbm, learningrate = learningrate_rbm, learningrate_scale = learningrate_scale_rbm, momentum = 0.5, visible_type = "bin", hidden_type = "bin", cd = cd)#
#
transformed_train <- rbm.up(rbm, X_train)#
transformed_test <- rbm.up(rbm, X_test)#
#
nn = nn.train(x=transformed_train, y_train, initW = NULL, initB = NULL, hidden = hidden_nn, activationfun = "sigm", learningrate = learningrate_nn, momentum = 0.5, learningrate_scale = learningrate_scale_nn, output = "sigm", numepochs = numepochs_nn, batchsize = batchsize_nn, hidden_dropout = 0, visible_dropout = 0)#
#
score <- 0#
score <- nn.test(nn, transformed_test, y_test, t = 0.5)#
#
pred <- nn.predict(nn, transformed_test)#
pred_norm <- matrix(0L, nrow=dim(pred)[1], ncol=dim(pred)[2])#
for(i in 1:dim(pred)[1]){#
	max_row <- which.max(pred[i,])#
	pred_norm[i,max_row] <- 1#
}#
#
score_norm <- 0#
for(i in 1:dim(pred_norm)[1]){#
	if(which.max(pred_norm[i,]) == which.max(y_test[i,])){#
		score_norm <- score_norm +1#
	}#
}#
score_norm <- 1- score_norm/dim(pred)[1]#
#
configs <- rbind(configs,data.frame(hidden_rbm=hidden_rbm, numempochs_rbm = numepochs_rbm, batchsize_rbm=batchsize_rbm, lr_rbm=learningrate_rbm, cd=cd, hidden_nn=paste(hidden_nn,collapse=" "), lr_nn=learningrate_nn, numepochs_nn=numepochs_nn, batchsize_nn=batchsize_nn, onehot=onehot, err_score=score, err_score_norm = score_norm))#
predict_list[[dim(configs)[1]]] <- pred#
predict_norm_list[[dim(configs)[1]]] <- pred_norm#
nn_list[[dim(configs)[1]]] <- nn#
#
configs
# run this only once#
if(exists("configs") == F){#
	library(deepnet)#
	configs = data.frame(hidden_rbm=integer(), numempochs_rbm=integer(), batchsize_rbm=integer(), lr_rbm=numeric(), cd=integer(), hidden_nn=character(), lr_nn=numeric(), numepochs_nn=integer(), batchsize_nn=numeric(), onehot=integer(), err_score=numeric(), err_score_norm=numeric())#
	predict_list <- list()#
	predict_norm_list <- list()#
	nn_list <- list()#
}#
onehot <- 1#
#
hidden_rbm <- 100#
numepochs_rbm <- 10#
batchsize_rbm <- 100#
learningrate_rbm <- 0.001#
learningrate_scale_rbm <- 0.9#
cd <- 3#
#
hidden_nn <- c(100)#
learningrate_nn <- 0.01#
learningrate_scale_nn <- 0.9#
numepochs_nn <- 10#
batchsize_nn <- 100#
#
X_train <- read.csv(file="../X_train.csv", header=T, sep=",", row.names=1)#
X_test <- read.csv(file="../X_test.csv", header=T, sep=",", row.names=1)#
y_train_org <- read.csv(file="../y_train.csv", header=T, sep=",", row.names=1)#
y_test_org <- read.csv(file="../y_test.csv", header=T, sep=",", row.names=1)#
#
X_train <- X_train[1:939,]#
X_test <- X_test[1:313,]#
#
# trying one hot encoding#
onehot_test <- matrix(0L, nrow=dim(y_test_org)[1], ncol=max(y_test_org)+1)#
counter <- 1#
for(y in 1:dim(y_test_org)[1]){#
	onehot_test[counter,y_test_org[y,]+1] <- 1#
	counter <- counter+1#
}#
# trying one hot encoding#
onehot_train <- matrix(0L, nrow=dim(y_train_org)[1], ncol=max(y_train_org)+1)#
counter <- 1#
for(y in 1:dim(y_train_org)[1]){#
	onehot_train[counter, y_train_org[y,]+1] <- 1#
	counter <- counter+1#
}#
X_train <- as.matrix(X_train)#
X_test <- as.matrix(X_test)#
y_train <- as.matrix(y_train_org)#
y_test <- as.matrix(y_test_org)#
#
if(onehot){#
	y_test <- onehot_test#
	y_train <- onehot_train#
}#
#
rbm <- rbm.train(x=X_train, hidden=hidden_rbm, numepochs = numepochs_rbm, batchsize = batchsize_rbm, learningrate = learningrate_rbm, learningrate_scale = learningrate_scale_rbm, momentum = 0.5, visible_type = "bin", hidden_type = "bin", cd = cd)#
#
transformed_train <- rbm.up(rbm, X_train)#
transformed_test <- rbm.up(rbm, X_test)#
#
nn = nn.train(x=transformed_train, y_train, initW = NULL, initB = NULL, hidden = hidden_nn, activationfun = "sigm", learningrate = learningrate_nn, momentum = 0.5, learningrate_scale = learningrate_scale_nn, output = "sigm", numepochs = numepochs_nn, batchsize = batchsize_nn, hidden_dropout = 0, visible_dropout = 0)#
#
score <- 0#
score <- nn.test(nn, transformed_test, y_test, t = 0.5)#
#
pred <- nn.predict(nn, transformed_test)#
pred_norm <- matrix(0L, nrow=dim(pred)[1], ncol=dim(pred)[2])#
for(i in 1:dim(pred)[1]){#
	max_row <- which.max(pred[i,])#
	pred_norm[i,max_row] <- 1#
}#
#
score_norm <- 0#
for(i in 1:dim(pred_norm)[1]){#
	if(which.max(pred_norm[i,]) == which.max(y_test[i,])){#
		score_norm <- score_norm +1#
	}#
}#
score_norm <- 1- score_norm/dim(pred)[1]#
#
configs <- rbind(configs,data.frame(hidden_rbm=hidden_rbm, numempochs_rbm = numepochs_rbm, batchsize_rbm=batchsize_rbm, lr_rbm=learningrate_rbm, cd=cd, hidden_nn=paste(hidden_nn,collapse=" "), lr_nn=learningrate_nn, numepochs_nn=numepochs_nn, batchsize_nn=batchsize_nn, onehot=onehot, err_score=score, err_score_norm = score_norm))#
predict_list[[dim(configs)[1]]] <- pred#
predict_norm_list[[dim(configs)[1]]] <- pred_norm#
nn_list[[dim(configs)[1]]] <- nn#
#
configs
# run this only once#
if(exists("configs") == F){#
	library(deepnet)#
	configs = data.frame(hidden_rbm=integer(), numempochs_rbm=integer(), batchsize_rbm=integer(), lr_rbm=numeric(), cd=integer(), hidden_nn=character(), lr_nn=numeric(), numepochs_nn=integer(), batchsize_nn=numeric(), onehot=integer(), err_score=numeric(), err_score_norm=numeric())#
	predict_list <- list()#
	predict_norm_list <- list()#
	nn_list <- list()#
}#
onehot <- 1#
#
hidden_rbm <- 100#
numepochs_rbm <- 10#
batchsize_rbm <- 100#
learningrate_rbm <- 0.001#
learningrate_scale_rbm <- 0.9#
cd <- 3#
#
hidden_nn <- c(100)#
learningrate_nn <- 0.001#
learningrate_scale_nn <- 0.9#
numepochs_nn <- 10#
batchsize_nn <- 100#
#
X_train <- read.csv(file="../X_train.csv", header=T, sep=",", row.names=1)#
X_test <- read.csv(file="../X_test.csv", header=T, sep=",", row.names=1)#
y_train_org <- read.csv(file="../y_train.csv", header=T, sep=",", row.names=1)#
y_test_org <- read.csv(file="../y_test.csv", header=T, sep=",", row.names=1)#
#
X_train <- X_train[1:939,]#
X_test <- X_test[1:313,]#
#
# trying one hot encoding#
onehot_test <- matrix(0L, nrow=dim(y_test_org)[1], ncol=max(y_test_org)+1)#
counter <- 1#
for(y in 1:dim(y_test_org)[1]){#
	onehot_test[counter,y_test_org[y,]+1] <- 1#
	counter <- counter+1#
}#
# trying one hot encoding#
onehot_train <- matrix(0L, nrow=dim(y_train_org)[1], ncol=max(y_train_org)+1)#
counter <- 1#
for(y in 1:dim(y_train_org)[1]){#
	onehot_train[counter, y_train_org[y,]+1] <- 1#
	counter <- counter+1#
}#
X_train <- as.matrix(X_train)#
X_test <- as.matrix(X_test)#
y_train <- as.matrix(y_train_org)#
y_test <- as.matrix(y_test_org)#
#
if(onehot){#
	y_test <- onehot_test#
	y_train <- onehot_train#
}#
#
rbm <- rbm.train(x=X_train, hidden=hidden_rbm, numepochs = numepochs_rbm, batchsize = batchsize_rbm, learningrate = learningrate_rbm, learningrate_scale = learningrate_scale_rbm, momentum = 0.5, visible_type = "bin", hidden_type = "bin", cd = cd)#
#
transformed_train <- rbm.up(rbm, X_train)#
transformed_test <- rbm.up(rbm, X_test)#
#
nn = nn.train(x=transformed_train, y_train, initW = NULL, initB = NULL, hidden = hidden_nn, activationfun = "sigm", learningrate = learningrate_nn, momentum = 0.5, learningrate_scale = learningrate_scale_nn, output = "sigm", numepochs = numepochs_nn, batchsize = batchsize_nn, hidden_dropout = 0, visible_dropout = 0)#
#
score <- 0#
score <- nn.test(nn, transformed_test, y_test, t = 0.5)#
#
pred <- nn.predict(nn, transformed_test)#
pred_norm <- matrix(0L, nrow=dim(pred)[1], ncol=dim(pred)[2])#
for(i in 1:dim(pred)[1]){#
	max_row <- which.max(pred[i,])#
	pred_norm[i,max_row] <- 1#
}#
#
score_norm <- 0#
for(i in 1:dim(pred_norm)[1]){#
	if(which.max(pred_norm[i,]) == which.max(y_test[i,])){#
		score_norm <- score_norm +1#
	}#
}#
score_norm <- 1- score_norm/dim(pred)[1]#
#
configs <- rbind(configs,data.frame(hidden_rbm=hidden_rbm, numempochs_rbm = numepochs_rbm, batchsize_rbm=batchsize_rbm, lr_rbm=learningrate_rbm, cd=cd, hidden_nn=paste(hidden_nn,collapse=" "), lr_nn=learningrate_nn, numepochs_nn=numepochs_nn, batchsize_nn=batchsize_nn, onehot=onehot, err_score=score, err_score_norm = score_norm))#
predict_list[[dim(configs)[1]]] <- pred#
predict_norm_list[[dim(configs)[1]]] <- pred_norm#
nn_list[[dim(configs)[1]]] <- nn#
#
configs
# run this only once#
if(exists("configs") == F){#
	library(deepnet)#
	configs = data.frame(hidden_rbm=integer(), numempochs_rbm=integer(), batchsize_rbm=integer(), lr_rbm=numeric(), cd=integer(), hidden_nn=character(), lr_nn=numeric(), numepochs_nn=integer(), batchsize_nn=numeric(), onehot=integer(), err_score=numeric(), err_score_norm=numeric())#
	predict_list <- list()#
	predict_norm_list <- list()#
	nn_list <- list()#
}#
onehot <- 1#
#
hidden_rbm <- 1000#
numepochs_rbm <- 10#
batchsize_rbm <- 100#
learningrate_rbm <- 0.01#
learningrate_scale_rbm <- 0.9#
cd <- 3#
#
hidden_nn <- c(100)#
learningrate_nn <- 0.01#
learningrate_scale_nn <- 0.9#
numepochs_nn <- 10#
batchsize_nn <- 100#
#
X_train <- read.csv(file="../X_train.csv", header=T, sep=",", row.names=1)#
X_test <- read.csv(file="../X_test.csv", header=T, sep=",", row.names=1)#
y_train_org <- read.csv(file="../y_train.csv", header=T, sep=",", row.names=1)#
y_test_org <- read.csv(file="../y_test.csv", header=T, sep=",", row.names=1)#
#
X_train <- X_train[1:939,]#
X_test <- X_test[1:313,]#
#
# trying one hot encoding#
onehot_test <- matrix(0L, nrow=dim(y_test_org)[1], ncol=max(y_test_org)+1)#
counter <- 1#
for(y in 1:dim(y_test_org)[1]){#
	onehot_test[counter,y_test_org[y,]+1] <- 1#
	counter <- counter+1#
}#
# trying one hot encoding#
onehot_train <- matrix(0L, nrow=dim(y_train_org)[1], ncol=max(y_train_org)+1)#
counter <- 1#
for(y in 1:dim(y_train_org)[1]){#
	onehot_train[counter, y_train_org[y,]+1] <- 1#
	counter <- counter+1#
}#
X_train <- as.matrix(X_train)#
X_test <- as.matrix(X_test)#
y_train <- as.matrix(y_train_org)#
y_test <- as.matrix(y_test_org)#
#
if(onehot){#
	y_test <- onehot_test#
	y_train <- onehot_train#
}#
#
rbm <- rbm.train(x=X_train, hidden=hidden_rbm, numepochs = numepochs_rbm, batchsize = batchsize_rbm, learningrate = learningrate_rbm, learningrate_scale = learningrate_scale_rbm, momentum = 0.5, visible_type = "bin", hidden_type = "bin", cd = cd)#
#
transformed_train <- rbm.up(rbm, X_train)#
transformed_test <- rbm.up(rbm, X_test)#
#
nn = nn.train(x=transformed_train, y_train, initW = NULL, initB = NULL, hidden = hidden_nn, activationfun = "sigm", learningrate = learningrate_nn, momentum = 0.5, learningrate_scale = learningrate_scale_nn, output = "sigm", numepochs = numepochs_nn, batchsize = batchsize_nn, hidden_dropout = 0, visible_dropout = 0)#
#
score <- 0#
score <- nn.test(nn, transformed_test, y_test, t = 0.5)#
#
pred <- nn.predict(nn, transformed_test)#
pred_norm <- matrix(0L, nrow=dim(pred)[1], ncol=dim(pred)[2])#
for(i in 1:dim(pred)[1]){#
	max_row <- which.max(pred[i,])#
	pred_norm[i,max_row] <- 1#
}#
#
score_norm <- 0#
for(i in 1:dim(pred_norm)[1]){#
	if(which.max(pred_norm[i,]) == which.max(y_test[i,])){#
		score_norm <- score_norm +1#
	}#
}#
score_norm <- 1- score_norm/dim(pred)[1]#
#
configs <- rbind(configs,data.frame(hidden_rbm=hidden_rbm, numempochs_rbm = numepochs_rbm, batchsize_rbm=batchsize_rbm, lr_rbm=learningrate_rbm, cd=cd, hidden_nn=paste(hidden_nn,collapse=" "), lr_nn=learningrate_nn, numepochs_nn=numepochs_nn, batchsize_nn=batchsize_nn, onehot=onehot, err_score=score, err_score_norm = score_norm))#
predict_list[[dim(configs)[1]]] <- pred#
predict_norm_list[[dim(configs)[1]]] <- pred_norm#
nn_list[[dim(configs)[1]]] <- nn#
#
configs
# run this only once#
if(exists("configs") == F){#
	library(deepnet)#
	configs = data.frame(hidden_rbm=integer(), numempochs_rbm=integer(), batchsize_rbm=integer(), lr_rbm=numeric(), cd=integer(), hidden_nn=character(), lr_nn=numeric(), numepochs_nn=integer(), batchsize_nn=numeric(), onehot=integer(), err_score=numeric(), err_score_norm=numeric())#
	predict_list <- list()#
	predict_norm_list <- list()#
	nn_list <- list()#
}#
onehot <- 1#
#
hidden_rbm <- 100#
numepochs_rbm <- 10#
batchsize_rbm <- 30#
learningrate_rbm <- 0.01#
learningrate_scale_rbm <- 0.9#
cd <- 3#
#
hidden_nn <- c(100)#
learningrate_nn <- 0.01#
learningrate_scale_nn <- 0.9#
numepochs_nn <- 10#
batchsize_nn <- 100#
#
X_train <- read.csv(file="../X_train.csv", header=T, sep=",", row.names=1)#
X_test <- read.csv(file="../X_test.csv", header=T, sep=",", row.names=1)#
y_train_org <- read.csv(file="../y_train.csv", header=T, sep=",", row.names=1)#
y_test_org <- read.csv(file="../y_test.csv", header=T, sep=",", row.names=1)#
#
X_train <- X_train[1:939,]#
X_test <- X_test[1:313,]#
#
# trying one hot encoding#
onehot_test <- matrix(0L, nrow=dim(y_test_org)[1], ncol=max(y_test_org)+1)#
counter <- 1#
for(y in 1:dim(y_test_org)[1]){#
	onehot_test[counter,y_test_org[y,]+1] <- 1#
	counter <- counter+1#
}#
# trying one hot encoding#
onehot_train <- matrix(0L, nrow=dim(y_train_org)[1], ncol=max(y_train_org)+1)#
counter <- 1#
for(y in 1:dim(y_train_org)[1]){#
	onehot_train[counter, y_train_org[y,]+1] <- 1#
	counter <- counter+1#
}#
X_train <- as.matrix(X_train)#
X_test <- as.matrix(X_test)#
y_train <- as.matrix(y_train_org)#
y_test <- as.matrix(y_test_org)#
#
if(onehot){#
	y_test <- onehot_test#
	y_train <- onehot_train#
}#
#
rbm <- rbm.train(x=X_train, hidden=hidden_rbm, numepochs = numepochs_rbm, batchsize = batchsize_rbm, learningrate = learningrate_rbm, learningrate_scale = learningrate_scale_rbm, momentum = 0.5, visible_type = "bin", hidden_type = "bin", cd = cd)#
#
transformed_train <- rbm.up(rbm, X_train)#
transformed_test <- rbm.up(rbm, X_test)#
#
nn = nn.train(x=transformed_train, y_train, initW = NULL, initB = NULL, hidden = hidden_nn, activationfun = "sigm", learningrate = learningrate_nn, momentum = 0.5, learningrate_scale = learningrate_scale_nn, output = "sigm", numepochs = numepochs_nn, batchsize = batchsize_nn, hidden_dropout = 0, visible_dropout = 0)#
#
score <- 0#
score <- nn.test(nn, transformed_test, y_test, t = 0.5)#
#
pred <- nn.predict(nn, transformed_test)#
pred_norm <- matrix(0L, nrow=dim(pred)[1], ncol=dim(pred)[2])#
for(i in 1:dim(pred)[1]){#
	max_row <- which.max(pred[i,])#
	pred_norm[i,max_row] <- 1#
}#
#
score_norm <- 0#
for(i in 1:dim(pred_norm)[1]){#
	if(which.max(pred_norm[i,]) == which.max(y_test[i,])){#
		score_norm <- score_norm +1#
	}#
}#
score_norm <- 1- score_norm/dim(pred)[1]#
#
configs <- rbind(configs,data.frame(hidden_rbm=hidden_rbm, numempochs_rbm = numepochs_rbm, batchsize_rbm=batchsize_rbm, lr_rbm=learningrate_rbm, cd=cd, hidden_nn=paste(hidden_nn,collapse=" "), lr_nn=learningrate_nn, numepochs_nn=numepochs_nn, batchsize_nn=batchsize_nn, onehot=onehot, err_score=score, err_score_norm = score_norm))#
predict_list[[dim(configs)[1]]] <- pred#
predict_norm_list[[dim(configs)[1]]] <- pred_norm#
nn_list[[dim(configs)[1]]] <- nn#
#
configs
pred_norm
# run this only once#
if(exists("configs") == F){#
	library(deepnet)#
	configs = data.frame(hidden_rbm=integer(), numempochs_rbm=integer(), batchsize_rbm=integer(), lr_rbm=numeric(), cd=integer(), hidden_nn=character(), lr_nn=numeric(), numepochs_nn=integer(), batchsize_nn=numeric(), onehot=integer(), err_score=numeric(), err_score_norm=numeric())#
	predict_list <- list()#
	predict_norm_list <- list()#
	nn_list <- list()#
}#
onehot <- 1#
#
hidden_rbm <- 100#
numepochs_rbm <- 10#
batchsize_rbm <- 60#
learningrate_rbm <- 0.01#
learningrate_scale_rbm <- 0.9#
cd <- 3#
#
hidden_nn <- c(100)#
learningrate_nn <- 0.01#
learningrate_scale_nn <- 0.9#
numepochs_nn <- 10#
batchsize_nn <- 100#
#
X_train <- read.csv(file="../X_train.csv", header=T, sep=",", row.names=1)#
X_test <- read.csv(file="../X_test.csv", header=T, sep=",", row.names=1)#
y_train_org <- read.csv(file="../y_train.csv", header=T, sep=",", row.names=1)#
y_test_org <- read.csv(file="../y_test.csv", header=T, sep=",", row.names=1)#
#
X_train <- X_train[1:939,]#
X_test <- X_test[1:313,]#
#
# trying one hot encoding#
onehot_test <- matrix(0L, nrow=dim(y_test_org)[1], ncol=max(y_test_org)+1)#
counter <- 1#
for(y in 1:dim(y_test_org)[1]){#
	onehot_test[counter,y_test_org[y,]+1] <- 1#
	counter <- counter+1#
}#
# trying one hot encoding#
onehot_train <- matrix(0L, nrow=dim(y_train_org)[1], ncol=max(y_train_org)+1)#
counter <- 1#
for(y in 1:dim(y_train_org)[1]){#
	onehot_train[counter, y_train_org[y,]+1] <- 1#
	counter <- counter+1#
}#
X_train <- as.matrix(X_train)#
X_test <- as.matrix(X_test)#
y_train <- as.matrix(y_train_org)#
y_test <- as.matrix(y_test_org)#
#
if(onehot){#
	y_test <- onehot_test#
	y_train <- onehot_train#
}#
#
rbm <- rbm.train(x=X_train, hidden=hidden_rbm, numepochs = numepochs_rbm, batchsize = batchsize_rbm, learningrate = learningrate_rbm, learningrate_scale = learningrate_scale_rbm, momentum = 0.5, visible_type = "bin", hidden_type = "bin", cd = cd)#
#
transformed_train <- rbm.up(rbm, X_train)#
transformed_test <- rbm.up(rbm, X_test)#
#
nn = nn.train(x=transformed_train, y_train, initW = NULL, initB = NULL, hidden = hidden_nn, activationfun = "sigm", learningrate = learningrate_nn, momentum = 0.5, learningrate_scale = learningrate_scale_nn, output = "sigm", numepochs = numepochs_nn, batchsize = batchsize_nn, hidden_dropout = 0, visible_dropout = 0)#
#
score <- 0#
score <- nn.test(nn, transformed_test, y_test, t = 0.5)#
#
pred <- nn.predict(nn, transformed_test)#
pred_norm <- matrix(0L, nrow=dim(pred)[1], ncol=dim(pred)[2])#
for(i in 1:dim(pred)[1]){#
	max_row <- which.max(pred[i,])#
	pred_norm[i,max_row] <- 1#
}#
#
score_norm <- 0#
for(i in 1:dim(pred_norm)[1]){#
	if(which.max(pred_norm[i,]) == which.max(y_test[i,])){#
		score_norm <- score_norm +1#
	}#
}#
score_norm <- 1- score_norm/dim(pred)[1]#
#
configs <- rbind(configs,data.frame(hidden_rbm=hidden_rbm, numempochs_rbm = numepochs_rbm, batchsize_rbm=batchsize_rbm, lr_rbm=learningrate_rbm, cd=cd, hidden_nn=paste(hidden_nn,collapse=" "), lr_nn=learningrate_nn, numepochs_nn=numepochs_nn, batchsize_nn=batchsize_nn, onehot=onehot, err_score=score, err_score_norm = score_norm))#
predict_list[[dim(configs)[1]]] <- pred#
predict_norm_list[[dim(configs)[1]]] <- pred_norm#
nn_list[[dim(configs)[1]]] <- nn#
#
configs
# run this only once#
if(exists("configs") == F){#
	library(deepnet)#
	configs = data.frame(hidden_rbm=integer(), numempochs_rbm=integer(), batchsize_rbm=integer(), lr_rbm=numeric(), cd=integer(), hidden_nn=character(), lr_nn=numeric(), numepochs_nn=integer(), batchsize_nn=numeric(), onehot=integer(), err_score=numeric(), err_score_norm=numeric())#
	predict_list <- list()#
	predict_norm_list <- list()#
	nn_list <- list()#
}#
onehot <- 1#
#
hidden_rbm <- 100#
numepochs_rbm <- 10#
batchsize_rbm <- 90#
learningrate_rbm <- 0.01#
learningrate_scale_rbm <- 0.9#
cd <- 3#
#
hidden_nn <- c(100)#
learningrate_nn <- 0.01#
learningrate_scale_nn <- 0.9#
numepochs_nn <- 10#
batchsize_nn <- 100#
#
X_train <- read.csv(file="../X_train.csv", header=T, sep=",", row.names=1)#
X_test <- read.csv(file="../X_test.csv", header=T, sep=",", row.names=1)#
y_train_org <- read.csv(file="../y_train.csv", header=T, sep=",", row.names=1)#
y_test_org <- read.csv(file="../y_test.csv", header=T, sep=",", row.names=1)#
#
X_train <- X_train[1:939,]#
X_test <- X_test[1:313,]#
#
# trying one hot encoding#
onehot_test <- matrix(0L, nrow=dim(y_test_org)[1], ncol=max(y_test_org)+1)#
counter <- 1#
for(y in 1:dim(y_test_org)[1]){#
	onehot_test[counter,y_test_org[y,]+1] <- 1#
	counter <- counter+1#
}#
# trying one hot encoding#
onehot_train <- matrix(0L, nrow=dim(y_train_org)[1], ncol=max(y_train_org)+1)#
counter <- 1#
for(y in 1:dim(y_train_org)[1]){#
	onehot_train[counter, y_train_org[y,]+1] <- 1#
	counter <- counter+1#
}#
X_train <- as.matrix(X_train)#
X_test <- as.matrix(X_test)#
y_train <- as.matrix(y_train_org)#
y_test <- as.matrix(y_test_org)#
#
if(onehot){#
	y_test <- onehot_test#
	y_train <- onehot_train#
}#
#
rbm <- rbm.train(x=X_train, hidden=hidden_rbm, numepochs = numepochs_rbm, batchsize = batchsize_rbm, learningrate = learningrate_rbm, learningrate_scale = learningrate_scale_rbm, momentum = 0.5, visible_type = "bin", hidden_type = "bin", cd = cd)#
#
transformed_train <- rbm.up(rbm, X_train)#
transformed_test <- rbm.up(rbm, X_test)#
#
nn = nn.train(x=transformed_train, y_train, initW = NULL, initB = NULL, hidden = hidden_nn, activationfun = "sigm", learningrate = learningrate_nn, momentum = 0.5, learningrate_scale = learningrate_scale_nn, output = "sigm", numepochs = numepochs_nn, batchsize = batchsize_nn, hidden_dropout = 0, visible_dropout = 0)#
#
score <- 0#
score <- nn.test(nn, transformed_test, y_test, t = 0.5)#
#
pred <- nn.predict(nn, transformed_test)#
pred_norm <- matrix(0L, nrow=dim(pred)[1], ncol=dim(pred)[2])#
for(i in 1:dim(pred)[1]){#
	max_row <- which.max(pred[i,])#
	pred_norm[i,max_row] <- 1#
}#
#
score_norm <- 0#
for(i in 1:dim(pred_norm)[1]){#
	if(which.max(pred_norm[i,]) == which.max(y_test[i,])){#
		score_norm <- score_norm +1#
	}#
}#
score_norm <- 1- score_norm/dim(pred)[1]#
#
configs <- rbind(configs,data.frame(hidden_rbm=hidden_rbm, numempochs_rbm = numepochs_rbm, batchsize_rbm=batchsize_rbm, lr_rbm=learningrate_rbm, cd=cd, hidden_nn=paste(hidden_nn,collapse=" "), lr_nn=learningrate_nn, numepochs_nn=numepochs_nn, batchsize_nn=batchsize_nn, onehot=onehot, err_score=score, err_score_norm = score_norm))#
predict_list[[dim(configs)[1]]] <- pred#
predict_norm_list[[dim(configs)[1]]] <- pred_norm#
nn_list[[dim(configs)[1]]] <- nn#
#
configs
# run this only once#
if(exists("configs") == F){#
	library(deepnet)#
	configs = data.frame(hidden_rbm=integer(), numempochs_rbm=integer(), batchsize_rbm=integer(), lr_rbm=numeric(), cd=integer(), hidden_nn=character(), lr_nn=numeric(), numepochs_nn=integer(), batchsize_nn=numeric(), onehot=integer(), err_score=numeric(), err_score_norm=numeric())#
	predict_list <- list()#
	predict_norm_list <- list()#
	nn_list <- list()#
}#
onehot <- 1#
#
hidden_rbm <- 100#
numepochs_rbm <- 10#
batchsize_rbm <- 90#
learningrate_rbm <- 0.01#
learningrate_scale_rbm <- 0.9#
cd <- 3#
#
hidden_nn <- c(100)#
learningrate_nn <- 0.01#
learningrate_scale_nn <- 0.9#
numepochs_nn <- 20#
batchsize_nn <- 100#
#
X_train <- read.csv(file="../X_train.csv", header=T, sep=",", row.names=1)#
X_test <- read.csv(file="../X_test.csv", header=T, sep=",", row.names=1)#
y_train_org <- read.csv(file="../y_train.csv", header=T, sep=",", row.names=1)#
y_test_org <- read.csv(file="../y_test.csv", header=T, sep=",", row.names=1)#
#
X_train <- X_train[1:939,]#
X_test <- X_test[1:313,]#
#
# trying one hot encoding#
onehot_test <- matrix(0L, nrow=dim(y_test_org)[1], ncol=max(y_test_org)+1)#
counter <- 1#
for(y in 1:dim(y_test_org)[1]){#
	onehot_test[counter,y_test_org[y,]+1] <- 1#
	counter <- counter+1#
}#
# trying one hot encoding#
onehot_train <- matrix(0L, nrow=dim(y_train_org)[1], ncol=max(y_train_org)+1)#
counter <- 1#
for(y in 1:dim(y_train_org)[1]){#
	onehot_train[counter, y_train_org[y,]+1] <- 1#
	counter <- counter+1#
}#
X_train <- as.matrix(X_train)#
X_test <- as.matrix(X_test)#
y_train <- as.matrix(y_train_org)#
y_test <- as.matrix(y_test_org)#
#
if(onehot){#
	y_test <- onehot_test#
	y_train <- onehot_train#
}#
#
rbm <- rbm.train(x=X_train, hidden=hidden_rbm, numepochs = numepochs_rbm, batchsize = batchsize_rbm, learningrate = learningrate_rbm, learningrate_scale = learningrate_scale_rbm, momentum = 0.5, visible_type = "bin", hidden_type = "bin", cd = cd)#
#
transformed_train <- rbm.up(rbm, X_train)#
transformed_test <- rbm.up(rbm, X_test)#
#
nn = nn.train(x=transformed_train, y_train, initW = NULL, initB = NULL, hidden = hidden_nn, activationfun = "sigm", learningrate = learningrate_nn, momentum = 0.5, learningrate_scale = learningrate_scale_nn, output = "sigm", numepochs = numepochs_nn, batchsize = batchsize_nn, hidden_dropout = 0, visible_dropout = 0)#
#
score <- 0#
score <- nn.test(nn, transformed_test, y_test, t = 0.5)#
#
pred <- nn.predict(nn, transformed_test)#
pred_norm <- matrix(0L, nrow=dim(pred)[1], ncol=dim(pred)[2])#
for(i in 1:dim(pred)[1]){#
	max_row <- which.max(pred[i,])#
	pred_norm[i,max_row] <- 1#
}#
#
score_norm <- 0#
for(i in 1:dim(pred_norm)[1]){#
	if(which.max(pred_norm[i,]) == which.max(y_test[i,])){#
		score_norm <- score_norm +1#
	}#
}#
score_norm <- 1- score_norm/dim(pred)[1]#
#
configs <- rbind(configs,data.frame(hidden_rbm=hidden_rbm, numempochs_rbm = numepochs_rbm, batchsize_rbm=batchsize_rbm, lr_rbm=learningrate_rbm, cd=cd, hidden_nn=paste(hidden_nn,collapse=" "), lr_nn=learningrate_nn, numepochs_nn=numepochs_nn, batchsize_nn=batchsize_nn, onehot=onehot, err_score=score, err_score_norm = score_norm))#
predict_list[[dim(configs)[1]]] <- pred#
predict_norm_list[[dim(configs)[1]]] <- pred_norm#
nn_list[[dim(configs)[1]]] <- nn#
#
configs
numepochs_rbm <- 20
# run this only once#
if(exists("configs") == F){#
	library(deepnet)#
	configs = data.frame(hidden_rbm=integer(), numempochs_rbm=integer(), batchsize_rbm=integer(), lr_rbm=numeric(), cd=integer(), hidden_nn=character(), lr_nn=numeric(), numepochs_nn=integer(), batchsize_nn=numeric(), onehot=integer(), err_score=numeric(), err_score_norm=numeric())#
	predict_list <- list()#
	predict_norm_list <- list()#
	nn_list <- list()#
}#
onehot <- 1#
#
hidden_rbm <- 100#
numepochs_rbm <- 20#
batchsize_rbm <- 90#
learningrate_rbm <- 0.01#
learningrate_scale_rbm <- 0.9#
cd <- 3#
#
hidden_nn <- c(100)#
learningrate_nn <- 0.01#
learningrate_scale_nn <- 0.9#
numepochs_nn <- 20#
batchsize_nn <- 100#
#
X_train <- read.csv(file="../X_train.csv", header=T, sep=",", row.names=1)#
X_test <- read.csv(file="../X_test.csv", header=T, sep=",", row.names=1)#
y_train_org <- read.csv(file="../y_train.csv", header=T, sep=",", row.names=1)#
y_test_org <- read.csv(file="../y_test.csv", header=T, sep=",", row.names=1)#
#
X_train <- X_train[1:939,]#
X_test <- X_test[1:313,]#
#
# trying one hot encoding#
onehot_test <- matrix(0L, nrow=dim(y_test_org)[1], ncol=max(y_test_org)+1)#
counter <- 1#
for(y in 1:dim(y_test_org)[1]){#
	onehot_test[counter,y_test_org[y,]+1] <- 1#
	counter <- counter+1#
}#
# trying one hot encoding#
onehot_train <- matrix(0L, nrow=dim(y_train_org)[1], ncol=max(y_train_org)+1)#
counter <- 1#
for(y in 1:dim(y_train_org)[1]){#
	onehot_train[counter, y_train_org[y,]+1] <- 1#
	counter <- counter+1#
}#
X_train <- as.matrix(X_train)#
X_test <- as.matrix(X_test)#
y_train <- as.matrix(y_train_org)#
y_test <- as.matrix(y_test_org)#
#
if(onehot){#
	y_test <- onehot_test#
	y_train <- onehot_train#
}#
#
rbm <- rbm.train(x=X_train, hidden=hidden_rbm, numepochs = numepochs_rbm, batchsize = batchsize_rbm, learningrate = learningrate_rbm, learningrate_scale = learningrate_scale_rbm, momentum = 0.5, visible_type = "bin", hidden_type = "bin", cd = cd)#
#
transformed_train <- rbm.up(rbm, X_train)#
transformed_test <- rbm.up(rbm, X_test)#
#
nn = nn.train(x=transformed_train, y_train, initW = NULL, initB = NULL, hidden = hidden_nn, activationfun = "sigm", learningrate = learningrate_nn, momentum = 0.5, learningrate_scale = learningrate_scale_nn, output = "sigm", numepochs = numepochs_nn, batchsize = batchsize_nn, hidden_dropout = 0, visible_dropout = 0)#
#
score <- 0#
score <- nn.test(nn, transformed_test, y_test, t = 0.5)#
#
pred <- nn.predict(nn, transformed_test)#
pred_norm <- matrix(0L, nrow=dim(pred)[1], ncol=dim(pred)[2])#
for(i in 1:dim(pred)[1]){#
	max_row <- which.max(pred[i,])#
	pred_norm[i,max_row] <- 1#
}#
#
score_norm <- 0#
for(i in 1:dim(pred_norm)[1]){#
	if(which.max(pred_norm[i,]) == which.max(y_test[i,])){#
		score_norm <- score_norm +1#
	}#
}#
score_norm <- 1- score_norm/dim(pred)[1]#
#
configs <- rbind(configs,data.frame(hidden_rbm=hidden_rbm, numempochs_rbm = numepochs_rbm, batchsize_rbm=batchsize_rbm, lr_rbm=learningrate_rbm, cd=cd, hidden_nn=paste(hidden_nn,collapse=" "), lr_nn=learningrate_nn, numepochs_nn=numepochs_nn, batchsize_nn=batchsize_nn, onehot=onehot, err_score=score, err_score_norm = score_norm))#
predict_list[[dim(configs)[1]]] <- pred#
predict_norm_list[[dim(configs)[1]]] <- pred_norm#
nn_list[[dim(configs)[1]]] <- nn#
#
configs
# run this only once#
if(exists("configs") == F){#
	library(deepnet)#
	configs = data.frame(hidden_rbm=integer(), numempochs_rbm=integer(), batchsize_rbm=integer(), lr_rbm=numeric(), cd=integer(), hidden_nn=character(), lr_nn=numeric(), numepochs_nn=integer(), batchsize_nn=numeric(), onehot=integer(), err_score=numeric(), err_score_norm=numeric())#
	predict_list <- list()#
	predict_norm_list <- list()#
	nn_list <- list()#
}#
onehot <- 1#
#
hidden_rbm <- 100#
numepochs_rbm <- 20#
batchsize_rbm <- 90#
learningrate_rbm <- 0.01#
learningrate_scale_rbm <- 0.8#
cd <- 3#
#
hidden_nn <- c(100)#
learningrate_nn <- 0.01#
learningrate_scale_nn <- 0.8#
numepochs_nn <- 20#
batchsize_nn <- 100#
#
X_train <- read.csv(file="../X_train.csv", header=T, sep=",", row.names=1)#
X_test <- read.csv(file="../X_test.csv", header=T, sep=",", row.names=1)#
y_train_org <- read.csv(file="../y_train.csv", header=T, sep=",", row.names=1)#
y_test_org <- read.csv(file="../y_test.csv", header=T, sep=",", row.names=1)#
#
X_train <- X_train[1:939,]#
X_test <- X_test[1:313,]#
#
# trying one hot encoding#
onehot_test <- matrix(0L, nrow=dim(y_test_org)[1], ncol=max(y_test_org)+1)#
counter <- 1#
for(y in 1:dim(y_test_org)[1]){#
	onehot_test[counter,y_test_org[y,]+1] <- 1#
	counter <- counter+1#
}#
# trying one hot encoding#
onehot_train <- matrix(0L, nrow=dim(y_train_org)[1], ncol=max(y_train_org)+1)#
counter <- 1#
for(y in 1:dim(y_train_org)[1]){#
	onehot_train[counter, y_train_org[y,]+1] <- 1#
	counter <- counter+1#
}#
X_train <- as.matrix(X_train)#
X_test <- as.matrix(X_test)#
y_train <- as.matrix(y_train_org)#
y_test <- as.matrix(y_test_org)#
#
if(onehot){#
	y_test <- onehot_test#
	y_train <- onehot_train#
}#
#
rbm <- rbm.train(x=X_train, hidden=hidden_rbm, numepochs = numepochs_rbm, batchsize = batchsize_rbm, learningrate = learningrate_rbm, learningrate_scale = learningrate_scale_rbm, momentum = 0.5, visible_type = "bin", hidden_type = "bin", cd = cd)#
#
transformed_train <- rbm.up(rbm, X_train)#
transformed_test <- rbm.up(rbm, X_test)#
#
nn = nn.train(x=transformed_train, y_train, initW = NULL, initB = NULL, hidden = hidden_nn, activationfun = "sigm", learningrate = learningrate_nn, momentum = 0.5, learningrate_scale = learningrate_scale_nn, output = "sigm", numepochs = numepochs_nn, batchsize = batchsize_nn, hidden_dropout = 0, visible_dropout = 0)#
#
score <- 0#
score <- nn.test(nn, transformed_test, y_test, t = 0.5)#
#
pred <- nn.predict(nn, transformed_test)#
pred_norm <- matrix(0L, nrow=dim(pred)[1], ncol=dim(pred)[2])#
for(i in 1:dim(pred)[1]){#
	max_row <- which.max(pred[i,])#
	pred_norm[i,max_row] <- 1#
}#
#
score_norm <- 0#
for(i in 1:dim(pred_norm)[1]){#
	if(which.max(pred_norm[i,]) == which.max(y_test[i,])){#
		score_norm <- score_norm +1#
	}#
}#
score_norm <- 1- score_norm/dim(pred)[1]#
#
configs <- rbind(configs,data.frame(hidden_rbm=hidden_rbm, numempochs_rbm = numepochs_rbm, batchsize_rbm=batchsize_rbm, lr_rbm=learningrate_rbm, cd=cd, hidden_nn=paste(hidden_nn,collapse=" "), lr_nn=learningrate_nn, numepochs_nn=numepochs_nn, batchsize_nn=batchsize_nn, onehot=onehot, err_score=score, err_score_norm = score_norm))#
predict_list[[dim(configs)[1]]] <- pred#
predict_norm_list[[dim(configs)[1]]] <- pred_norm#
nn_list[[dim(configs)[1]]] <- nn#
#
configs
# run this only once#
if(exists("configs") == F){#
	library(deepnet)#
	configs = data.frame(hidden_rbm=integer(), numempochs_rbm=integer(), batchsize_rbm=integer(), lr_rbm=numeric(), cd=integer(), hidden_nn=character(), lr_nn=numeric(), numepochs_nn=integer(), batchsize_nn=numeric(), onehot=integer(), err_score=numeric(), err_score_norm=numeric())#
	predict_list <- list()#
	predict_norm_list <- list()#
	nn_list <- list()#
}#
onehot <- 1#
#
hidden_rbm <- 100#
numepochs_rbm <- 20#
batchsize_rbm <- 90#
learningrate_rbm <- 0.01#
learningrate_scale_rbm <- 0.8#
cd <- 3#
#
hidden_nn <- c(10)#
learningrate_nn <- 0.01#
learningrate_scale_nn <- 0.8#
numepochs_nn <- 20#
batchsize_nn <- 100#
#
X_train <- read.csv(file="../X_train.csv", header=T, sep=",", row.names=1)#
X_test <- read.csv(file="../X_test.csv", header=T, sep=",", row.names=1)#
y_train_org <- read.csv(file="../y_train.csv", header=T, sep=",", row.names=1)#
y_test_org <- read.csv(file="../y_test.csv", header=T, sep=",", row.names=1)#
#
X_train <- X_train[1:939,]#
X_test <- X_test[1:313,]#
#
# trying one hot encoding#
onehot_test <- matrix(0L, nrow=dim(y_test_org)[1], ncol=max(y_test_org)+1)#
counter <- 1#
for(y in 1:dim(y_test_org)[1]){#
	onehot_test[counter,y_test_org[y,]+1] <- 1#
	counter <- counter+1#
}#
# trying one hot encoding#
onehot_train <- matrix(0L, nrow=dim(y_train_org)[1], ncol=max(y_train_org)+1)#
counter <- 1#
for(y in 1:dim(y_train_org)[1]){#
	onehot_train[counter, y_train_org[y,]+1] <- 1#
	counter <- counter+1#
}#
X_train <- as.matrix(X_train)#
X_test <- as.matrix(X_test)#
y_train <- as.matrix(y_train_org)#
y_test <- as.matrix(y_test_org)#
#
if(onehot){#
	y_test <- onehot_test#
	y_train <- onehot_train#
}#
#
rbm <- rbm.train(x=X_train, hidden=hidden_rbm, numepochs = numepochs_rbm, batchsize = batchsize_rbm, learningrate = learningrate_rbm, learningrate_scale = learningrate_scale_rbm, momentum = 0.5, visible_type = "bin", hidden_type = "bin", cd = cd)#
#
transformed_train <- rbm.up(rbm, X_train)#
transformed_test <- rbm.up(rbm, X_test)#
#
nn = nn.train(x=transformed_train, y_train, initW = NULL, initB = NULL, hidden = hidden_nn, activationfun = "sigm", learningrate = learningrate_nn, momentum = 0.5, learningrate_scale = learningrate_scale_nn, output = "sigm", numepochs = numepochs_nn, batchsize = batchsize_nn, hidden_dropout = 0, visible_dropout = 0)#
#
score <- 0#
score <- nn.test(nn, transformed_test, y_test, t = 0.5)#
#
pred <- nn.predict(nn, transformed_test)#
pred_norm <- matrix(0L, nrow=dim(pred)[1], ncol=dim(pred)[2])#
for(i in 1:dim(pred)[1]){#
	max_row <- which.max(pred[i,])#
	pred_norm[i,max_row] <- 1#
}#
#
score_norm <- 0#
for(i in 1:dim(pred_norm)[1]){#
	if(which.max(pred_norm[i,]) == which.max(y_test[i,])){#
		score_norm <- score_norm +1#
	}#
}#
score_norm <- 1- score_norm/dim(pred)[1]#
#
configs <- rbind(configs,data.frame(hidden_rbm=hidden_rbm, numempochs_rbm = numepochs_rbm, batchsize_rbm=batchsize_rbm, lr_rbm=learningrate_rbm, cd=cd, hidden_nn=paste(hidden_nn,collapse=" "), lr_nn=learningrate_nn, numepochs_nn=numepochs_nn, batchsize_nn=batchsize_nn, onehot=onehot, err_score=score, err_score_norm = score_norm))#
predict_list[[dim(configs)[1]]] <- pred#
predict_norm_list[[dim(configs)[1]]] <- pred_norm#
nn_list[[dim(configs)[1]]] <- nn#
#
configs
# run this only once#
if(exists("configs") == F){#
	library(deepnet)#
	configs = data.frame(hidden_rbm=integer(), numempochs_rbm=integer(), batchsize_rbm=integer(), lr_rbm=numeric(), cd=integer(), hidden_nn=character(), lr_nn=numeric(), numepochs_nn=integer(), batchsize_nn=numeric(), onehot=integer(), err_score=numeric(), err_score_norm=numeric())#
	predict_list <- list()#
	predict_norm_list <- list()#
	nn_list <- list()#
}#
onehot <- 1#
#
hidden_rbm <- 100#
numepochs_rbm <- 20#
batchsize_rbm <- 90#
learningrate_rbm <- 0.01#
learningrate_scale_rbm <- 0.8#
cd <- 3#
#
hidden_nn <- c(500)#
learningrate_nn <- 0.01#
learningrate_scale_nn <- 0.8#
numepochs_nn <- 20#
batchsize_nn <- 100#
#
X_train <- read.csv(file="../X_train.csv", header=T, sep=",", row.names=1)#
X_test <- read.csv(file="../X_test.csv", header=T, sep=",", row.names=1)#
y_train_org <- read.csv(file="../y_train.csv", header=T, sep=",", row.names=1)#
y_test_org <- read.csv(file="../y_test.csv", header=T, sep=",", row.names=1)#
#
X_train <- X_train[1:939,]#
X_test <- X_test[1:313,]#
#
# trying one hot encoding#
onehot_test <- matrix(0L, nrow=dim(y_test_org)[1], ncol=max(y_test_org)+1)#
counter <- 1#
for(y in 1:dim(y_test_org)[1]){#
	onehot_test[counter,y_test_org[y,]+1] <- 1#
	counter <- counter+1#
}#
# trying one hot encoding#
onehot_train <- matrix(0L, nrow=dim(y_train_org)[1], ncol=max(y_train_org)+1)#
counter <- 1#
for(y in 1:dim(y_train_org)[1]){#
	onehot_train[counter, y_train_org[y,]+1] <- 1#
	counter <- counter+1#
}#
X_train <- as.matrix(X_train)#
X_test <- as.matrix(X_test)#
y_train <- as.matrix(y_train_org)#
y_test <- as.matrix(y_test_org)#
#
if(onehot){#
	y_test <- onehot_test#
	y_train <- onehot_train#
}#
#
rbm <- rbm.train(x=X_train, hidden=hidden_rbm, numepochs = numepochs_rbm, batchsize = batchsize_rbm, learningrate = learningrate_rbm, learningrate_scale = learningrate_scale_rbm, momentum = 0.5, visible_type = "bin", hidden_type = "bin", cd = cd)#
#
transformed_train <- rbm.up(rbm, X_train)#
transformed_test <- rbm.up(rbm, X_test)#
#
nn = nn.train(x=transformed_train, y_train, initW = NULL, initB = NULL, hidden = hidden_nn, activationfun = "sigm", learningrate = learningrate_nn, momentum = 0.5, learningrate_scale = learningrate_scale_nn, output = "sigm", numepochs = numepochs_nn, batchsize = batchsize_nn, hidden_dropout = 0, visible_dropout = 0)#
#
score <- 0#
score <- nn.test(nn, transformed_test, y_test, t = 0.5)#
#
pred <- nn.predict(nn, transformed_test)#
pred_norm <- matrix(0L, nrow=dim(pred)[1], ncol=dim(pred)[2])#
for(i in 1:dim(pred)[1]){#
	max_row <- which.max(pred[i,])#
	pred_norm[i,max_row] <- 1#
}#
#
score_norm <- 0#
for(i in 1:dim(pred_norm)[1]){#
	if(which.max(pred_norm[i,]) == which.max(y_test[i,])){#
		score_norm <- score_norm +1#
	}#
}#
score_norm <- 1- score_norm/dim(pred)[1]#
#
configs <- rbind(configs,data.frame(hidden_rbm=hidden_rbm, numempochs_rbm = numepochs_rbm, batchsize_rbm=batchsize_rbm, lr_rbm=learningrate_rbm, cd=cd, hidden_nn=paste(hidden_nn,collapse=" "), lr_nn=learningrate_nn, numepochs_nn=numepochs_nn, batchsize_nn=batchsize_nn, onehot=onehot, err_score=score, err_score_norm = score_norm))#
predict_list[[dim(configs)[1]]] <- pred#
predict_norm_list[[dim(configs)[1]]] <- pred_norm#
nn_list[[dim(configs)[1]]] <- nn#
#
configs
# run this only once#
if(exists("configs") == F){#
	library(deepnet)#
	configs = data.frame(hidden_rbm=integer(), numempochs_rbm=integer(), batchsize_rbm=integer(), lr_rbm=numeric(), cd=integer(), hidden_nn=character(), lr_nn=numeric(), numepochs_nn=integer(), batchsize_nn=numeric(), onehot=integer(), err_score=numeric(), err_score_norm=numeric())#
	predict_list <- list()#
	predict_norm_list <- list()#
	nn_list <- list()#
}#
onehot <- 1#
#
hidden_rbm <- 100#
numepochs_rbm <- 10#
batchsize_rbm <- 90#
learningrate_rbm <- 0.01#
learningrate_scale_rbm <- 0.8#
cd <- 3#
#
hidden_nn <- c(500)#
learningrate_nn <- 0.01#
learningrate_scale_nn <- 0.8#
numepochs_nn <- 10#
batchsize_nn <- 100#
#
X_train <- read.csv(file="../X_train.csv", header=T, sep=",", row.names=1)#
X_test <- read.csv(file="../X_test.csv", header=T, sep=",", row.names=1)#
y_train_org <- read.csv(file="../y_train.csv", header=T, sep=",", row.names=1)#
y_test_org <- read.csv(file="../y_test.csv", header=T, sep=",", row.names=1)#
#
X_train <- X_train[1:939,]#
X_test <- X_test[1:313,]#
#
# trying one hot encoding#
onehot_test <- matrix(0L, nrow=dim(y_test_org)[1], ncol=max(y_test_org)+1)#
counter <- 1#
for(y in 1:dim(y_test_org)[1]){#
	onehot_test[counter,y_test_org[y,]+1] <- 1#
	counter <- counter+1#
}#
# trying one hot encoding#
onehot_train <- matrix(0L, nrow=dim(y_train_org)[1], ncol=max(y_train_org)+1)#
counter <- 1#
for(y in 1:dim(y_train_org)[1]){#
	onehot_train[counter, y_train_org[y,]+1] <- 1#
	counter <- counter+1#
}#
X_train <- as.matrix(X_train)#
X_test <- as.matrix(X_test)#
y_train <- as.matrix(y_train_org)#
y_test <- as.matrix(y_test_org)#
#
if(onehot){#
	y_test <- onehot_test#
	y_train <- onehot_train#
}#
#
rbm <- rbm.train(x=X_train, hidden=hidden_rbm, numepochs = numepochs_rbm, batchsize = batchsize_rbm, learningrate = learningrate_rbm, learningrate_scale = learningrate_scale_rbm, momentum = 0.5, visible_type = "bin", hidden_type = "bin", cd = cd)#
#
transformed_train <- rbm.up(rbm, X_train)#
transformed_test <- rbm.up(rbm, X_test)#
#
nn = nn.train(x=transformed_train, y_train, initW = NULL, initB = NULL, hidden = hidden_nn, activationfun = "sigm", learningrate = learningrate_nn, momentum = 0.5, learningrate_scale = learningrate_scale_nn, output = "sigm", numepochs = numepochs_nn, batchsize = batchsize_nn, hidden_dropout = 0, visible_dropout = 0)#
#
score <- 0#
score <- nn.test(nn, transformed_test, y_test, t = 0.5)#
#
pred <- nn.predict(nn, transformed_test)#
pred_norm <- matrix(0L, nrow=dim(pred)[1], ncol=dim(pred)[2])#
for(i in 1:dim(pred)[1]){#
	max_row <- which.max(pred[i,])#
	pred_norm[i,max_row] <- 1#
}#
#
score_norm <- 0#
for(i in 1:dim(pred_norm)[1]){#
	if(which.max(pred_norm[i,]) == which.max(y_test[i,])){#
		score_norm <- score_norm +1#
	}#
}#
score_norm <- 1- score_norm/dim(pred)[1]#
#
configs <- rbind(configs,data.frame(hidden_rbm=hidden_rbm, numempochs_rbm = numepochs_rbm, batchsize_rbm=batchsize_rbm, lr_rbm=learningrate_rbm, cd=cd, hidden_nn=paste(hidden_nn,collapse=" "), lr_nn=learningrate_nn, numepochs_nn=numepochs_nn, batchsize_nn=batchsize_nn, onehot=onehot, err_score=score, err_score_norm = score_norm))#
predict_list[[dim(configs)[1]]] <- pred#
predict_norm_list[[dim(configs)[1]]] <- pred_norm#
nn_list[[dim(configs)[1]]] <- nn#
#
configs
# run this only once#
if(exists("configs") == F){#
	library(deepnet)#
	configs = data.frame(hidden_rbm=integer(), numempochs_rbm=integer(), batchsize_rbm=integer(), lr_rbm=numeric(), cd=integer(), hidden_nn=character(), lr_nn=numeric(), numepochs_nn=integer(), batchsize_nn=numeric(), onehot=integer(), err_score=numeric(), err_score_norm=numeric())#
	predict_list <- list()#
	predict_norm_list <- list()#
	nn_list <- list()#
}#
onehot <- 1#
#
hidden_rbm <- 100#
numepochs_rbm <- 10#
batchsize_rbm <- 90#
learningrate_rbm <- 0.01#
learningrate_scale_rbm <- 0.8#
cd <- 3#
#
hidden_nn <- c(1000)#
learningrate_nn <- 0.01#
learningrate_scale_nn <- 0.8#
numepochs_nn <- 10#
batchsize_nn <- 100#
#
X_train <- read.csv(file="../X_train.csv", header=T, sep=",", row.names=1)#
X_test <- read.csv(file="../X_test.csv", header=T, sep=",", row.names=1)#
y_train_org <- read.csv(file="../y_train.csv", header=T, sep=",", row.names=1)#
y_test_org <- read.csv(file="../y_test.csv", header=T, sep=",", row.names=1)#
#
X_train <- X_train[1:939,]#
X_test <- X_test[1:313,]#
#
# trying one hot encoding#
onehot_test <- matrix(0L, nrow=dim(y_test_org)[1], ncol=max(y_test_org)+1)#
counter <- 1#
for(y in 1:dim(y_test_org)[1]){#
	onehot_test[counter,y_test_org[y,]+1] <- 1#
	counter <- counter+1#
}#
# trying one hot encoding#
onehot_train <- matrix(0L, nrow=dim(y_train_org)[1], ncol=max(y_train_org)+1)#
counter <- 1#
for(y in 1:dim(y_train_org)[1]){#
	onehot_train[counter, y_train_org[y,]+1] <- 1#
	counter <- counter+1#
}#
X_train <- as.matrix(X_train)#
X_test <- as.matrix(X_test)#
y_train <- as.matrix(y_train_org)#
y_test <- as.matrix(y_test_org)#
#
if(onehot){#
	y_test <- onehot_test#
	y_train <- onehot_train#
}#
#
rbm <- rbm.train(x=X_train, hidden=hidden_rbm, numepochs = numepochs_rbm, batchsize = batchsize_rbm, learningrate = learningrate_rbm, learningrate_scale = learningrate_scale_rbm, momentum = 0.5, visible_type = "bin", hidden_type = "bin", cd = cd)#
#
transformed_train <- rbm.up(rbm, X_train)#
transformed_test <- rbm.up(rbm, X_test)#
#
nn = nn.train(x=transformed_train, y_train, initW = NULL, initB = NULL, hidden = hidden_nn, activationfun = "sigm", learningrate = learningrate_nn, momentum = 0.5, learningrate_scale = learningrate_scale_nn, output = "sigm", numepochs = numepochs_nn, batchsize = batchsize_nn, hidden_dropout = 0, visible_dropout = 0)#
#
score <- 0#
score <- nn.test(nn, transformed_test, y_test, t = 0.5)#
#
pred <- nn.predict(nn, transformed_test)#
pred_norm <- matrix(0L, nrow=dim(pred)[1], ncol=dim(pred)[2])#
for(i in 1:dim(pred)[1]){#
	max_row <- which.max(pred[i,])#
	pred_norm[i,max_row] <- 1#
}#
#
score_norm <- 0#
for(i in 1:dim(pred_norm)[1]){#
	if(which.max(pred_norm[i,]) == which.max(y_test[i,])){#
		score_norm <- score_norm +1#
	}#
}#
score_norm <- 1- score_norm/dim(pred)[1]#
#
configs <- rbind(configs,data.frame(hidden_rbm=hidden_rbm, numempochs_rbm = numepochs_rbm, batchsize_rbm=batchsize_rbm, lr_rbm=learningrate_rbm, cd=cd, hidden_nn=paste(hidden_nn,collapse=" "), lr_nn=learningrate_nn, numepochs_nn=numepochs_nn, batchsize_nn=batchsize_nn, onehot=onehot, err_score=score, err_score_norm = score_norm))#
predict_list[[dim(configs)[1]]] <- pred#
predict_norm_list[[dim(configs)[1]]] <- pred_norm#
nn_list[[dim(configs)[1]]] <- nn#
#
configs
# run this only once#
if(exists("configs") == F){#
	library(deepnet)#
	configs = data.frame(hidden_rbm=integer(), numempochs_rbm=integer(), batchsize_rbm=integer(), lr_rbm=numeric(), cd=integer(), hidden_nn=character(), lr_nn=numeric(), numepochs_nn=integer(), batchsize_nn=numeric(), onehot=integer(), err_score=numeric(), err_score_norm=numeric())#
	predict_list <- list()#
	predict_norm_list <- list()#
	nn_list <- list()#
}#
onehot <- 1#
#
hidden_rbm <- 500#
numepochs_rbm <- 10#
batchsize_rbm <- 90#
learningrate_rbm <- 0.01#
learningrate_scale_rbm <- 0.8#
cd <- 3#
#
hidden_nn <- c(1000)#
learningrate_nn <- 0.01#
learningrate_scale_nn <- 0.8#
numepochs_nn <- 10#
batchsize_nn <- 100#
#
X_train <- read.csv(file="../X_train.csv", header=T, sep=",", row.names=1)#
X_test <- read.csv(file="../X_test.csv", header=T, sep=",", row.names=1)#
y_train_org <- read.csv(file="../y_train.csv", header=T, sep=",", row.names=1)#
y_test_org <- read.csv(file="../y_test.csv", header=T, sep=",", row.names=1)#
#
X_train <- X_train[1:939,]#
X_test <- X_test[1:313,]#
#
# trying one hot encoding#
onehot_test <- matrix(0L, nrow=dim(y_test_org)[1], ncol=max(y_test_org)+1)#
counter <- 1#
for(y in 1:dim(y_test_org)[1]){#
	onehot_test[counter,y_test_org[y,]+1] <- 1#
	counter <- counter+1#
}#
# trying one hot encoding#
onehot_train <- matrix(0L, nrow=dim(y_train_org)[1], ncol=max(y_train_org)+1)#
counter <- 1#
for(y in 1:dim(y_train_org)[1]){#
	onehot_train[counter, y_train_org[y,]+1] <- 1#
	counter <- counter+1#
}#
X_train <- as.matrix(X_train)#
X_test <- as.matrix(X_test)#
y_train <- as.matrix(y_train_org)#
y_test <- as.matrix(y_test_org)#
#
if(onehot){#
	y_test <- onehot_test#
	y_train <- onehot_train#
}#
#
rbm <- rbm.train(x=X_train, hidden=hidden_rbm, numepochs = numepochs_rbm, batchsize = batchsize_rbm, learningrate = learningrate_rbm, learningrate_scale = learningrate_scale_rbm, momentum = 0.5, visible_type = "bin", hidden_type = "bin", cd = cd)#
#
transformed_train <- rbm.up(rbm, X_train)#
transformed_test <- rbm.up(rbm, X_test)#
#
nn = nn.train(x=transformed_train, y_train, initW = NULL, initB = NULL, hidden = hidden_nn, activationfun = "sigm", learningrate = learningrate_nn, momentum = 0.5, learningrate_scale = learningrate_scale_nn, output = "sigm", numepochs = numepochs_nn, batchsize = batchsize_nn, hidden_dropout = 0, visible_dropout = 0)#
#
score <- 0#
score <- nn.test(nn, transformed_test, y_test, t = 0.5)#
#
pred <- nn.predict(nn, transformed_test)#
pred_norm <- matrix(0L, nrow=dim(pred)[1], ncol=dim(pred)[2])#
for(i in 1:dim(pred)[1]){#
	max_row <- which.max(pred[i,])#
	pred_norm[i,max_row] <- 1#
}#
#
score_norm <- 0#
for(i in 1:dim(pred_norm)[1]){#
	if(which.max(pred_norm[i,]) == which.max(y_test[i,])){#
		score_norm <- score_norm +1#
	}#
}#
score_norm <- 1- score_norm/dim(pred)[1]#
#
configs <- rbind(configs,data.frame(hidden_rbm=hidden_rbm, numempochs_rbm = numepochs_rbm, batchsize_rbm=batchsize_rbm, lr_rbm=learningrate_rbm, cd=cd, hidden_nn=paste(hidden_nn,collapse=" "), lr_nn=learningrate_nn, numepochs_nn=numepochs_nn, batchsize_nn=batchsize_nn, onehot=onehot, err_score=score, err_score_norm = score_norm))#
predict_list[[dim(configs)[1]]] <- pred#
predict_norm_list[[dim(configs)[1]]] <- pred_norm#
nn_list[[dim(configs)[1]]] <- nn#
#
configs
# run this only once#
if(exists("configs") == F){#
	library(deepnet)#
	configs = data.frame(hidden_rbm=integer(), numempochs_rbm=integer(), batchsize_rbm=integer(), lr_rbm=numeric(), cd=integer(), hidden_nn=character(), lr_nn=numeric(), numepochs_nn=integer(), batchsize_nn=numeric(), onehot=integer(), err_score=numeric(), err_score_norm=numeric())#
	predict_list <- list()#
	predict_norm_list <- list()#
	nn_list <- list()#
}#
onehot <- 1#
#
hidden_rbm <- 1000#
numepochs_rbm <- 10#
batchsize_rbm <- 90#
learningrate_rbm <- 0.01#
learningrate_scale_rbm <- 0.8#
cd <- 3#
#
hidden_nn <- c(1000)#
learningrate_nn <- 0.01#
learningrate_scale_nn <- 0.8#
numepochs_nn <- 10#
batchsize_nn <- 100#
#
X_train <- read.csv(file="../X_train.csv", header=T, sep=",", row.names=1)#
X_test <- read.csv(file="../X_test.csv", header=T, sep=",", row.names=1)#
y_train_org <- read.csv(file="../y_train.csv", header=T, sep=",", row.names=1)#
y_test_org <- read.csv(file="../y_test.csv", header=T, sep=",", row.names=1)#
#
X_train <- X_train[1:939,]#
X_test <- X_test[1:313,]#
#
# trying one hot encoding#
onehot_test <- matrix(0L, nrow=dim(y_test_org)[1], ncol=max(y_test_org)+1)#
counter <- 1#
for(y in 1:dim(y_test_org)[1]){#
	onehot_test[counter,y_test_org[y,]+1] <- 1#
	counter <- counter+1#
}#
# trying one hot encoding#
onehot_train <- matrix(0L, nrow=dim(y_train_org)[1], ncol=max(y_train_org)+1)#
counter <- 1#
for(y in 1:dim(y_train_org)[1]){#
	onehot_train[counter, y_train_org[y,]+1] <- 1#
	counter <- counter+1#
}#
X_train <- as.matrix(X_train)#
X_test <- as.matrix(X_test)#
y_train <- as.matrix(y_train_org)#
y_test <- as.matrix(y_test_org)#
#
if(onehot){#
	y_test <- onehot_test#
	y_train <- onehot_train#
}#
#
rbm <- rbm.train(x=X_train, hidden=hidden_rbm, numepochs = numepochs_rbm, batchsize = batchsize_rbm, learningrate = learningrate_rbm, learningrate_scale = learningrate_scale_rbm, momentum = 0.5, visible_type = "bin", hidden_type = "bin", cd = cd)#
#
transformed_train <- rbm.up(rbm, X_train)#
transformed_test <- rbm.up(rbm, X_test)#
#
nn = nn.train(x=transformed_train, y_train, initW = NULL, initB = NULL, hidden = hidden_nn, activationfun = "sigm", learningrate = learningrate_nn, momentum = 0.5, learningrate_scale = learningrate_scale_nn, output = "sigm", numepochs = numepochs_nn, batchsize = batchsize_nn, hidden_dropout = 0, visible_dropout = 0)#
#
score <- 0#
score <- nn.test(nn, transformed_test, y_test, t = 0.5)#
#
pred <- nn.predict(nn, transformed_test)#
pred_norm <- matrix(0L, nrow=dim(pred)[1], ncol=dim(pred)[2])#
for(i in 1:dim(pred)[1]){#
	max_row <- which.max(pred[i,])#
	pred_norm[i,max_row] <- 1#
}#
#
score_norm <- 0#
for(i in 1:dim(pred_norm)[1]){#
	if(which.max(pred_norm[i,]) == which.max(y_test[i,])){#
		score_norm <- score_norm +1#
	}#
}#
score_norm <- 1- score_norm/dim(pred)[1]#
#
configs <- rbind(configs,data.frame(hidden_rbm=hidden_rbm, numempochs_rbm = numepochs_rbm, batchsize_rbm=batchsize_rbm, lr_rbm=learningrate_rbm, cd=cd, hidden_nn=paste(hidden_nn,collapse=" "), lr_nn=learningrate_nn, numepochs_nn=numepochs_nn, batchsize_nn=batchsize_nn, onehot=onehot, err_score=score, err_score_norm = score_norm))#
predict_list[[dim(configs)[1]]] <- pred#
predict_norm_list[[dim(configs)[1]]] <- pred_norm#
nn_list[[dim(configs)[1]]] <- nn#
#
configs
# run this only once#
if(exists("configs") == F){#
	library(deepnet)#
	configs = data.frame(hidden_rbm=integer(), numempochs_rbm=integer(), batchsize_rbm=integer(), lr_rbm=numeric(), cd=integer(), hidden_nn=character(), lr_nn=numeric(), numepochs_nn=integer(), batchsize_nn=numeric(), onehot=integer(), err_score=numeric(), err_score_norm=numeric())#
	predict_list <- list()#
	predict_norm_list <- list()#
	nn_list <- list()#
}#
onehot <- 1#
#
hidden_rbm <- 100#
numepochs_rbm <- 10#
batchsize_rbm <- 100#
learningrate_rbm <- 0.01#
learningrate_scale_rbm <- 1#
cd <- 3#
#
hidden_nn <- c(100)#
learningrate_nn <- 0.01#
learningrate_scale_nn <- 1#
numepochs_nn <- 10#
batchsize_nn <- 100#
#
X_train <- read.csv(file="../X_train.csv", header=T, sep=",", row.names=1)#
X_test <- read.csv(file="../X_test.csv", header=T, sep=",", row.names=1)#
y_train_org <- read.csv(file="../y_train.csv", header=T, sep=",", row.names=1)#
y_test_org <- read.csv(file="../y_test.csv", header=T, sep=",", row.names=1)#
#
X_train <- X_train[1:939,]#
X_test <- X_test[1:313,]#
#
# trying one hot encoding#
onehot_test <- matrix(0L, nrow=dim(y_test_org)[1], ncol=max(y_test_org)+1)#
counter <- 1#
for(y in 1:dim(y_test_org)[1]){#
	onehot_test[counter,y_test_org[y,]+1] <- 1#
	counter <- counter+1#
}#
# trying one hot encoding#
onehot_train <- matrix(0L, nrow=dim(y_train_org)[1], ncol=max(y_train_org)+1)#
counter <- 1#
for(y in 1:dim(y_train_org)[1]){#
	onehot_train[counter, y_train_org[y,]+1] <- 1#
	counter <- counter+1#
}#
X_train <- as.matrix(X_train)#
X_test <- as.matrix(X_test)#
y_train <- as.matrix(y_train_org)#
y_test <- as.matrix(y_test_org)#
#
if(onehot){#
	y_test <- onehot_test#
	y_train <- onehot_train#
}#
#
rbm <- rbm.train(x=X_train, hidden=hidden_rbm, numepochs = numepochs_rbm, batchsize = batchsize_rbm, learningrate = learningrate_rbm, learningrate_scale = learningrate_scale_rbm, momentum = 0.5, visible_type = "bin", hidden_type = "bin", cd = cd)#
#
transformed_train <- rbm.up(rbm, X_train)#
transformed_test <- rbm.up(rbm, X_test)#
#
nn = nn.train(x=transformed_train, y_train, initW = NULL, initB = NULL, hidden = hidden_nn, activationfun = "sigm", learningrate = learningrate_nn, momentum = 0.5, learningrate_scale = learningrate_scale_nn, output = "sigm", numepochs = numepochs_nn, batchsize = batchsize_nn, hidden_dropout = 0, visible_dropout = 0)#
#
score <- 0#
score <- nn.test(nn, transformed_test, y_test, t = 0.5)#
#
pred <- nn.predict(nn, transformed_test)#
pred_norm <- matrix(0L, nrow=dim(pred)[1], ncol=dim(pred)[2])#
for(i in 1:dim(pred)[1]){#
	max_row <- which.max(pred[i,])#
	pred_norm[i,max_row] <- 1#
}#
#
score_norm <- 0#
for(i in 1:dim(pred_norm)[1]){#
	if(which.max(pred_norm[i,]) == which.max(y_test[i,])){#
		score_norm <- score_norm +1#
	}#
}#
score_norm <- 1- score_norm/dim(pred)[1]#
#
configs <- rbind(configs,data.frame(hidden_rbm=hidden_rbm, numempochs_rbm = numepochs_rbm, batchsize_rbm=batchsize_rbm, lr_rbm=learningrate_rbm, cd=cd, hidden_nn=paste(hidden_nn,collapse=" "), lr_nn=learningrate_nn, numepochs_nn=numepochs_nn, batchsize_nn=batchsize_nn, onehot=onehot, err_score=score, err_score_norm = score_norm))#
predict_list[[dim(configs)[1]]] <- pred#
predict_norm_list[[dim(configs)[1]]] <- pred_norm#
nn_list[[dim(configs)[1]]] <- nn#
#
configs
